\documentclass[aps,prc,reprint,amsmath,nofootinbib]{revtex4-1}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[inline]{enumitem}

\usepackage{color}
\definecolor{theblue}{RGB}{0,50,230}

\usepackage[pdfencoding=auto, psdextra]{hyperref}
\usepackage[font=small, justification=raggedright, labelsep=quad]{caption}

\hypersetup{
  colorlinks=true,
  linkcolor=theblue,
  citecolor=theblue,
  urlcolor=theblue
}

\usepackage{tikz}

\usepackage{graphicx}
\graphicspath{{../plots/}{./fig/}}

\newcommand{\trento}{T\raisebox{-0.5ex}{R}ENTo}
\newcommand{\sqrts}{\sqrt{s_{NN}}}
\newcommand{\nch}{N_\text{ch}}
\newcommand{\ntrk}{N_\text{trk}^\text{offline}}
\newcommand{\sigmaf}{\sigma_\mathrm{fluct}}
\newcommand{\X}{\chi_\mathrm{struct}}
\newcommand{\taufs}{\tau_\mathrm{fs}}
\newcommand{\dmin}{d_\mathrm{min}}
\newcommand{\T}{\tilde{T}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\vnk}[2]{v_#1\{#2\}}
\newcommand{\paddedhline}{\noalign{\smallskip}\hline\noalign{\smallskip}}
\newcommand{\order}[1]{$\mathcal O(10^{#1})$}
\newcommand{\note}{\textcolor{theblue}{[?]}}

% Reduce the size of the itemize bullets
\renewcommand\labelitemi{$\vcenter{\hbox{\scriptsize$\bullet$}}$}

% Create a temporary full-page single column environment
\newenvironment{fullpage}{\onecolumngrid}{\clearpage\twocolumngrid}

% hyperref throws warning when a line break is used in the title
% this removes the warning by redefining the line break in a pdf string
\pdfstringdefDisableCommands{%
  \def\\#1{ #1}
}


\begin{document}

\title{
  Estimating initial state and quark-gluon plasma medium properties \\
  using a hybrid model with nucleon substructure \\
  calibrated to p-Pb and Pb-Pb collisions at \texorpdfstring{$\mathbf{\sqrts=5.02}$}{}~TeV
}

\author{J.\ Scott Moreland}
\author{Jonah E.\ Bernhard}
\author{Steffen A.\ Bass}

\affiliation{Department of Physics, Duke University, Durham, NC 27708-0305}

\date{\today}

\begin{abstract}
We posit a unified hydrodynamic and microscopic description of the quark-gluon plasma (QGP) produced in ultrarelativistic $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV and evaluate our assertion using Bayesian inference. Specifically, we model the dynamics of both collision systems using initial conditions with parametric nucleon substructure, a pre-equilibrium free-streaming stage, event-by-event viscous hydrodynamics, and a microscopic hadronic afterburner.
Free parameters of the model which describe the initial state and QGP medium are then simultaneously calibrated to fit charged particle yields, mean $p_T$ and flow cumulants.
We argue that the global agreement of the calibrated model with the experimental data strongly supports the existence of hydrodynamic flow in small collision systems at ultrarelativistic energies, and that the flow produced develops at length scales smaller than a single proton.
Posterior estimates for the model's input parameters are obtained, and new insights into the temperature dependence of the QGP transport coefficients and event-by-event structure of the proton are discussed.
\end{abstract}

\maketitle


\section{Introduction}

Ultrarelativistic nuclear collisions between one light-ion and one heavy-ion, e.g.\ $^3$He-Au and $p$-Pb collisions, generate dense, compact sources of nuclear matter which produce long-range multiparticle correlations that are strikingly similar to the correlations observed in heavy-ion collisions where collectivity is commonly explained by the existence of hydrodynamic flow \cite{CMS:2012qk, Abelev:2012ola, Aad:2012gla, Adare:2015ctn}.
This observation suggests that hydrodynamic behavior could be manifest in small droplets of quark-gluon-plasma (QGP) \cite{Bozek:2011if, Bozek:2013uha}, and that flow might develop at length scales smaller than a single proton \note.

Hydrodynamic models of ultrarelativistic nuclear collisions are complicated by a number of theoretical unknowns, including the detailed geometry of the QGP initial conditions, the strength and duration of pre-equilibrium dynamics, the temperature dependence of QGP transport coefficients, and the boundaries of hydrodynamic applicability \cite{Niemi:2014lha, deSouza:2015ena, Ollitrault:2012cm, Song:2012ua}.
In general, these theoretical uncertainties tend to grow with decreasing system size, where emergent physics at sub-fermi length scales becomes important to describe bulk properties of the produced system.

One method for reducing theoretical uncertainties is to test model calculations by varying the species of colliding nuclei at a single beam energy \cite{Adare:2015bua, Schenke:2014tga, Aidala:2018mcw, Adare:2017wlc, Adamczyk:2015obl, Shen:2016zpp, Aidala:2017ajz, Adare:2006ti}.
Since initial condition and hydrodynamic models generally factorize the structure of the colliding nuclei from the subsequent time dynamics of the collision, a single theory framework can be simultaneously tested and compared to measurements from multiple collision systems using a self-consistent set of model parameters where only the nuclear structure in the model is permitted to vary.

Typically, the macroscopic structure of heavy nuclei, characterized e.g.\ by an atomic mass number and set of Woods-Saxon coefficients \cite{MOLLER1995185, DEVRIES1987495}, is regarded as a known input to hydrodynamic models which contributes negligible uncertainty to simulation predictions, outweighed by large uncertainties in modeling initial energy deposition and off-equilibrium dynamics \cite{Niemi:2014lha, Song:2011hk, Retinskaya:2013gca, Liu:2015nwa, Kurkela:2016vts}.
The geometry of light ions, meanwhile, is naturally more sensitive to the detailed size and shape of individual protons and neutrons inside the nucleus, which may fluctuate event-by-event and differ signficantly from the round blobs typically used to approximate nucleons in heavy-ion collisions \cite{Schenke:2014zha, Welsh:2016siu, Moreland:2017kdx, Schenke:2014gaa, Schlichting:2014ipa}.
These nucleon substructure properties are difficult to measure and calculate from first principles and hence contribute significant uncertainty to model predictions of small systems.

Early substructure studies replaced round protons with composite protons, described by a few salient model parameters, in order to investigate the effect of each parameter on simulated observables \cite{Adler:2013aqf, Mitchell:2016jio, Welsh:2016siu, Broniowski:2016pvx, Bozek:2017jog}.
These sensitivity studies were able to identify cause and effect relationships between model inputs and outputs, but lacked the ability to constrain nucleon substructure parameters in any kind of global or systematic fashion.
It quickly became apparent that numerous substructure implementations might be compatible with available data, and that additional work would be required to identify observables which are particularly sensitive to the average size, shape and fluctuations of the proton.

Several such observables have been identified in proton-proton and proton-lepton scattering data.
Measurements by the TOTEM collaboration at $\sqrt{s}=7$~TeV, for instance, found an unexpected dip in the inelasticity density of $p$-$p$ collisions at zero impact parameter \cite{Antchev:2011zz}.
It was later realized that this depression, or so-called hollowness effect in the $p$-$p$ inelastic collision profile \cite{Arriola2016}, can be explained by the existence of correlated domains inside the proton, and that aspects of these domains, such as their size and correlation strength, may be constrained by comparing model predictions to inelastic $p$-$p$ measurements \cite{Albacete:2016gxu, Albacete:2016pmp}.

Independently, studies of coherent and incoherent $J/\psi$ production based on a color dipole picture of vector meson production were used to simultaneously constrain both the average color charge density of the proton as well as its event-by-event fluctuations in a saturation based framework \cite{Mantysaari:2016ykx, Mantysaari:2016jaz, Aaron:2009aa, Abramowicz:2015mha}.
Initial condition studies using the IP-Glasma model of color-glass condensate effective field theory \cite{Schenke:2012wb} simultaneously demonstrated that these color charge fluctuations leave a lasting imprint on the \mbox{small-x} gluon distribution of the proton and hence the initial geometry of QGP energy deposition \cite{Schlichting:2014ipa}.
In addition, it was recently shown that hydrodynamic simulations using IP-Glasma initial conditions with color charge fluctuations calibrated to fit coherent and incoherent $J/\psi$ diffraction measured by the H1 and Zeus experiments at HERA \cite{Aaron:2009aa, Abramowicz:2015mha} provide a good description of collectivity in small and large collision systems \cite{Schenke:2018qmc}.

Model parameters, such as those calibrated by the aforementioned studies, are of course always in some degree of tension.
For instance, fitting one observable may require parameter values that degrade the quantitative description of some other observable.
Similarly, parameters which provide an optimal description of small-system observables may lead to a sub-optimal description of heavy-ion observables or \emph{vice versa}.
It is thus import to look at the experimental data holistically, and to use model calibration methods which
\begin{enumerate*}[label=(\arabic*)]
  \item explore all parameter combinations and
  \item compare model predictions to all relevant experimental measurements in a statistically rigorous fashion.
\end{enumerate*}

With these considerations in mind, we present progress towards a fully global analysis of $p$-Pb and Pb-Pb bulk observables at $\sqrts=5.02$~TeV using a model calibration framework known as Bayesian parameter estimation.
We begin, in Sec.~\ref{sec:model}, by constructing a nuclear collision model for $p$-Pb and Pb-Pb collisions using initial conditions with parametric nucleon substructure, and transport dynamics described by a pre-equilibrium free-streaming stage, viscous hydrodynamics and microscopic Boltzmann transport.
In Sec.~\ref{sec:calibration}, we calibrate free parameters of the model to fit charged particle yields, mean $p_T$ and anisotropic flow cumulants of \emph{both} collision systems at $\sqrts=5.02$~TeV, and finally, in Secs.~\ref{sec:results} and \ref{sec:summary}, we present posterior results for the model input parameters and comment on the implications for hydrodynamic descriptions of small collision systems.


\section{Nuclear collision model}
\label{sec:model}

We employ a multi-stage hybrid transport model that uses relativistic viscous hydrodynamics to describe the QGP medium and microscopic Boltzmann transport to simulate the dynamics of the system after hadronization \cite{Shen:2014vra, Bernhard:2016tnd}.
The hydrodynamic initial conditions are provided by a modified version of the \trento\ model \cite{Moreland:2014oya} with additional parameters to vary the size and shape of the nucleon.
Each initial condition profile is free streamed to the hydrodynamic starting time and matched onto the hydrodynamic energy-momentum tensor using the Landau matching procedure \cite{Broniowski:2008qk, Heinz:2015arc}.
Many of the components of the present model have been documented in previous studies \cite{Moreland:2014oya, Bernhard:2016tnd, Bernhard:2018hnz}; we review each component here for completeness.

\subsection{Initial state}
\label{sec:initial_state}

We model the QGP initial state in $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV using a simple parametric form for boost-invariant entropy deposition employed by the \trento\ model \cite{Moreland:2014oya}.
Generally speaking, the initial three-dimensional distribution of matter produced in relativistic nuclear collisions is \emph{not} boost-invariant; longitudinal entropy deposition fluctuates both locally point-to-point in the transverse plane as well as globally event-by-event due to asymmetries in the sampled density of participant matter \cite{Ke:2016jrd, Bozek:2010vz}.
Nevertheless, boost-invariance has been shown to be a good approximation for both large and small collision systems when hydrodynamic observables are calculated from particles that are detected close to midrapidity \cite{Shen:2016zpp}.

The \trento\ model operates in the ultrarelativistic limit with a Lorentz factor $\gamma \gg 1$ such that each nucleus appears as a thin sheet of nuclear density in the laboratory frame.
The sheets of colliding nuclear density penetrate and pass through each other in time $\Delta t = D_\text{nucl} / \sqrt{\gamma^2 - 1}$ in the laboratory frame, where $D_\text{nucl}$ is the diameter of the nucleus in its rest frame and $\gamma$ is the usual Lorentz factor of the accelerated ions.
The resulting nuclear overlap time $\Delta t \lesssim 0.1$~fm/$c$ at top RHIC and LHC energies is thus sufficiently short to neglect initial transverse dynamics which occur while the nuclei pass through each other.
We therefore assume that the collision produces all secondary particles at uniform proper time $\tau = 0^+$~fm/$c$, and that it deposits entropy (energy) at midrapidity which is a function of the locally varying beam integrated density of each nucleus.

Consider the collision of two protons $A$, $B$ with three-dimensional densities $\rho_{A,B}$ in their local rest frames.
The proton-proton overlap function
\begin{equation}
  \label{eq:tpp}
  T_{pp}(b) \equiv \int dx\, dy \int dz\, \rho_A(\x) \int dz\, \rho_B(\x + \mathbf{b}),
\end{equation}
describes the eikonal overlap of the two proton wave packets at fixed impact parameter $b$ where coordinates $(x, y)$ lie in the transverse plane and $z$ is parallel to the beam axis.
Here we assume that each proton is comprised of smaller constituents---e.g.\ valence quarks, sea quarks, and small-x gluons---which may collide to produce secondary particles and contribute to the observed inelastic proton-proton cross section.

Within a picture of independent pairwise collisions between the constituents, a Glauber model model may be used to calculate the probability $P_\mathrm{coll}$ that the two protons collide inelastically at impact parameter $b$. In the limit when the number of constituents is large, it yields the particularly simple form
\begin{equation}
  \label{eq:pcoll}
  P_\mathrm{coll} = 1 - \exp[-\sigma_\mathrm{eff}\, T_{pp}(b)], \\
\end{equation}
where $\sigma_\mathrm{eff}$ is an effective cross section for pairwise inelastic collisions between the constituents, and $\sigma_{pp}^\mathrm{inel}$ is the total inelastic proton-proton cross section
\begin{equation}
  \label{eq:sigma_nn}
  \sigma_{pp}^\mathrm{inel} = \int 2 \pi b\, db\, P_\mathrm{coll}(b).
\end{equation}
The proton densities $\rho_{A,B}$ in Eqn.~\eqref{eq:tpp} are commonly modeled using a spherically symmetric distribution.
For instance, the original implementation of the \trento\ model uses Gaussian protons, largely because it yields a simple analytic solution to Eqn.~\eqref{eq:pcoll}.
Needless to say, such approximations are admittedly crude and may have a significant effect on the dynamics of small collision systems where the proton size is comparable to the size of the produced QGP medium.

A number of previous studies have investigated the effects of deformed or ``lumpy'' protons.
One common implementation is a superposition of three valence quarks, typically described by Gaussian or exponential form factors \cite{Welsh:2016siu, Bozek:2017jog, Schenke:2014zha, Schlichting:2014ipa, Adare:2015bua, Broniowski:2016pvx}.
The corresponding proton density $\rho(\mathbf{x})$ is then assumed to be that of predominantly small-x gluons, seeded by the distribution of color charge in each of the three valence quarks.

\begin{figure}
  \begin{tikzpicture}
    % spherical proton
    \draw[dashed, xshift=-2.5cm] (0,0) circle (1cm);
    % three partons
    \draw[dashed] (0,0) circle (1cm);
    \foreach \theta in {0, 120, 240}{
      \draw ({\theta}:.5) circle (.4cm);
    }
    % ten partons
    \draw[dashed, xshift=2.5cm] (0,0) circle (1cm);
    \foreach \theta/\radius in {
      0/0.6, 40/0.5, 90/0.6, 130/0.5, 180/0.6, 220/0.6,
      260/0.5, 320/0.6, 300/0.2
    }{
      \draw[xshift=2.5cm] ({\theta}:\radius) circle (.3cm);
    }
  \end{tikzpicture}
  \caption{Schematic of plausible proton shapes. The sketch on the left shows a spherically symmetric proton (dashed line), while the middle and right illustrations depict a fluctuating proton with three and nine constituents respectively (solid lines).}
  \label{fig:substructure}
\end{figure}

In this work, we pursue a less restrictive and more parametric description of the proton where the number of substructure degrees of freedom are uncertain as depicted in Fig.~\ref{fig:substructure}.
We model each nucleon's density $\rho_{A,B}$ as a sum of $n_c$ independent constituents
\begin{equation}
  \label{eq:rho}
  \rho_{A, B}(\x) = \frac{1}{n_c} \sum\limits_{i=1}^{n_c} \rho_c(\mathbf{x} - \mathbf{x_i}),
\end{equation}
where each constituent density $\rho_c$ is described by a Gaussian distribution of width $v$
\begin{equation}
  \label{eq:constituent_density}
  \rho_c(\mathbf{x}) = \frac{1}{(2\pi v^2)^{3/2}} \exp\left(-\frac{\x^2}{2 v^2}\right),
\end{equation}
and each constituent's position $\mathbf{x_i}$ in Eqn.~\eqref{eq:rho} is sampled from a Gaussian radial distribution with standard deviation $r$.

The two protons $A$, $B$ are assigned a random impact parameter, and Eqn.~\eqref{eq:pcoll} is used to sample their inelastic collision probability $P_\mathrm{coll}(b)$.
Note that this proton-proton inelastic collision probability has no direct knowledge of the individual constituent degrees of freedom; it is only \emph{indirectly} sensitive through the geometry of $\rho_{A, B}$ which depends on each of the constituent positions.
This is an important distinction between the present model and a similar nucleon substructure implementation known as the participant or ``wounded'' quark model which allows for a subset of quarks (constituents) to participate inside a single nucleon \cite{ANISOVICH1978477, Broniowski:2016pvx}.
The proton, unlike the nucleus, cannot produce semi-stable spectator fragments in a high-energy collision.
Any spectator quarks produced by a wounded quark model would be colored objects that necessarily contribute to secondary particle production as they fragment and recombine to form color-neutral hadrons.
We correspondingly require that the nucleons in Eqn.~\eqref{eq:rho} participate as singular objects, such that all spectator matter discarded by the simulation is appropriately color-neutral and inert.

Assuming our two protons collide at the sampled impact parameter $\mathbf{b}$, we assign each a \emph{fluctuated} thickness
\begin{equation}
  \label{eq:fluctuated_thick}
  \T_{A, B}(\x) \equiv \int dz\, \frac{1}{n_c} \sum\limits_{i=1}^{n_c} \gamma_i\, \rho_c \,(\mathbf{x} - \mathbf{x_i} \pm \mathbf{b}/2),
\end{equation}
equal to the beam-integrated proton density in Eqn.~\eqref{eq:rho}, with each constituent shifted by the appropriate impact parameter offset, and multiplied by a gamma random variable $\gamma_i$ with unit mean and variance $1/k$.
These \emph{ad hoc} gamma random weights are necessary to describe the large fluctuations observed in minimum bias proton-proton collisions, although their exact physical origin is not well understood.

Within the eikonal approximation, the initial entropy deposited at midrapidity at proper time $\tau=0^+$~fm/$c$ can be represented by some mapping ${f(\T_A, \T_B) \mapsto dS/dy}$.
A natural first guess for the mapping is the arithmetic mean
\begin{equation}
  \frac{dS}{dy}\bigg\vert_{y=0} \propto \frac{\T_A + \T_B}{2}
\end{equation}
of the participant nuclear matter $\T_A$ and $T_B$, which is simply a wounded nucleon model up to meaningless factor of two in the normalization.
The wounded nucleon model was in fact one of the first such mappings used as a proxy for initial particle production and entropy deposition in relativistic heavy-ion collisions \cite{Bialas:1976ed}.
It was quickly realized, however, that the wounded nucleon model predicts the wrong scaling for charged particle production as a function of collision centrality and hence the wrong scaling for initial entropy deposition as a function of participant thicknesses $\T_A$ and $\T_B$ \cite{Kharzeev:2000ph}.

A simple remedy is to replace the arithmetic mean of the wounded nucleon model with a more flexible parametrization
\begin{equation}
  \label{eq:gmean}
  \frac{dS}{dy}\bigg\vert_{y=0} \propto \left( \frac{\T_A^p + \T_B^p}{2} \right)^{1/p},
\end{equation}
based on a family of functions known as the generalized mean(s).
This parametrization introduces a dimensionless parameter $p$ which varies the scaling behavior of initial entropy deposition at midrapidity.
For certain discrete values of $p$, it reduces to well known functional forms such as the arithmetic, geometric and harmonic means:
\begin{equation}
  \newlength{\extraspace}
  \setlength{\extraspace}{0.5ex}
  \frac{dS}{dy}\bigg\vert_{y=0} \propto
  \begin{cases}
    \max(T_A, T_B) & p \rightarrow +\infty, \\[\extraspace]
    (T_A + T_B)/2 & p = +1, \hfill \text{ (arithmetic)} \\[\extraspace]
    \sqrt{T_A T_B} & p = 0, \hfill \text{ (geometric)} \\[\extraspace]
    2\, T_A T_B/(T_A + T_B) & p = -1, \hfill \text{ (harmonic)} \\[\extraspace]
    \min(T_A, T_B) & p \rightarrow -\infty.
  \end{cases}
  \label{eq:trento_p}
\end{equation}
Conveniently, it has been shown that the generalized mean ansatz is able to mimic the scaling behavior of certain saturation based initial condition models \cite{Bernhard:2016tnd}, and hence it should serve as a reasonable parametric form for exploring QGP entropy deposition, assuming imperfect knowledge of saturation effects in nature.
Of course the model is \emph{not} a substitute for first principle theory calculations, and it may fail to reproduce nuanced features of \emph{ab initio} models such as the existence of short-range gluon field fluctuations \cite{Schenke:2012wb}.

\begin{figure}
  \begin{tikzpicture}
    \node {\includegraphics{thickness}};
    \node[label=above:{10 fm}] (a) at (0, 1.1) {};
    \node (b) at (0, -1.1) {};
    \draw [<->, semithick] (a) to (b);
  \end{tikzpicture}
  \caption{Effect of nucleon substructure on the nuclear thickness function $T(x, y) \equiv \int dz\, \rho(x, y, z)$ of a $^{208}\mathrm{Pb}$ nucleus. The nucleus on the left has Gaussian nucleons of width $0.8$~fm, while the nucleus on the right has composite nucleons, each containing six constituents of width $0.4$~fm.}
  \label{fig:thickness}
\end{figure}

Equation~\eqref{eq:gmean} is a purely local function of nuclear density in the transverse plane and should (in principle) be equally valid for any pair of colliding nuclei at sufficiently high beam energy.
The model readily generalizes from individual proton-proton collisions to arbitrary nucleus-nucleus collisions by summing the participant thicknesses $\T_{A,B}$ over all nucleons which participate in one or more inelastic collisions.
The only modeling difference between $p$-$p$ collisions and Pb-Pb collisions is the number and the position of the nucleons.

When applying the model to heavy-ions, we sample nucleon positions from a Woods-Saxon density distribution subject to a minimum distance criteria $|\mathbf{x_i} - \mathbf{x_j}| > d_\mathrm{min}$ between all pairs of nucleons $i$, $j$.
The minimum distance algorithm, first described in Ref.~\cite{Bernhard:2018hnz}, uses a simple trick to resample the nucleon positions without modifying the target Woods-Saxon radial distribution.
We first presample the radii of all nucleons in a given nucleus and sort them in ascending order.
We then sample the solid angles of each nucleon one-by-one, starting with the nucleon closest to the center of the nucleus and working our way outwards.
If a sampled nucleon position is too close to any of its previously placed neighbors, its solid angle is resampled until the minimum distance criteria is satisfied.
Similar methods could be used to model correlations between individual constituents inside each nucleon, although the numerical implementation would be somewhat tedious.

\subsection{Pre-equilibrium dynamics}

\begin{figure}[b]
  \includegraphics{coupling}
  \caption{Cartoon of the free-streaming approximation for hydrodynamic initialization. The initial state is free-streamed for proper time $\taufs$ (zero coupling) before it is matched to hydrodynamics (strong coupling). This piecewise evolution approximates the more realistic scenario expected in nature where the medium's coupling strength smoothly changes as a function of time.}
  \label{fig:coupling}
\end{figure}

There are of course two limiting cases for the strength of interactions inside the QGP medium immediately at the collision: infinitely weak coupling where the secondary partons free-stream without interacting, and infinitely strong coupling where the fluid's inter-particle mean free path effectively vanishes.
Realistically, one expects the initial parton interactions to lie somewhere between these two extremes.
We therefore choose to model the QGP's initial off-equilibrium dynamics using a simple step-function approximation, depicted in Fig.~\ref{fig:coupling}, which free-streams the initial state for proper time $\taufs$ (zero coupling) before instantaneously switching to viscous hydrodynamics (strong coupling) \cite{Liu:2015nwa, Broniowski:2008qk}.
The free parameter $\taufs$ allows us to parametrically vary the \emph{time averaged} coupling strength in the approximate window $0 < \taufs \lesssim1$~fm/$c$.

The parametric entropy deposition ansatz in Eqn.~\eqref{eq:gmean} does not provide any information about the initial masses or momenta of particles produced in the collision.
In general, these details will affect the dynamics predicted by the collisionless Boltzmann equation
\begin{equation}
  \label{freestream}
  p^\mu \partial_\mu f(x, p) = 0,
\end{equation}
through its dependence on the underlying distribution function $f(x, p)$, and hence are necessary inputs for any free-streaming implementation.
Equation~\eqref{freestream}, however, simplifies for massless partons with momentum distributions that are locally isotropic.
Under this assumption, it was shown in Refs.~\cite{Broniowski:2008qk, Liu:2015nwa} that the energy-momentum tensor $T^{\mu\nu}$ of partons at midrapidity and time $\tau$ only depends on its spatial distribution at some earlier time $\tau_0$, but not its $p_\perp$-distribution, which could vary as a function of position.

We thus follow the procedure of the authors and reinterpret our initial transverse entropy density at midrapidity $s(\mathbf{x_\perp}, \tau_0)\vert_{\eta=0}$ as a transverse density of noninteracting massless partons $n(\mathbf{x}, \tau_0)\vert_{\eta=0}$.
The free-streamed energy-momentum tensor $T^{\mu\nu}(x, y, \eta, \tau)$ at transverse coordinate $(x, y)$, midrapidity $\eta=0$, and proper time $\tau > \tau_0$ is then given by
\begin{multline}
  \label{energy-momentum}
  T^{\mu\nu}(x, y, 0, \tau) = \frac{1}{\tau} \int_{0}^{2\pi} d\phi\, n(x - \Delta \tau \cos \phi, y - \Delta \tau \sin \phi) \\ \times
  \begin{bmatrix}
    1 & \cos\phi & \sin\phi  & 0\\
    \cos\phi & \cos^2\phi & \cos\phi\sin\phi & 0 \\
    \sin\phi & \sin\phi \cos\phi & \sin^2\phi & 0\\
    0 & 0 & 0 & 0
  \end{bmatrix},
\end{multline}
where $\Delta\tau$ is the free-streaming time $\Delta\tau = \tau - \tau_0$.
The solution \eqref{energy-momentum} is decomposed in hydrodynamic form
\begin{equation}
  \label{hydro-eqn}
  T^{\mu\nu} = e u^\mu u^\nu - (P + \Pi) \Delta^{\mu\nu} + \pi^{\mu\nu},
\end{equation}
where $e$ and $P$ are the energy density and pressure in the local fluid rest frame, $u^\mu$ is the local fluid velocity, ${\Delta^{\mu\nu} \equiv g^{\mu\nu} - u^\mu u^\nu}$ is the projector onto the space orthogonal to $u^\mu$, and $\Pi$ and $\pi^{\mu\nu}$ are the bulk pressure and shear stress tensor respectively.
We then solve for the energy density $e$ and fluid velocity $u^\mu$ using the Landau matching condition which defines the fluid rest frame velocity as the time-like eigenvector of $T^{\mu\nu}$ with energy density $e$ as its eigenvalue,
\begin{equation}
  T^{\mu\nu} u_\nu = e u^\mu.
\end{equation}
The initial bulk and shear corrections are finally solved for by subtracting the ideal pressure from the total pressure to find $\Pi$, then solving for $\pi^{\mu\nu}$ using Eqn.~\eqref{hydro-eqn}
\begin{align}
  \Pi &= -\frac{1}{3} \mathrm{Tr}(\Delta_{\mu\nu} T^{\mu\nu}) - P,\\
  \pi^{\mu\nu} &= T^{\mu\nu} - e u^\mu u^\nu + (P + \Pi) \Delta^{\mu\nu}.
\end{align}

This procedure provides initial values for $T^{\mu\nu}$, $u^\mu$, $\Pi$ and $\pi^{\mu\nu}$ which conserve energy and are consistent with the underlying hydrodynamic equation of state.
We therefore expect it to provide a more realistic description of the initial stages of the collision as compared to a previous study using the \trento\ initial condition model which set $\Pi$, $\pi^{\mu\nu}$ and $u^\mu$ initially to zero \cite{Bernhard:2016tnd}.

\subsection{Hydrodynamics}

After free-streaming for proper time $\taufs$, we transition to viscous hydrodynamics which solves the conservation equations
\begin{equation}
  \label{eq:continuity}
  \partial_\mu T^{\mu\nu} = 0
\end{equation}
for the hydrodynamic energy-momentum tensor $T^{\mu\nu}$ expressed in Eqn.~\eqref{hydro-eqn} using a set of second-order Israel-Stewart equations formulated in the 14-moment approximation
\cite{Israel:1979wp, Israel:1976aa, Denicol:2012cn, Denicol:2010xn}.
This produces a pair of relaxation-type equations
\begin{subequations}
  \label{eq:relaxation}
  \begin{align}
    \tau_\Pi \Pi + \dot{\Pi} &=
      - \zeta \theta - \delta_{\Pi\Pi} \Pi\theta
      + \lambda_{\Pi\pi} \pi^{\mu\nu} \sigma_{\mu\nu}, \\[1ex]
    \tau_\pi \dot{\pi}^{\langle \mu\nu \rangle} + \pi^{\mu\nu} &=
      2\eta\sigma^{\mu\nu} - \delta_{\pi\pi} \pi^{\mu\nu} \theta
      + \phi_7 \pi_\alpha^{\langle \mu} \pi^{\nu \rangle \alpha} \nonumber \\
      &\qquad {} - \tau_{\pi\pi} \pi_\alpha^{\langle \mu}\sigma^{\nu \rangle \alpha}
      + \lambda_{\pi\Pi} \Pi \sigma^{\mu\nu},
  \end{align}
\end{subequations}
for the bulk pressure $\Pi$ and shear-stress $\pi^{\mu\nu}$.
We model the shear viscosity $\eta$ and bulk viscosity $\zeta$ as unknown temperature dependent quantities and fix the remaining transport coefficients $\{\tau_\Pi, \delta_{\Pi\Pi}, \lambda_{\Pi\pi}, \tau_\pi, \delta_{\pi\pi}, \phi_7, \tau_{\pi\pi}, \lambda_{\pi\Pi}\}$ using analytic results derived in the limit of small but finite masses \cite{Denicol:2014vaa}.

\begin{figure}[t]
  \includegraphics{viscosity_dof}
  \caption{Degrees of freedom in the temperature dependent shear and bulk viscosity parametrizations. Lines are chosen for illustrative purposes only and do not represent all possible variability. For instance, $\eta/s$ could have a large slope and negative curvature, or $\zeta/s$ could have a large max and narrow width, neither of which are depicted above.}
  \label{fig:viscosity_dof}
\end{figure}

The hydrodynamic equations of motion are necessarily closed using an equation of state (EoS) to relate the energy density $e$ and pressure $P$ of the produced medium.
We use a parametrization for $P(e)$ that matches a hadron resonance gas EoS at low temperature to a lattice QCD EoS at high temperature by smoothly connecting their trace anomalies in the interval
$165 \le T \le 200$~MeV \cite{Bernhard:2018hnz}.
For the lattice EoS, we use a calculation by the HotQCD collaboration for (2+1)-flavor QCD which was extrapolated to the continuum limit \cite{Bazavov:2014pvz}. Recent developments in lattice QCD now enable calculations in (2+1+1)-flavors \cite{Borsanyi:2016ksw}, i.e.\ with thermalized charm quarks, and the additional charm flavor has been shown to visibly affect predictions of $p_T$-differential flow observables \cite{Noronha-Hostler:2018zxc}.
Investigating this sensitivity would thus be a natural target for future improvements to the present work.

We parametrize the temperature dependence of the QGP viscosities in order to marginalize over their uncertainty when calibrating to data.
For the specific shear viscosity $\eta/s$, we use a modified linear ansatz
\begin{equation}
  \label{eq:shear_viscosity}
  (\eta/s)(T) = (\eta/s)_\mathrm{min} + (\eta/s)_\mathrm{slope}\cdot(T - T_c)\cdot(T/T_c)^{(\eta/s)_\mathrm{crv}},
\end{equation}
where $\eta/s$ min, slope and curvature are tunable parameters and $T_c=0.154$~GeV is the pseudocritical transition temperature of the HotQCD EoS.
While for the specific bulk viscosity $\zeta/s$, we use an unnormalized Cauchy distribution
\begin{equation}
  \label{eq:bulk_viscosity}
  (\zeta/s)(T) = \frac{(\zeta/s)_\mathrm{max}}{1 + \left(\dfrac{T - (\zeta/s)_{T_0}}{(\zeta/s)_\mathrm{width}}\right)^2},
\end{equation}
described by tunable maximum, width, and location ($T_0$) parameters.
Figure~\ref{fig:viscosity_dof} shows several of the possible curves parametrized by Eqns.~\eqref{eq:shear_viscosity} and \eqref{eq:bulk_viscosity}, although many more are possible.

The aforementioned hydrodynamic equations are solved numerically using the boost-invariant \textsc{VISH2+1} viscous hydrodynamics code \cite{Song:2007ux, Shen:2014vra}.
We simulate each hydrodynamic event on a spacetime grid with transverse extent $x_\mathrm{max}$, spatial grid step $dx$, and time step $d\tau$ which are optimized \emph{event-by-event} to balance trade-offs between numerical accuracy and computation time (see Appendix~\ref{app:adaptive_grid}).
Although these details are somewhat mundane, they are critically important to the present study, since the computation time scales with the number of spacetime cells $n_x^2\, n_\tau$, and $n_x$ and $n_\tau$ typically have to be quite large to resolve the small length scales associated with nucleon substructure.

\subsection{Particlization and Boltzmann transport}

We evolve the system hydrodynamically down to a pre-specified switching isotherm $T_{sw}$ at which point the medium is converted into particles using the Cooper-Frye formula \cite{PhysRevD.10.186}
\begin{equation}
  \label{cooper-frye}
  E \frac{dN_i}{d^3p} = \frac{g_i}{(2\pi)^3} \int_\Sigma f_i(x, p)\, p^\mu\, d^3\sigma_\mu,
\end{equation}
where $i$ is an index over species, $f_i$ is the distribution function of that species, and $d^3\sigma_\mu$ is a volume element of the isothermal hypersurface $\Sigma$ defined by $T_\mathrm{sw}$.
Thermal particles are then sampled in the rest frame of each fluid cell according to a Bose-Einstein or Fermi-Dirac distribution at zero baryon chemical potential
\begin{equation}
  \label{distribution}
  f(m, p) = \frac{1}{\exp(\sqrt{m^2 + p^2}/T) \mp 1},
\end{equation}
where $m$ is the mass of the sampled particle, $p$ is its momentum, and $T$ is the temperature of the fluid cell.

Traditionally, particlization models have sampled resonances using each particle's pole mass in Eqn.~\eqref{distribution}.
This approximation, however, is somewhat crude and has been known to underpredict pion production, particularly at low $p_T$ \cite{Sollfrank:1991xm, Huovinen:2016xxq, Vovchenko:2018fmh}.
We thus follow Ref.~\cite{Bernhard:2018hnz}, and sample particles with a \emph{distribution} of masses
\begin{equation}
  f(p) = \int dm\, \mathcal{P}(m)\, f(m, p),
\end{equation}
where $\mathcal{P}(m)$ is modeled by a Breit-Wigner distribution
\begin{equation}
  \mathcal{P}(m) \propto \frac{\Gamma(m)}{(m - m_0)^2 + \Gamma(m)^2/4}.
\end{equation}
Here $m_0$ is the resonance's Breit-Wigner mass and $\Gamma(m)$ is its mass-dependent width, for which we use a simple form:
\begin{equation}
  \Gamma(m) = \Gamma_0 \sqrt{\frac{m - m_\mathrm{min}}{m_0 - m_\mathrm{min}}},
\end{equation}
where $\Gamma_0$ is the usual Breit-Wigner width and $m_\mathrm{min}$ is a production threshold equal to the total mass of the lightest decay products.
We tabulate the values of $\{\Gamma_0, m_0, m_\mathrm{min}\}$ for all particles and sample the masses of each particle during particlization \cite{PDG:2017}.
The resonances are then passed to a hadronic transport model, described shortly, which simulates subsequent scatterings and decays.

When the viscous terms $\pi^{\mu\nu}$ and $\Pi$ are nonzero in Eqn.~\eqref{hydro-eqn}, the distribution function $f$ must be modified to preserve the continuity of $T^{\mu\nu}$ as the system transitions from hydrodynamics to Boltzmann transport.
We perform the appropriate modification using a general method which transforms the momentum vector \emph{inside} the distribution function \cite{Pratt:2010jt}
\begin{align}
  \label{viscous_correction}
  p_i \rightarrow p'_i &= p_i + \sum\limits_j \lambda_{ij}\, p_j,\\
  \lambda_{ij} &= (\lambda_\mathrm{shear})_{ij} + \lambda_\mathrm{bulk}\, \delta_{ij},
\end{align}
where $\lambda_{ij}$ is a linear transformation matrix consisting of a traceless shear part and a bulk part which is proportional to the identity matrix.

We use for the shear viscous correction the form \cite{Pratt:2010jt}
\begin{equation}
  (\lambda_\mathrm{shear})_{ij} = \frac{\tau}{2 \eta} \pi_{ij},
\end{equation}
with a value for $\eta/\tau$ obtained from the noninteracting hadron resonance gas model
\begin{equation}
  \frac{\eta}{\tau} = \frac{1}{15 T} \sum\limits_{sp} g \int \frac{d^3p}{(2\pi)^3}\frac{p^4}{E^2} f_0 (1 \pm f_0).
\end{equation}
For the bulk viscous correction, we use a novel procedure developed in Ref.~\cite{Bernhard:2018hnz}.
The total kinetic pressure of the system is
\begin{equation}
  \label{kinetic-pressure}
  P + \Pi = \sum\limits_\mathrm{sp} g \int \frac{d^3p}{(2\pi)^3} \frac{p^2}{3E} f(p).
\end{equation}
For a given bulk pressure, we rescale the momentum $p$ inside the distribution function $f(p) \rightarrow f(p + \lambda_\mathrm{bulk}\, p)$ and adjust the parameter $\lambda_\mathrm{bulk}$ to match the total pressure on the left side of Eqn.~\eqref{kinetic-pressure}.
This substitution of course also modifies the energy density
\begin{equation}
  e = \sum\limits_\mathrm{sp} g \int \frac{d^3p}{(2 \pi)^3} E f(p),
\end{equation}
and so a fugacity term $z_\mathrm{bulk}$ is introduced which decreases the yield of all particles by the same overall factor to compensate.
The full transformation is then given by $f(p) \rightarrow z_\mathrm{bulk}\, f(p + \lambda_\mathrm{bulk}\, p)$, where the parameters $\lambda_\mathrm{bulk}$ and $z_\mathrm{bulk}$ are determined numerically for each value of the bulk pressure.

Once the fluid is converted into particles, we simulate its subsequent microscopic dynamics using the Ultra-relativistic Quantum Molecular Dynamics (\textsc{UrQMD}) transport model \cite{Bass:1998ca, Bleicher:1999xi}.
It solves the microscopic Boltzmann equation
\begin{equation}
  \frac{df_i(x, p)}{dt} = \mathcal{C}_i(x, p)
\end{equation}
where $f_i$ is the distribution function for species $i$, and $\mathcal{C}_i$ is its microscopic collision kernel.
The model propagates all produced hadrons along classical trajectories, and simulates their scatterings, resonance formations and decays until the last interactions cease.

One primary advantage of using a microscopic transport model such as \textsc{UrQMD} as an afterburner, is that it realistically simulates the system break-up when the mean free path becomes large relative to the system size.
This dilute limit is expected to play a significant role in small collision systems where the produced medium is smaller and shorter lived.


\section{Parameter estimation}
\label{sec:calibration}

The nuclear collision model constructed in Sec.~\ref{sec:model} includes a number of free parameters $\x$ which describe the initial state, pre-equilibrium dynamics, and hydrodynamic medium.
Given values for the parameters $\x$, the model may be used to predict a vector of simulated observables $\y_m$.
For example, $\y_m$ might be a vector consisting of charged particle yields in different centrality bins.
The physics model thus represents a vector-valued function $f(\x) = \y_m$ which maps the parameter values $\x$ to the simulated observables $\y_m$.

The goal of this work is to estimate the true model parameters $\x_\star$ provided some evidence that our model predictions $\y_m$ describe experimental measurements $\y_e$.
The problem involves three distinct components:
\begin{enumerate}[itemsep=0pt, leftmargin=2\parindent]
  \item $H_f$: the hypothesis that the nuclear collision model $f$ formulated in this work provides a realistic description of reality,
  \item $H_\x$: the hypothesis that the model parameters $\x$ are the true model parameters $\x_\star$ of $f$, and
  \item $E$: the evidence provided by the experimental data $\y_e$ and its corresponding uncertainties.
\end{enumerate}
As a practical matter, we always assume that hypothesis $H_f$ is correct, meaning there are no glaring flaws in our chosen theoretical framework.
This is a significant assumption; the application of hydrodynamic simulations to small collision systems is speculative, and our conclusions are conditional on the framework making sense.

Subject to this assumption, we can apply Bayes' theorem to evaluate the hypothesis $H_\x$ for the true model parameters
\begin{equation}
  \label{eq:bayes}
  P(H_\x | E) \propto P(E | H_\x)\, P(H_\x).
\end{equation}
The left-side of this expression is the \emph{posterior}: the probability for hypothesis $H_\x$ given the experimental evidence $E$.
On the right-side there are two separate terms.
The first term $P(E | H_\x)$ is the \emph{likelihood} function: the probability of observing the experimental evidence $E$ given our model and the hypothesis $H_\x$ for the true model parameters $\x_\star$, and the second term $P(H_\x)$ is the \emph{prior}: an estimate of the probability of hypothesis $H_\x$ in the absence of evidence $E$.

We assume that the likelihood function in Eqn.~\eqref{eq:bayes} is described by a multivariate normal distribution
\begin{equation}
  \label{eq:likelihood}
  P(E | H_\x) = \frac{1}{\sqrt{(2\pi)^m \det \Sigma}} \exp \left ( -\frac{1}{2}\Delta\y^\intercal \Sigma^{-1} \Delta\y \right ),
\end{equation}
where $\Delta\y = \y_m(\x) - \y_e$ is the discrepancy of the model and experiment, and $\Sigma = \Sigma_m(\x) + \Sigma_e$ is the \emph{total} covariance matrix, equal to the sum of a modeling component $\Sigma_m(\x)$ and an experimental component $\Sigma_e$ which account for all known sources of uncertainty in the simulated and measured observables.

\begin{table}[t]
  \caption{Input parameter ranges for the physics model.}
  \begin{ruledtabular}
  \begin{tabular}{lll}
    Parameter         & Description                        & Range             \\
    \paddedhline
    Norm              & Normalization factor                 & 9--28           \\
    $p$               & Entropy deposition parameter         & $-1$ to $+1$    \\
    $\sigmaf$         & Nucleon fluctuation std.\ dev.\      & 0--2            \\
    $w$               & Nucleon width parameter              & 0.4--1.2 fm     \\
    $n_c$             & Number of nucleon constituents       & 1--9            \\
    $\X$              & Nucleon structure parameter          & 0--1            \\
    $\dmin$           & Minimum inter-nucleon distance       & 0--1.7 fm       \\
    $\taufs$          & Free-streaming time                  & 0.1--1.5 fm/$c$ \\
    $\eta/s$ min      & Minimum value of $\eta/s$ (at $T_c$) & 0--0.2          \\
    $\eta/s$ slope    & Slope of $\eta/s$ above $T_c$        & 0--8 GeV$^{-1}$ \\
    $\eta/s$ crv      & Curvature of $\eta/s$ above $T_c$    & $-1$ to $+1$    \\
    $\zeta/s$ max     & Maximum value of $\zeta/s$           & 0--0.1          \\
    $\zeta/s$ width   & Width of $\zeta/s$ peak              & 0--0.1 GeV      \\
    $\zeta/s$ $T_0$   & Temperature of $\zeta/s$ maximum     & 150--200 MeV    \\
    $T_\text{switch}$ & Switching/particlization temp.       & 135--165 MeV    \\
  \end{tabular}
  \end{ruledtabular}
  \label{tab:design}
\end{table}

\subsection{Parameter design}

For the prior $P(H_\x)$, we specify ranges, i.e.\ minimum and maximum values, for each parameter which are listed in Tbl.~\ref{tab:design}.
We assume the prior distribution is constant and nonzero within each specified range and zero otherwise.
The selected parameter ranges are intentionally wide to avoid clipping the calibrated posterior; for example, a previous analysis of the \trento\ model \cite{Bernhard:2016tnd} found $p \sim 0$, but we use a prior range $p \in [-1, 1]$ to account for differences in the present model, e.g.\ nucleon substructure, which could modify its posterior.
Several of the model parameters require special care and are reparametrized accordingly:
\begin{itemize}[leftmargin=1\parindent]
  \item The constituent fluctuations, modeled by the gamma random weights $\gamma_i$ in Eqn.~\eqref{eq:fluctuated_thick}, generate overall nucleon fluctuations which are suppressed by the number of constituents $n_c$ inside the nucleon.
    The observed nucleon fluctuation variance falls like $1 / n_c$ which means the natural range for the constituent fluctuations is larger when the number of constituents is larger and \emph{vice versa}.
    We therefore reparametrize the constituent fluctuations using the standard deviation of the resulting nucleon fluctuations
    \begin{equation}
      \sigma_\mathrm{fluct} = 1 / \sqrt{k\, n_c},
    \end{equation}
    where $k$ is the shape parameter of the gamma distribution used to fluctuate each individual constituent.
  \item In Sec.~\ref{sec:initial_state} we parametrized nucleon substructure using three degrees of freedom:
    \begin{enumerate*}[label=(\roman*)]
      \item a parameter $v$ for the Gaussian width of each constituent,
      \item a parameter $r$ for the Gaussian width of the radial distribution used to sample the constituent centers, and
      \item a parameter $n_c$ to vary the number of constituents inside the nucleon.
    \end{enumerate*}
    In the limit $n_c \to \infty$, the composite nucleon's root-mean-square (RMS) radius is simply the convolution of its sampling radius $r$ and constituent width $v$ which add together in quadrature
  \begin{equation}
    \lim_{n_c \to\infty} r_\mathrm{RMS}\{\rho\} = \sqrt{r^2 + v^2}.
  \end{equation}
  We therefore choose to reparametrize the sampling radius $r$ in terms of a new variable
  \begin{equation}
    \label{eq:nucleon_width}
    w = \sqrt{r^2 + v^2},
  \end{equation}
  which approximates the RMS radius of the nucleon when the number of sampled constituents is large (see Fig.~\ref{fig:nucleon_schematic} for an example).
  We thus call $w$ a nucleon ``width'', although for smaller numbers of constituents, the \emph{actual} RMS radius of our sampled nucleons can be significantly smaller than our width parameter $w$ due to fluctuations in the nucleon's center of mass, and hence one should account for the difference when discussing the nucleon's posterior RMS radius.

\begin{figure}[t]
  \begin{tikzpicture}
    \draw[dashed] (0,0) circle (1.6cm);
    \foreach \theta in {60, 180, 300}{
      \draw ({\theta}:.8) circle (.65cm);
    }
    \coordinate (a) at (0, 0);
    \coordinate (b) at (.8, 1.385);
    \coordinate (c) at (.4, -.693);
    \coordinate (d) at (-1.45, 0);
    \coordinate (e) at (-.8, 0);
    \draw[<->] (a) -- (b) node[midway, xshift=-1ex, yshift=.8ex]{$w$};
    \draw[<->] (a) -- (c) node[midway, xshift=1ex, yshift=.8ex]{$r$};
    \draw[<->] (d) -- (e) node[midway, yshift=1ex]{$v$};
  \end{tikzpicture}
  \caption{Schematic illustrating the constituent sampling radius $r$, constituent width $v$ and nucleon width parameter $w = \sqrt{r^2 + v^2}$ for a nucleon with $n_c=3$ constituents.}
  \label{fig:nucleon_schematic}
\end{figure}

\begin{table*}
  \caption{
    \label{tab:observables}
    Experimental data used to calibrate the model.
  }
  \begin{ruledtabular}
  \begin{tabular}{cc}
    Pb-Pb $\sqrts=5.02$~TeV & $p$-Pb $\sqrts=5.02$~TeV \\
    \paddedhline
    Charged-particle multiplicity $d\nch/d\eta$, $|\eta| < 0.5$ \cite{Adam:2015ptt} & Charged-particle multiplicity $d\nch/d\eta$, $|\eta| < 1.4$ \cite{Adam:2014qja} \\
    \noalign{\smallskip}
  Two-particle flow cumulants  $\vnk{n}{2}$ for $n=2,3,4$, $|\eta| < 0.8$,  & Two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3$, $|\eta| < 2.4$, \\
    charged-particles, $|\Delta\eta| > 1$,\, $0.2 < p_T < 5.0$~GeV \cite{Adam:2016izf} & charged-particles, $|\Delta\eta| > 2$,\, $0.3 < p_T < 3.0$~GeV \cite{Chatrchyan:2013nka}\\
    \noalign{\smallskip}
    & Charged-particle mean $p_T$, $0.15 < p_T < 10$~GeV, $|\eta| < 0.3$ \cite{Abelev:2013bla}\\
  \end{tabular}
  \end{ruledtabular}
\end{table*}

\item Equation~\eqref{eq:nucleon_width} requires the nucleon width to be larger than the constituent width, i.e. $w > v$, lest the sampling radius $r$ turn imaginary, and thus we cannot vary the nucleon width $w$ and the constituent width $v$ independently.
  We therefore reparametrize the constituent width $v$ using a new variable $\X$ to interpolate between minimum and maximum allowed values:
  \begin{equation}
    \label{eq:struct_param}
    v = v_\mathrm{min} + \X (v_\mathrm{max} - v_\mathrm{min}).
  \end{equation}
  Here we choose a minimum constituent width $v_\mathrm{min} = 0.2$~fm determined by computational limits and a maximum width $v_\mathrm{max} = w$ equal to the nucleon width.
  Thus for $\X=0$, the nucleons consist of small, distinct hot spots, whereas for $\X=1$ the nucleon is a single Gaussian blob of width $w$.
  The parameter $\X$ thus varies the granularity of the nucleon while keeping the number of constituents $n_c$ and approximate size of the nucleon $w$ fixed.
\end{itemize}

\subsection{Observables}
\label{sec:observables}

The likelihood function \eqref{eq:likelihood} provides evidence for \mbox{(or against)} the model parameters $\x$ by comparing the model predictions $\y_m(\x)$ to experimental data $\y_e$.
We focus on simple experimental observables in the present study which are sensitive to the bulk properties of the produced medium.
We calculate for each set of model parameters the following observables at midrapidity:
\begin{itemize}[leftmargin=1\parindent, itemsep=0pt]
  \item Charged-particle multiplicity $d\nch/d\eta$.
  \item Identified particle yields $dN/dy$ of pions, kaons, and protons.
  \item Transverse energy production $dE_T/d\eta$.
  \item Charged particle mean transverse momentum $\langle p_T \rangle$ ($0.15 < p_T < 10$~GeV).
  \item Identified particle mean transverse momentum $\langle p_T \rangle$ of pions, kaons and protons.
  \item Mean transverse momentum fluctuations $\delta p_T / \langle p_T \rangle$ (charged particles, $0.15 < p_T < 2.0$~GeV).
  \item Two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3,4$\\ (charged particles, $0.2 < p_T < 5.0$~GeV, and \\$0.3 < p_T < 3.0$~GeV).
  \item Four-particle flow cumulant $\vnk{2}{4}$ \\(charged particles, $0.2 < p_T < 5.0$).
  \item Symmetric cumulants $\mathrm{SC}(4, 2)$ and $\mathrm{SC}(3,2)$.
\end{itemize}
Each observable is calculated from the list of final state particles produced by \textsc{UrQMD} using the same methods applied by experiment.
We generally match the kinematic cuts of all measurements with two exceptions: we use use a larger rapidity interval $|\eta| < 0.8$ than experiment for some boost-invariant observables to improve our finite particle statistics, and we do not apply a rapidity gap, e.g.\ $|\Delta \eta| > 1$, between pairs of particles when calculating the two-particle cumulant $\vnk{n}{2}$ since we already oversample particles from each hydrodynamic event, and this oversampling suppresses non-flow correlations.

At the time of this writing, many of the aforementioned experimental observables are not yet published for $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV.
We therefore restrict our calibration to the subset of measured and published observables listed in Tbl.~\ref{tab:observables}.
Notably absent from this list are the four-particle cumulants $\vnk{n}{4}$ at $\sqrts=5.02$~TeV despite being measured and published.
Unfortunately, the four-particle cumulants require minimum-bias event statistics an order of magnitude larger than those used in this work.
We therefore refrain from \emph{calibrating} on the four-particle cumulants, although we do show calculations of the four-particle cumulant $\vnk{2}{4}$ later in the text, using a single set of calibration parameters.

Most of the calibration observables listed in Tbl.~\ref{tab:observables} are calculated as a function of collision centrality, where centrality is defined using some measure of the underlying event activity, e.g.\ the charged particle yield in a given rapidity window.
When calculating these observables, we generate \order{4} minimum-bias events at each design point and divide the events into centrality bins using the charged particle yield at midrapidity, similar to the procedure used by experiment.

However, for some observables such as $p$-Pb mean $p_T$ \cite{Abelev:2013bla} and flow cumulants $\vnk{n}{k}$ \cite{Chatrchyan:2013nka}, the experiments use a special high-multiplicity trigger to select rare, ultra-central events according to the number of charged particles produced or detector tracks offline.
These high-multiplicity bins are too selective for our modest minimum-bias event sample, and so a different procedure is required.
We exploit, for this purpose, the approximate correspondence
\begin{equation}
  \label{eq:bin_equiv}
  \ntrk \sim \nch \propto N_\rho
\end{equation}
between each event's initial secondary-parton density ${N_\rho = \int d^2x\,n(\x)}$ in Eqn.~\eqref{energy-momentum} and its final-state event activity, characterized by $\nch$ or $\ntrk$ \cite{Abelev:2013bla, Chatrchyan:2013nka}.

Consider, for example, an experimental multiplicity bin $(\nch^\text{low}, \nch^\text{high})$ with some kinematic cuts on $p_T$ and $\eta$.
We rescale this bin by the average multiplicity $\langle \nch \rangle$ of the corresponding minimum-bias event sample, i.e.\
\begin{equation}
  (\nch^\text{low}, \nch^\text{high}) \rightarrow (\nch^\text{low} / \langle \nch \rangle, \nch^\text{high} / \langle \nch \rangle)
\end{equation}
in order to reexpress each bin edge as a unitless variable.
These rescaled bins are then used to select initial condition events using the equivalence \eqref{eq:bin_equiv}:
\begin{equation}
  \label{eq:mult_trigger}
  \left (\frac{N^\text{low}_\rho}{\langle N_\rho \rangle}, \frac{N^\text{high}_\rho}{\langle N_\rho \rangle} \right) \leftrightarrow \left (\frac{\nch^\text{low}}{\langle \nch \rangle}, \frac{\nch^\text{high}}{\langle \nch \rangle} \right ).
\end{equation}

\begin{figure*}[t]
  \includegraphics{correlation_matrices}
  \caption{Visualization of the Pb-Pb correlation matrix $\mathrm{corr}(y_i, y_j) = \mathrm{cov}(y_i,y_j)/(\sigma_i \sigma_j)$ for the model (emulator) at a random point in parameter space (left-side) and for the experimental data (right-side). Each cell represents an observable in a single centrality bin. Experimental statistical and systematic errors are from ALICE \cite{Adam:2015ptt, Adam:2016izf}. The experimental correlation structure is modeled using Eqn.~\eqref{eq:corr}.}
  \label{fig:correlation}
\end{figure*}

Finally, we mimic the method used by experiment and apply \eqref{eq:mult_trigger} to select rare high-multiplicity events from a continuous stream of minimum-bias \trento\ events satisfying the correct relative multiplicity bin edges.
This of course means that, in addition to running a large sample of minimum-bias events for centrality binned observables, we must also generate (much like experiment) a separate sample of multiplicity triggered events.
In practice, we use a few hundred to a few thousand events per multiplicity bin, depending on the type of observable.

We also take stock of the statistical and systematic errors reported by each experiment and incorporate their uncertainty into the likelihood covariance matrix
\begin{equation}
  \Sigma = \Sigma_m + \Sigma_e
\end{equation}
in Eqn.~\eqref{eq:likelihood}, which includes uncertainty contributions from both the model $\Sigma_m$ and experimental data $\Sigma_e$.
The experimental contribution to the covariance $\Sigma_e$ is further broken down into statistical and systematic components,
\begin{equation}
  \Sigma_e = \Sigma_e^\text{stat} + \Sigma_e^\text{sys}.
\end{equation}
The statistical errors in $\Sigma_e^\text{stat}$ are uncorrelated and thus its covariance matrix is diagonal:
\begin{equation}
  \Sigma_e^\text{stat} = \text{diag}[(\sigma^\text{stat}_1)^2, (\sigma^\text{stat}_2)^2, \dots (\sigma^\text{stat}_m)^2 ],
\end{equation}
where $\sigma^\text{stat}_i$ is the statistical uncertainty of observable $y_i$ in the experimental observable vector $\y_e = [y_1, \dots, y_m]$.
The systematic errors, meanwhile, are typically correlated, but the correlation structure is not reported by the experiments so we assert a reasonable form.
We can expand the systematic covariance matrix as
\begin{equation}
  (\Sigma_e^\text{sys})_{ij} =  \rho_{ij} \sigma_i \sigma_j,
\end{equation}
where $\sigma_i$ and $\sigma_j$ are the systematic errors of observables $y_i$ and $y_j$ respectively, and $\rho_{ij}$ is the Pearson correlation coefficient between observable $y_i$ and $y_j$:
\begin{equation}
  \rho_{ij} = \frac{\text{cov}(y_i, y_j)}{\sigma_i \sigma_j},
\end{equation}
which satisfies $\rho_{ij}=1$ for $i=j$ and $|\rho_{ij}| \le 1$ for $i \ne j$.
We assume that each observable is correlated across different centrality/multiplicity bins, and uncorrelated with observables of a different type, e.g.\ correlations between yields and flows.
This is a crude simplifying assumption but it is better than neglecting the correlation structure of the experimental data entirely.

For the correlation structure between different observable bins, we assert a simple Gaussian form
\begin{equation}
  \label{eq:corr}
  \rho_{ij}^\text{sys} = \exp \left[ -\frac{1}{2} \left(\frac{b_i - b_j}{l} \right)^2 \right],
\end{equation}
where $b_i$ and $b_j$ are the midpoints of two observable bins of a single type (centrality or relative multiplicity) and $l$ is a correlation length which describes how quickly the observable bins decorrelate as the distance between the bins increases.
We use centrality correlation lengths $l=100$ for all of the centrality-binned Pb-Pb observables and $l=30$ for the centrality-binned $p$-Pb charged particle yield $d\nch/d\eta$.
The \mbox{$p$-Pb} mean $p_T$ and flow observables, meanwhile, use relative multiplicity bins $\nch / \langle \nch \rangle$ and $\ntrk / \langle \ntrk \rangle$ which necessitate a smaller correlation length $l=5$.

We show, as an example, the resulting correlation matrix
\begin{equation}
  \mathrm{corr}(y_i, y_j) = \mathrm{cov}(y_i, y_j)/(\sigma_i \sigma_j)
\end{equation}
for the Pb-Pb experimental data constructed using Eqn.~\eqref{eq:corr} on the right-side of Fig.~\ref{fig:correlation}.
Here $y_i$ denotes an element of the experimental data $\y_e$ and $\sigma_i$ its corresponding uncertainty.
The correlation matrix is block diagonal, with each block representing the correlations within a single class of observable.

\subsection{Model emulator}

In principle, one could calculate the likelihood function in Eqn.~\eqref{eq:likelihood} directly, e.g.\ by running the model a large number of times at a given parameter point $\x$ to calculate the model observables $\y_m(\x)$ from the ensemble of simulated events, but in practice such a procedure would be intractable.
The model is computationally intensive to evaluate, and thousands of events are required to calculate the simplest observables at a single parameter point.
Moreover, we need to evaluate Eqn.~\eqref{eq:likelihood} \emph{numerous} times in order to sample the multidimensional posterior distribution so that the samples may be histogrammed and visualized.

We therefore follow an established framework for computationally intensive models and train an emulator to act as a fast surrogate for the full physics simulation \cite{OHagan:2006ba, Higdon:2008cmc, Higdon:2014tva}.
The emulator enables essentially instantaneous predictions for $\y_m = f(\x)$ and allows us to sample the posterior distribution millions of times.
In order to train the emulator, we first generate a scaffolding of the parameter space using a maximin Latin hypercube design \cite{Morris:1995lh} to distribute 500 points uniformly throughout our 15-dimensional parameter space according to the parameter ranges in Tbl.~\ref{tab:design}.
We then run minimum-bias and multiplicity triggered $p$-Pb and Pb-Pb events at each design point and calculate the model observables from the ensemble of events.

Specifically, let $X$ denote the ${d \times n}$ design matrix of $d=500$ training points, where each training point is a vector $\x = (x_1, x_2, \dots, x_n)$ of the $n=15$ model parameters in Tbl.~\ref{tab:design}.
Similarly, let $Y$ denote the corresponding ${d \times m}$ observables matrix, where each row of $Y$ is a vector $\y_m = (y_1, y_2, \dots, y_m)$ of $m$ different simulated observables (here we overload our notation so the subscript $m$ means both \emph{model} and \emph{observable number}).
Our goal is to train an emulator for the physics model $f$ using the discrete observations $f: X \mapsto Y$.

We use for this purpose a specific type of emulator known as a Gaussian process emulator (GPE) \cite{Rasmussen:2006gp}.
The advantage of using GPE's is that they provide an estimate of their own uncertainty which allows us to account for this uncertainty when constructing the covariance matrix $\Sigma$ in Eqn.~\eqref{eq:likelihood}.
One quirk of GPE's is that they are restricted to scalar-valued functions, i.e.\ functions of one output, whereas we require an emulator for vector-valued functions with multiple outputs.
This restriction is commonly circumvented using principal component analysis (PCA): a general procedure which transforms a set of correlated variables $\y = (y_1, y_2, \dots, y_m)$ into a new basis representation $\z = (z_1, z_2, \dots, z_m)$ where the linear correlations between $z_i$, $z_j$ vanish for all $i\ne j \in m$ \cite{Tipping:1999}.

We first preprocess our model observables by centering and scaling each column of $Y$ (single observable) to zero mean and unit variance to generate a standardized observable matrix $\tilde{Y}$.
PCA is then used to reexpress each row-vector $\tilde{\y}$ of $\tilde{Y}$ (all standardized observables at a single design point) in the new principal component basis:
\begin{equation}
  \tilde{\y}_i = \sum\limits_{j=1}^m z_{ij} \mathbf{v_j},
\end{equation}
where $\tilde{\y}_i$ are the standardized observables of the $i$-th row-vector (design point) of matrix $\tilde{Y}$, and $z_{ij}$ and $\mathbf{v}_j$ are the coefficients and corresponding vectors of its $j$-th principal component.

The principal components are reported in order of explained variance, with the first principal component vector $\mathbf{v}_1$ accounting for the most variance in $\tilde{Y}$, and the last principal component vector $\mathbf{v}_m$ accounting for the least.
We then train a set of independent GPE's $z_i = \mathrm{gp}_i(\x)$ for $i=1$ to $k$ to predict the principal components $(z_1, \dots, z_k)$ of the first $k$ principal component vectors as a function of the model parameters $\x$ which vary across the design $X$.
For the present study, we use $k=7$ principal components when emulating the $p$-Pb system and $k=8$ principal components when emulating Pb-Pb, chosen to describe 99.5\% of the total observed variance of each system.

The GPE's are essentially fancy interpolators applied to the model's training points and PCA transformed observables.
Each GPE reports a mean value $z(\x)$ as well as an estimated error $\delta z(\x)$ which accounts for statistical noise in the training data and interpolation error between the design points.
Once the GPE's are trained, we can predict the model observables $\y_m$ at parameter point $\x$ by transforming the vector of emulated principal components
\begin{equation}
  \mathbf{z}(\x) = (z_1(\x), z_2(\x), \dots, z_k(\x))
\end{equation}
\emph{back} to physical space.
Similarly, we can construct the covariance matrix of the observables in PCA space,
\begin{equation}
\mathrm{cov}(z_i, z_j) = \mathrm{diag}[(\delta z_{1})^2, (\delta z_2)^2, \dots, (\delta z_k)^2 ],
\end{equation}
and transform it back to physical space as well to obtain the covariance matrix of the model observables $\y_m$ at a given parameter point $\x$.

The resulting emulator therefore predicts both a mean prediction $\y_m(\x)$ and an uncertainty covariance matrix $\Sigma_m(\x)$ which accounts for multiple sources of model and emulator uncertainty, including the truncation error expected from using a finite number of principal components $k < m$.
\begin{fullpage}
{\centering\includegraphics{observables_pbpb}\par}
\captionof{figure}{\label{fig:obs_pbpb} Simulated observables compared to experimental data for Pb-Pb collisions at $\sqrts=5.02$~TeV. Top row: explicit model calculations (no emulator) for each of the $d=500$ design points; bottom row: emulator predictions for $n=100$ random samples drawn from the posterior. Points with error bars are experimental data from ALICE with statistical and systematic errors added in quadrature \cite{Adam:2015ptt, Adam:2016izf}.}
\vspace{.5cm}
{\centering\includegraphics{observables_ppb}\par}
\captionof{figure}{\label{fig:obs_ppb} Same as Fig.~\ref{fig:obs_pbpb} but for $p$-Pb collisions at $\sqrts=5.02$~TeV. Note that multiplicity bins are used for mean $p_T$ and flow cumulant observables to match the bins used by experiment. Experimental data are from ALICE \cite{Adam:2014qja, Abelev:2013bla} and CMS \cite{Chatrchyan:2013nka}.}
\end{fullpage}
The model covariance matrix $\Sigma_m$ includes so-called \emph{known}-unknowns such as statistical error and emulator interpolation error, but not \emph{unknown}-unknowns such as the overall validity of small-system hydrodynamics, i.e.\ things which lack a unified consensus or are difficult to quantify.
We show in Fig.~\ref{fig:correlation} the resulting Pb-Pb correlation matrix $\mathrm{corr}(y_i,y_j)$ for the model (emulator) at a random parameter point $\x$ in the design space (left-side), along side the same correlation matrix for the experimental data (right-side) discussed previously.
For additional information on the model emulator, we direct the reader to Appendix~\ref{app:validation} which includes several validation tests of the emulator prediction accuracy.

\subsection{Bayesian calibration}

In order to calibrate the model on two different collision systems, we expand the likelihood function \eqref{eq:likelihood} into a joint likelihood
\begin{equation}
  \label{eq:joint_likelihood}
  P(E | H_\x) = P(E_\text{Pb-Pb} | H_\x) \cdot P(E_\text{$p$-Pb} | H_\x),
\end{equation}
where $E$ subsumes all evidence from the $p$-Pb and Pb-Pb collision systems and $H_\x$ is our hypothesis that $\x$ equals the true parameters $\x_\star$.

We then perform Markov-chain Monte Carlo (MCMC) importance sampling on the posterior distribution in Eqn.~\eqref{eq:bayes} to draw random samples for $P(H_\x | E)$, the estimate of the true model parameters given the model and the experimental data \cite{Goodman:2010en, FM:2013mc}.
For this we use an affine-invariant ensemble sampler which uses a large ensemble of interdependent walkers \cite{Goodman:2010en, FM:2013mc} and allow the MCMC chain to ``burn-in'' before generating \order{7} posterior samples.


\section{Results}
\label{sec:results}

We show simulated and emulated model observables (thin colored lines) for Pb-Pb collisions (Fig.~\ref{fig:obs_pbpb}) and \mbox{$p$-Pb} collisions (Fig.~\ref{fig:obs_ppb}) at $\sqrts=5.02$~TeV compared to experimental data from CMS \cite{Chatrchyan:2013nka} and ALICE \cite{Adam:2015ptt, Adam:2016izf, Adam:2014qja, Abelev:2013bla}.
The top row of each figure shows explicit model calculations at each of the $d=500$ design points (training data), while the bottom row shows emulator predictions for $n=100$ random parameter samples drawn from the Bayesian posterior (sampled from the MCMC chain).
Each column shows a different class of observable.
The charged-particle yield $d\nch/d\eta$ is shown on the left, mean $p_T$ is in the middle, and two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3,4$ are on the right.
The Pb-Pb mean $p_T$ and $p$-Pb $\vnk{4}{2}$ datasets are missing and hence are omitted from the present calibration.

Notice the large spread of the observables calculated at the training points (top row of each figure).
The design is constructed to vary each parameter across a wide range of values, specified in Tbl.~\ref{tab:design}, and hence the corresponding model calculations are equally uncertain.
We also point out that there is considerably more variance in the $p$-Pb training data than the Pb-Pb training data.
The $p$-Pb yields, mean $p_T$ and flow cumulants all vary wildly within the chosen parameter ranges.
For instance, we can turn the $p$-Pb flows completely \emph{off} with suitably chosen parameters which is not possible in the Pb-Pb system.
Evidently the $p$-Pb model predictions are far more sensitive to modeling uncertainties.

Conversely, the calibrated emulator predictions (bottom row of Figs.~\ref{fig:obs_pbpb} and \ref{fig:obs_ppb}) are far better constrained and nicely track the experimental data points.
We emphasize here that the posterior parameter values are obtained from a \emph{simultaneous} calibration to $p$-Pb and Pb-Pb data, and thus they are self-consistent between the two systems.
The spread in the posterior samples reflects different sources of model and experimental uncertainty as well as tension in the optimal fit parameters which describe each observable.
We demonstrate later in the text that a single set of model parameters well describes all of the calibration data, and thus we believe that much of the spread in the posterior samples is uncertainty contributed by our emulator.
We also note that although the $p$-Pb posterior samples have a somewhat larger spread than the Pb-Pb samples, the percentage uncertainty of the $p$-Pb emulator is similar to that of the Pb-Pb emulator, and thus the difference is likely due to the larger variance of the $p$-Pb training data.
The uncertainty in the posterior distribution could thus be improved by running the calibration with more design points or with a narrower range of parameter values to increase the density of the training points and reduce interpolation uncertainty.

We now direct our attention to Fig.~\ref{fig:posterior} which shows the main result of this work, the posterior distribution of the model input parameters.
Recall that the posterior $P(H_\x | E)$ is the probability that our hypothesis $H_\x$ for the true model parameters $\x_\star$ is correct, given the evidence $E$ provided by experiment.
The present posterior has 15 dimensions, one for each parameter listed in Tbl.~\ref{tab:design}, and thus its joint distribution cannot be summarized by one figure alone.
We thus sample the distribution and histogram the samples to project the distribution onto one or two dimensions at a time.
Each diagonal panel is the distribution of a \emph{single} model parameter (marginalized over all others), and each lower-diagonal panel is the joint distribution of a \emph{pair} of model parameters (marginalized over all others).

The posterior in Fig.~\ref{fig:posterior} includes a wealth of information.
Here we focus on its most important features, starting with the top-left corner of the figure and working our way to the bottom-right.
We report posterior estimates for individual parameters using their (marginalized) distribution medians and 90\% credible intervals.
For example, $x=2.45_{-0.15}^{+0.20}$ corresponds to a posterior median $\tilde{x}=2.45$ and 90\% credible interval $x \in (2.30, 2.65)$.

\begin{fullpage}
\begin{figure}
  \makebox[\textwidth]{\includegraphics[width=.95\paperwidth]{posterior}}
\end{figure}
\captionof{figure}{\label{fig:posterior} Bayesian posterior distribution of the model input parameters. The diagonal panels show the marginalized distributions of individual model parameters, while off-diagonal panels show the joint distributions for pairs of model parameters, visualizing their correlations. The marginalized distribution medians and 90\% credible intervals are annotated along the diagonal.}
\end{fullpage}

\subsection{Initial condition properties}

\begin{figure}
  \includegraphics[width=\columnwidth]{posterior_proton_shape}
  \caption{Posterior distribution for the nucleon width $w$ and constituent width $v$. The white trapezoidal region (lower-right) covers the initial range of allowed values for $w$ and $v$, while the gray triangular region (upper-left) indicates values which are excluded by the prior $v > w$. The posterior distribution, shown in blue, indicates the preferred values for $w$ and $v$ determined by the analysis.}
  \label{fig:posterior_proton_shape}
\end{figure}

The \trento\ normalization factor ${\text{Norm} = 20.3^{+2.2}_{-2.2}}$ and generalized mean parameter $p=0.018^{+0.146}_{-0.159}$ are well constrained by the present analysis.
Moreover, their posterior values nicely describe the $p$-Pb and Pb-Pb calibration observables in Figs.~\ref{fig:obs_pbpb} and \ref{fig:obs_ppb}.
While it would not be surprising, for example, to fit one or two of these observables using such a narrow range of values, the quality of the combined fit and the number of observables described is highly non-trivial.
For example, consider the ratio of the $p$-Pb yield over the Pb-Pb yield, which imposes a strong constraint on physically reasonable initial condition models.
As the entropy deposition parameter $p$ trends toward positive (negative) infinity, particle production scales like the local maximum (minimum) of the nuclear overlap density.
The parameter $p$ thus strongly affects the $p$-Pb and Pb-Pb yield ratio.
It just so happens that the small range of $p$-values needed to describe this yield ratio are the same values needed to describe all of the calibration observables in the present study, \emph{and} numerous Pb-Pb observables at $\sqrts=2.76$~TeV \cite{Bernhard:2018hnz}.
This work thus reaffirms an empirical scaling law reported in several previous studies \cite{Moreland:2014oya, Bernhard:2016tnd, Ke:2016jrd, Bernhard:2018hnz}, that entropy deposition at midrapidity in ultrarelativistic nuclear collisions scales approximately as
\begin{equation}
  \label{eq:geometric_mean}
  \frac{dS}{dy}\bigg\vert_{y=0} \propto \sqrt{\T_A \T_B},
\end{equation}
where $\T_A$ and $\T_B$ are the participant thickness functions \eqref{eq:fluctuated_thick} of each nucleus.
We emphasize that this specific analytic form should not be interpreted too literally.
For instance, a generalized mean described by $p=0.05$ is well within our 90\% credible interval, but it is not equal to the geometric mean in Eqn.~\ref{eq:geometric_mean}.
This approximate form nevertheless has been shown to mimic the scaling behavior of several saturation-based theory calculations in high-energy QCD \cite{Bernhard:2016tnd}, and thus appears consistent with theoretical expectations.

\begin{figure*}
  \includegraphics[width=\textwidth]{region_shear_bulk}
  \caption{Left figure: estimated temperature dependence of the QGP specific shear viscosity $(\eta/s)(T)$ determined by the present Bayesian analysis of $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV (orange line/band) compared to a previous Bayesian analysis of Pb-Pb collisions at $\sqrts=2.76$ and 5.02~TeV (blue line/band) \cite{Bernhard:2018hnz}. The lines are the medians of each posterior distribution, and the bands are their 90\% credible regions. Right figure: same as before, but for the temperature dependence of the QGP specific bulk viscosity $(\zeta/s)(T)$.}
  \label{fig:region_shear_bulk}
\end{figure*}

Continuing down the diagonal in Fig.~\ref{fig:posterior}, we see that the nucleon width parameter $w=0.97_{-0.17}^{+0.19}$~fm.
We caution that this parameter $w$ is \emph{not} the RMS radius of our nucleons due to idiosyncrasies of the constituent sampling procedure.
We can, however, easily calculate the RMS radius for a specific nucleon width $w$, constituent width $v$ and constituent number $n_c$.
For example, the single highest posterior probability region of the parameter space prefers a nucleon width $w=0.92$~fm, constituent width $v=0.43$~fm and constituent number $n_c=6$.
The corresponding RMS nucleon radius for these parameters is $R_n = 0.86$~fm, conspicuously close the proton's RMS electric charge radius $R_p = 0.879(8)$~fm \cite{Bernauer:2010wm}.
This is perhaps the single largest difference between our work and the conclusions of recent saturation-based calculations which constrained the event-by-event fluctuations of the proton using a color-dipole picture of vector meson production \cite{Mantysaari:2016ykx, Mantysaari:2016jaz}.
Those studies find that the measured coherent and incoherent $J/\Psi$ spectra at HERA prefer a compact gluon distribution inside each nucleon, with an RMS radius $R_g \approx 0.4$~fm, which is roughly \emph{half} the analogous RMS nucleon width preferred by our analysis.

Meanwhile, our posterior on the constituent number $n_c$ is not sharply peaked.
We therefore refrain from quoting a distribution median and 90\% credible interval, although we do note that the distribution clearly favors $n_c > 1$ constituents.
This is not surprising.
The \trento\ model mimics saturation-based initial condition models \cite{Bernhard:2016tnd}, and saturation models tend to produce ``proton-sized'' fireballs in $p$-Pb collisions \cite{Bzdak:2013zma}.
When the proton is spherically symmetric, the resulting proton-sized QGP is also largely symmetric and thus produces very little flow.
Saturation-based models are therefore unable to describe the significant flow measured in high-multiplicity $p$-Pb collisions without nucleon substructure, or alternatively, some other source of additional correlations \cite{Schenke:2017bog}.

The posterior on the nucleon substructure parameter $\X=0.34_{-0.17}^{+0.22}$, on the other hand, is particularly sharply peaked.
Recall that this parameter, defined in Eqn.~\eqref{eq:struct_param}, interpolates between the minimum and maximum widths of each constituent and hence
two different limits for the granularity of the nucleon.
When $\X=0$, the nucleon is populated by $n_c$ small compact hot-spots, and when $\X=1$ the hot-spots are large and fully overlapping, restoring spherical symmetry.
We use the parameter $\X$ purely for calibration purposes and then transform the variable back to a constituent width $v$ for discussion purposes.

We plot the joint posterior distribution for the nucleon width parameter $w$ and constituent width $v$ in Fig.~\ref{fig:posterior_proton_shape}.
The white trapezoidal region in the lower-right is the joint parameter prior, and the gray triangular region in the upper-left where $v > w$ is a region that we exclude from the prior.
The posterior distribution, shown in blue, is remarkably well constrained.
We report a posterior estimate for the constituent width $v=0.46_{-0.15}^{+0.21}$~fm which is roughly the same magnitude as the \emph{nucleon} hot-spots used in recent saturation-based substructure studies that employed IP-Glasma initial conditions \note.
Evidently, it may be necessary to place an informative prior on our nucleon substructure parameters in order to resolve the apparent tension between our parameter values and those needed to describe DIS measurements at HERA.
Alternatively, it is also possible that the fluctuations probed by coherent and incoherent $J/\Psi$ production are different than those probed by minimum-bias particle production.

\subsection{Transport properties}

In this section, we compare several of our posterior estimates to those obtained from a similar Bayesian analysis \cite{Bernhard:2018hnz} which used an (almost) identical version of the present physics model.
The only modeling difference is the inclusion of nucleon substructure in this work which was absent in Ref.~\cite{Bernhard:2018hnz}.
Several calibration details, however, are different between the two studies.
The present work uses a much smaller number of $p$-Pb and Pb-Pb observables at $\sqrts=5.02$~TeV (limited by availability), whereas Ref.~\cite{Bernhard:2018hnz} used a larger cross section of Pb-Pb experimental data at $\sqrts=2.76$ and 5.02~TeV.

The posterior free-streaming time $\taufs=0.37_{-0.27}^{+0.34}$~fm/$c$ obtained in this work is significantly smaller than our previous estimate $\taufs=1.16_{-0.25}^{+0.29}$~fm/$c$ in Ref.~\cite{Bernhard:2018hnz}.
We point out that the present study is missing several important observables which could affect the estimated free-streaming time, e.g.\ the Pb-Pb mean $p_T$ and mean $p_T$ fluctuations at $\sqrts=5.02$~TeV.
Nevertheless, it appears that the inclusion of nucleon substructure significantly reduces the maximum allowed free-streaming time, although more work is needed to establish if this is indeed the case.

We also compare in Fig.~\ref{fig:region_shear_bulk} our estimates for the temperature dependence of the QGP specific shear viscosity $(\eta/s)(T)$ and bulk viscosity $(\zeta/s)(T)$ with those of Ref.~\cite{Bernhard:2018hnz}.
The lines are the distribution medians, and the bands are their 90\% credible regions.
The results of this work are shown in orange, and the results of Ref.~\cite{Bernhard:2018hnz} are shown in blue.
In general, our estimates are broader and less certain but otherwise self-consistent.
Evidently, the combined analysis of Pb-Pb data at $\sqrts=2.76$ and 5.02~TeV in Ref.~\cite{Bernhard:2018hnz} provides a better constraint on the QGP viscosities which is not surprising given the additional observables and multiple beam energies studied.
The $p$-Pb data, meanwhile, does not appear to provide any unique viscous constraints.


\subsection{Verification of high-probability parameters}

In this section we select a single set of high-probability model parameters

We verify the emulator predictions and model accuracy by running a large number of $p$-Pb and Pb-Pb events using a single set of high-probability parameters selected from the mode of the posterior distribution.
These parameters approximate the ``best fit'' values
We then calculate all of the model observables listed in Sec.~\ref{sec:observables}.
%These ``best fit'' model predictions are plotted in Fig.~\ref{fig:obs_map} alongside the experimental data from CMS \cite{Chatrchyan:2013nka} and ALICE \cite{Adam:2015ptt, Adam:2016izf, Adam:2014qja, Abelev:2013bla}.
%They are explicit model calculations; \emph{there is no emulator}.
%

\begin{table}[t]
  \caption{
    \label{tab:mode_params}
    High-probability parameters selected from the posterior distribution and used to generate Fig.~\ref{fig:obs_map}.
    The posterior distribution on the particlization temperature $T_\text{switch}$ is flat (agnostic), so we fix it's value using Ref.~\cite{Bernhard:2018hnz}.
  }
  \begin{ruledtabular}
    \begin{tabular}{ll@{\hspace{2em}}ll}
      \multicolumn{2}{c}{Initial condition} & \multicolumn{2}{c}{QGP medium} \\
      \paddedhline
      Norm     & 20.            & $\eta/s$ min      & 0.11           \\
      $p$      & 0.0            & $\eta/s$ slope    & 1.6 GeV$^{-1}$ \\
      $k$      & 0.19           & $\eta/s$ curv     & -0.29          \\
      $n_c$    & 6              & $\zeta/s$ max     & 0.032          \\
      $w$      & 0.92 fm        & $\zeta/s$ width   & 0.024 GeV      \\
      $v$      & 0.43 fm        & $\zeta/s$ $T_0$   & 175 MeV        \\
      $\dmin$  & 0.81 fm        & $T_\text{switch}$ & 151 MeV        \\
      $\taufs$ & 0.37 fm/$c$
    \end{tabular}
  \end{ruledtabular}
\end{table}

\begin{fullpage}
{\centering\includegraphics{observables_map}\par}
\captionof{figure}{\label{fig:obs_map} Model calculations using the \emph{maximum a posterior} (MAP) parameters compared to experiment. Colored lines are model calculations for $p$-Pb collisions (left) and Pb-Pb collisions (right) at $\sqrts=5.02$~TeV. Points with error bars are the experimental data with statistical uncertainties, and gray bands their corresponding systematic uncertainties, from CMS \cite{Chatrchyan:2013nka} and ALICE \cite{Adam:2015ptt, Adam:2016izf, Adam:2014qja, Abelev:2013bla}. The sub-axes show the ratio of model over data where available with gray bands indicating $\pm 10\%$.}
\end{fullpage}

\begin{figure}
  \includegraphics{flow_corr}
  \caption{Model calculations of the symmetric cumulants for Pb-Pb collisions at $\sqrts=5.02$~TeV using the \emph{maximum a posteriori} (MAP) parameters.}
  \label{fig:flow_corr}
\end{figure}

\section{summary}
\label{sec:summary}

In this work, we constructed a multi-stage hybrid transport model for relativistic nuclear collisions and calibrated free parameters of the model using Bayesian parameter estimation.
Specifically, we focused on improving the model's description of small collision systems by investigating the effect of nucleon substructure on bulk observables in $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV.

The analysis was performed using a parametric data-driven approach to account for theoretical uncertainty (variability) in the QGP initial conditions and hydrodynamic transport coefficients using a handful of equally uncertain model parametersi.
We marginalized over the theoretical uncertainty in the parameter using a handful of equally uncertain model parameters to vary the size and shape of the nucleon, the scaling behavior of initial QGP entropy deposition, the duration of pre-equilibrium dynamics, and the temperature dependence of the hydrodynamic shear and bulk viscosities.

We placed a conservative prior on the ``true'' value of each parameter, and calculated a global likelihood

varied each parameter within its design range to explored the full 15-dimensional parameter space of our model.

constructed an emulator to interpolate the predictions of our model at arbitrary regions of parameter space.

Bayesian inference was then used to compare the predictions of the model to experimental data from CMS and ALICE, in order to quantify the posterior probability that a given set of model parameters describes the experimental data

\begin{acknowledgments}
  The authors thank ...
\end{acknowledgments}

\bibliography{substructure}
\appendix

\section{Event-by-event grid resizing}
\label{app:adaptive_grid}

The boost-invariant \textsc{VISH2+1} hydrodynamics code used in this work \cite{Song:2007ux, Shen:2014vra} runs on a Cartesian transverse grid specified by a maximum grid size $x_\text{max}$ and grid step width $dx$ which fix the transverse grid extent ${-x_\text{max} < x < x_\text{max}}$ and number of grid cells along each dimension ${n_x = 2\, x_\text{max} / dx}$.
In general, the maximum grid size $x_\text{max}$ should be set large enough to contain the full spacetime evolution of the event.
This means that the truncation of $T^{\mu\nu}$ at the boundaries of the grid should never modify the final-state observables.
We enforce this requirement by finding an energy density cutoff $e_\text{min}$ for which the matter $e < e_\text{min}$ can be effectively discarded without significantly modifying the simulation observables.
We then fix the maximum grid size $x_\text{max}$ such that it fully encloses the isotherm $T=T(e_\text{min})$ for the full lifetime of the fireball.

We find that we can quickly estimate the maximum radius $R_\text{max} = |\x_\text{max}|^2$ of the spacetime hypersurface ${T = T(e_\text{min})}$ by running the event on a coarse-grained spatial grid with one-third the spatial resolution we would otherwise require to resolve typical hydrodynamic observables such as mean $p_T$ and flows.
The simulation time of a single \textsc{VISH2+1} event scales like ${\sim}n_x^3$ since $dx \propto d\tau$, and thus our ``pre-run'' event requires only ${\sim}1/27$th the time of a production event.
We therefore run a coarse-grained pre-event on an excessively large grid for \emph{every} minimum-bias event to estimate $R_\text{max}$, then rerun the same event on a thrice finer grid with a trimmed spatial extent $x_\text{max} \equiv R_\text{max}$.
See Fig.~\ref{fig:adaptive_grid} for a simple diagram of the procedure.

In practice, we find that event-by-event grid resizing leads to a massive speed increase for minimum bias events compared to using a single fixed grid for the entire minimum bias sample.
This is because the maximum transverse size of each event varies dramatically, from a few fm in peripheral Pb-Pb collisions to 50 fm or more in central Pb-Pb collisions.
The procedure should generalize to other hydrodynamic codes, including those with $3+1$ spacetime dimensions, where the time savings could be even more pronounced.

\begin{figure}
  \begin{tikzpicture}
    \draw [black, thin, step=0.3cm] (0.9, 0.9) grid +(4.5, 4.5);
    \draw [gray, thin, step=0.9cm] (0, 0) grid +(6.3, 6.3);
    \draw [theblue] (3.15, 3.15) circle (2.25cm);
  \end{tikzpicture}
  \caption{Diagram of the adaptive grid resizing algorithm (not drawn to scale). Each initial condition event is first run on a very large coarse-grained mesh (large gray grid) of one-third the spatial resolution otherwise required to measure hydrodynamic observables. We then measure the maximum transverse radius $R_\text{max}$ (blue circle) of the hypersurface defined by the temperature isotherm $T = T(e_\text{min})$, where $e_\text{min}$ is the largest energy density which can be truncated without modifying the hydrodynamic observables calculated from the event. Finally, the initial condition event is rerun on a smaller and finer mesh (smaller black grid) with three-times the cell density of the pre-run event and a smaller transverse extent $-R_\text{max} < x < R_\text{max}$.}
  \label{fig:adaptive_grid}
\end{figure}

\section{Emulator validation}
\label{app:validation}

\begin{figure*}
  \includegraphics{validation_example}
  \caption{Example emulator validation for one observable, the Pb-Pb charged particle yield $d\nch/d\eta$ in the 20-30\% centrality class. We use the k-fold cross validation method (explained in the text) to partition the model inputs $X$ and outputs $Y$ into training and validation data. The scatterplot on the left shows the emulator predictions and one sigma error bars (x-axis) against explicit model calculations (y-axis). Perfect emulator/model agreement is indicated by the black like $y_\text{pred}=y_\text{obs}$. The histogram on the right shows that the errors are properly accounted for, i.e.\ the normalized residuals follow a normal distribution with unit variance and zero mean.
  }
  \label{fig:validation_example}
\end{figure*}

The emulator is a surrogate for the full physics simulation which generates probabilistic predictions for the model observables $\y_m$ at a given point $\x$.
Here we validate these probabilistic predictions using a method known as k-fold cross validation.
We first randomly partition our $d=500$ training points into $k=20$ equal sized subsamples or ``folds''.
One of the subsamples is used to validate the emulator and the remaining $k-1$ subsamples are used to train it.
The process is then repeated for each of the subsamples so that we end up validating on all of the training data.

Figure~\ref{fig:validation_example} shows a scatterplot of the emulator predictions with one-sigma error bars (x-axis) against explicit model calculations (y-axis).
Perfect emulator and model agreement is indicated by the black line $y_\text{pred} = y_\text{obs}$.
If the emulator errors are properly accounted for, then the normalized residuals ${z=(y_\text{pred} - y_\text{obs})/\sigma_\text{pred}}$ sample a unit normal distribution:
\begin{equation}
  P(z) \sim \mathcal{N}(\mu=0,\sigma=1).
\end{equation}

This comparison is shown by the histogram and box plot on the right side of Fig.~\ref{fig:validation_example}.
The emulator error is clearly significant, but it is also properly modeled, as indicated by the agreement between the normalized residuals and the unit normal distribution on the right (black curve).
Moreover, since we include this uncertainty in the likelihood covariance matrix \eqref{eq:likelihood}, we expect our results to be robust to the emulator limitations.
This is an important point that bears repeating.
The emulator uncertainty does not erode the veracity of the posterior distribution if it is correctly modeled and accounted for.

More generally, we can perform the validation test in Fig.~\ref{fig:validation_example} for \emph{every} observable $y \in \y_m$ and check that each observable's normalized residuals ${z=(y_\text{pred} - y_\text{obs})/\sigma_\text{pred}}$ follow a unit normal distribution.
This test is applied to the Pb-Pb system in Fig.~\ref{fig:validation_PbPb5020} and the $p$-Pb system in Fig.~\ref{fig:validation_pPb5020}.
The top row of each figure shows a box-plot for the normalized residuals of each observable compared to the quantiles of a unit normal distribution.
The thin horizontal black lines correspond to the 10th and 90th percentiles of a unit normal distribution, and the gray band its interquartile range.
These visual references should be compared to the whiskers and interquartile range respectively of each box plot, analogous to the comparison test of Fig.~\ref{fig:validation_example}.
The emulators generally behave as expected, although the validation is somewhat better for the Pb-Pb system than the $p$-Pb system.
For instance, the $p$-Pb charged particle yield $d\nch/d\eta$ uncertainties are over predicted.
It is not immediately clear why this would be the case, but the MAP observables in Fig.~\ref{fig:obs_map} are in good agreement with their emulator predictions which suggests it should not be a grave concern.

We also show in Fig.~\ref{fig:validation_PbPb5020} and Fig.~\ref{fig:validation_pPb5020} an estimate of the emulator error magnitude.
This error is expressed in terms of the unitless variable
\begin{equation}
  \hat{z} = \frac{y_\text{pred} - y_\text{obs}}{(\Delta y)_{99\%}},
\end{equation}
where $(\Delta y)_{99\%}$ is 99\% of the full variability of $y$ across the design.
Thus $\hat{z}$ can be thought of as a fractional emulator error relative to the full design variability.
The bottom row of each figure shows the root-mean-square value of $\hat{z}$.
We see that $\text{RMS}\{\hat{z}\}$ ranges from a few percent for most observables to a maximum value of 15\% for the $p$-Pb triangular flow $v_3\{2\}$ in the lowest multiplicity bin.
This suggests that the present analysis would benefit the most from more $p$-Pb events, in particular, from more multiplicity triggered events which are used to calculate the flows.

\begin{fullpage}
\begin{figure*}
  \includegraphics{validation_PbPb5020}
  \includegraphics{validation_pPb5020}
  \caption{\label{fig:val_pbpb} Emulator validation for the Pb-Pb collision system (top) and $p$-Pb collision system (bottom) at $\sqrts=5.02$~TeV. The ``piano keys'' in the top row of each figure are horizontally stacked box plots for the normalized residuals of each model observable. The boxes are 50\% interquartile ranges and whiskers are the 90\% interquantiles.}
\end{figure*}
\end{fullpage}

\end{document}
