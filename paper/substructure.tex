\documentclass[aps,prc,reprint,amsmath,nofootinbib]{revtex4-1}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}

\usepackage[inline]{enumitem}
\usepackage{lipsum}

\usepackage{color}
\definecolor{theblue}{RGB}{0,50,230}

\usepackage{multirow}

\usepackage[pdfencoding=auto, psdextra]{hyperref}

\usepackage[font=small, justification=raggedright, labelsep=quad]{caption}

\hypersetup{
  colorlinks=true,
  linkcolor=theblue,
  citecolor=theblue,
  urlcolor=theblue
}

\usepackage{tikz}

\usepackage{graphicx}
\graphicspath{{../plots/}{./fig/}}

\newcommand{\trento}{T\raisebox{-0.5ex}{R}ENTo}
\newcommand{\nch}{N_\text{ch}}
\newcommand{\ntrk}{N_\text{trk}^\text{offline}}
\newcommand{\sqrts}{\sqrt{s_{NN}}}
\newcommand{\sigmaf}{\sigma_\mathrm{fluct}}
\newcommand{\X}{\chi_\mathrm{struct}}
\newcommand{\taufs}{\tau_\mathrm{fs}}
\newcommand{\dmin}{d_\mathrm{min}}
\newcommand{\T}{\tilde{T}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\vnk}[2]{v_#1\{#2\}}
\newcommand{\paddedhline}{\noalign{\smallskip}\hline\noalign{\smallskip}}
\newcommand{\order}[1]{$\mathcal O(10^{#1})$}
\newcommand{\note}{\textcolor{theblue}{[?]}}


\renewcommand\labelitemi{$\vcenter{\hbox{\scriptsize$\bullet$}}$}


% hyperref throws warning when a line break is used in the title
% this removes the warning by redefining the line break in a pdf string
\pdfstringdefDisableCommands{%
  \def\\#1{ #1}
}

% Convenient figure macro.  Usage:
%
%   \fig[placement specifier = t]{filename}{caption}
%
% This creates a figure environment, includes the given filename as graphics,
% puts the given caption below the graphics, and labels it 'fig:filename'.
% The optional placement specifier defaults to 't' and is passed directly to
% the figure environment.
% Use \fig* to make a figure* environment, i.e. a wide figure.
\usepackage{xparse}
\NewDocumentCommand\fig{sO{t}mm}{
  \begin{figure\IfBooleanT{#1}{*}}[#2]
    \includegraphics{#3}
    \caption{\label{fig:#3}#4}
  \end{figure\IfBooleanT{#1}{*}}
}


\begin{document}

\title{
  Estimating initial state and quark-gluon plasma medium properties\\
  using a hybrid model with nucleon substructure\\
  calibrated to p-Pb and Pb-Pb collisions at \texorpdfstring{$\mathbf{\sqrts=5.02}$}{}~TeV
}

\author{J.\ Scott Moreland}
\author{Jonah E.\ Bernhard}
\author{Steffen A.\ Bass}

\affiliation{Department of Physics, Duke University, Durham, NC 27708-0305}

\date{\today}

\begin{abstract}
We posit a unified hydrodynamic and microscopic description of the quark-gluon plasma (QGP) produced in ultrarelativistic p-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV and evaluate our assertion using Bayesian inference. Specifically, we model the dynamics of both collision systems using initial conditions with parametric nucleon substructure, a pre-equilibrium free-streaming stage, event-by-event viscous hydrodynamics, and a microscopic hadronic afterburner.
Free parameters of the model which describe the initial state and QGP medium are then simultaneously calibrated to fit charged particle yields, mean $p_T$ and flow cumulants.
We argue that the global agreement of the calibrated model with the experimental data strongly supports the existence of hydrodynamic flow in small collision systems at ultrarelativistic energies, and that this flow necessarily develops at length scales smaller than a proton.
Posterior estimates for the model's input parameters are obtained, and new insights into the temperature dependence of the QGP transport coefficients and event-by-event structure of the proton are discussed.
\end{abstract}

\maketitle


\section{Introduction}

Ultrarelativistic nuclear collisions between one light-ion and one heavy-ion, e.g.\ $^3$He-Au and p-Pb collisions, generate dense, compact sources of nuclear matter which produce long-range multiparticle correlations that are strikingly similar to the correlations observed in heavy-ion collisions where collectivity is commonly explained by the existence of hydrodynamic flow \cite{CMS:2012qk, Abelev:2012ola, Aad:2012gla, Adare:2015ctn}.
This observation suggests that hydrodynamic behavior could be manifest in small droplets of quark-gluon-plasma (QGP), and that flow might develop at length scales smaller than a single proton.

Hydrodynamic models of ultrarelativistic nuclear collisions are complicated by a number of theoretical unknowns, including the detailed geometry of the QGP initial conditions, the strength and duration of pre-equilibrium dynamics, the temperature dependence of QGP transport coefficients, and the boundaries of hydrodynamic applicability \cite{Niemi:2014lha, deSouza:2015ena, Ollitrault:2012cm, Song:2012ua}.
In general, these theoretical uncertainties tend to grow with decreasing system size, where emergent physics at sub-fermi length scales becomes important to describe bulk properties of the produced system.

One method for reducing theoretical uncertainties is to test model calculations by varying the species of colliding nuclei at a single beam energy \cite{Adare:2015bua, Schenke:2014tga, Aidala:2018mcw, Adare:2017wlc, Adamczyk:2015obl, Shen:2016zpp, Aidala:2017ajz, Adare:2006ti}.
Since initial condition and hydrodynamic models generally factorize the structure of the colliding nuclei from the subsequent time dynamics of the collision, a single theory framework can be simultaneously tested and compared to measurements from multiple collision systems using a self-consistent set of model parameters where only the nuclear structure in the model is permitted to vary.

Typically, the macroscopic structure of heavy nuclei, characterized e.g.\ by an atomic mass number and set of Woods-Saxon coefficients \cite{MOLLER1995185, DEVRIES1987495}, is regarded as a known input to hydrodynamic models which contributes negligible uncertainty to simulation predictions, outweighed by large uncertainties in modeling initial energy deposition and off-equilibrium dynamics \cite{Niemi:2014lha, Song:2011hk, Retinskaya:2013gca, Liu:2015nwa, Kurkela:2016vts}.
The geometry of light ions, meanwhile, is naturally more sensitive to the detailed size and shape of individual protons and neutrons inside the nucleus, which may fluctuate event-by-event and differ signficantly from the round blobs typically used to approximate nucleons in heavy-ion collisions \cite{Schenke:2014zha, Welsh:2016siu, Moreland:2017kdx, Schenke:2014gaa, Schlichting:2014ipa}.
These nucleon substructure properties are difficult to measure and calculate from first principles and hence contribute significant uncertainty to model predictions of small systems.

Early substructure studies replaced round protons with composite protons, described by a few salient model parameters, in order to investigate the effect of each parameter on simulated observables \cite{Adler:2013aqf, Mitchell:2016jio, Welsh:2016siu, Broniowski:2016pvx, Bozek:2017jog}.
These sensitivity studies were able to identify cause and effect relationships between model inputs and outputs, but lacked the ability to constrain nucleon substructure parameters in any kind of global or systematic fashion.
It quickly became apparent that numerous substructure implementations might be compatible with available data, and that additional work would be required to identify observables which are particularly sensitive to the average size, shape and fluctuations of the proton.

Several such observables have been identified in proton-proton and proton-lepton scattering data.
Measurements by the TOTEM collaboration at $\sqrt{s}=7$~TeV, for instance, found an unexpected dip in the inelasticity density of $p$-$p$ collisions at zero impact parameter \cite{Antchev:2011zz}.
It was later realized that this depression, or so-called hollowness effect in the $p$-$p$ inelastic collision profile \cite{Arriola2016}, can be explained by the existence of correlated domains inside the proton, and that aspects of these domains, such as their size and correlation strength, may be constrained by comparing model predictions to inelastic $p$-$p$ measurements \cite{Albacete:2016gxu, Albacete:2016pmp}.

Independently, studies of coherent and incoherent $J/\psi$ production based on a color dipole picture of vector meson production were used to simultaneously constrain both the average color charge density of the proton as well as its event-by-event fluctuations in a saturation based framework \cite{Mantysaari:2016ykx, Aaron:2009aa, Abramowicz:2015mha}.
Initial condition studies using the IP-Glasma model of color-glass condensate effective field theory \cite{Schenke:2012wb} simultaneously demonstrated that these color charge fluctuations leave a lasting imprint on the \mbox{small-x} gluon distribution of the proton and hence the initial geometry of QGP energy deposition \cite{Schlichting:2014ipa}.
In addition, it was recently shown that hydrodynamic simulations using IP-Glasma initial conditions with color charge fluctuations calibrated to fit coherent and incoherent $J/\psi$ diffraction measured by the H1 and Zeus experiments at HERA \cite{Aaron:2009aa, Abramowicz:2015mha} provide a good description of collectivity in small and large collision systems \cite{Schenke:2018qmc}.

Model parameters, such as those calibrated by the aforementioned studies, are of course always in some degree of tension.
For instance, fitting one observable may require parameter values that degrade the quantitative description of some other observable.
Similarly, parameters which provide an optimal description of small-system observables may lead to a sub-optimal description of heavy-ion observables or \emph{vice versa}.
It is thus import to look at the experimental data holistically, and to use model calibration methods which
\begin{enumerate*}[label=(\arabic*)]
  \item explore all parameter combinations and
  \item compare model predictions to all relevant experimental measurements in a statistically rigorous fashion.
\end{enumerate*}

With these considerations in mind, we present progress towards a fully global analysis of $p$-Pb and Pb-Pb bulk observables at $\sqrts=5.02$~TeV using a model calibration framework known as Bayesian parameter estimation.
We begin, in Sec.~\ref{sec:model}, by constructing a nuclear collision model for $p$-Pb and Pb-Pb collisions using initial conditions with parametric nucleon substructure, and transport dynamics described by a pre-equilibrium free-streaming stage, viscous hydrodynamics and microscopic Boltzmann transport.
In Sec.~\ref{sec:calibration}, we calibrate free parameters of the model to fit charged particle yields, mean $p_T$ and anisotropic flow coefficients of \emph{both} collision systems at $\sqrts=5.02$~TeV, and finally, in Secs.~\ref{sec:results} and \ref{sec:summary}, we present posterior results for the model input parameters and comment on the implications for hydrodynamics descriptions of small collision systems.


\section{Nuclear collision model}
\label{sec:model}

We employ a multi-stage hybrid transport model that uses relativistic viscous hydrodynamics to describe the QGP medium and microscopic Boltzmann transport to simulate the dynamics of the system after hadronization \cite{Shen:2014vra, Bernhard:2016tnd}.
The hydrodynamic initial conditions are provided by a modified version of the \trento\ model \cite{Moreland:2014oya} with additional parameters to vary the size and shape of the nucleon.
Each initial condition profile is free streamed to the hydrodynamic starting time and matched onto the hydrodynamic energy-momentum tensor using the Landau matching procedure \cite{Broniowski:2008qk, Heinz:2015arc}.
Many of the components of the present model have been documented in previous studies \cite{Moreland:2014oya, Bernhard:2016tnd, Bernhard:2018hnz}; we review each component here for completeness.

\subsection{Initial state}
\label{sec:initial_state}

We model the QGP initial state in $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV using a simple parametric form for boost-invariant entropy deposition employed by the \trento\ model \cite{Moreland:2014oya}.
Generally speaking, the initial three-dimensional distribution of matter produced in relativistic nuclear collisions is \emph{not} boost-invariant; longitudinal entropy deposition fluctuates both locally point-to-point in the transverse plane as well as globally event-by-event due to asymmetries in the sampled density of participant matter \cite{Ke:2016jrd, Bozek:2010vz}.
Nevertheless, boost-invariance has been shown to be a good approximation for both large and small collision systems when hydrodynamic observables are calculated from particles that are detected close to midrapidity \cite{Shen:2016zpp}.

The \trento\ model operates in the ultrarelativistic limit with a Lorentz factor $\gamma \gg 1$ such that each nucleus appears as a thin sheet of nuclear density in the laboratory frame.
The sheets of colliding nuclear density penetrate and pass through each other in time $\Delta t = D_\text{nucl} / \sqrt{\gamma^2 - 1}$ in the laboratory frame, where $D_\text{nucl}$ is the diameter of the nucleus in its rest frame and $\gamma$ is the usual Lorentz factor of the accelerated ions.
The resulting nuclear overlap time $\Delta t \lesssim 0.1$~fm/$c$ at top RHIC and LHC energies is thus sufficiently short to neglect initial transverse dynamics which occur while the nuclei pass through each other.
We therefore assume that the collision produces all secondary particles at uniform proper time $\tau = 0^+$~fm/$c$, and that it deposits entropy (energy) at midrapidity which is a function of the locally varying beam integrated density of each nucleus.

Consider the collision of two protons $A$, $B$ with three-dimensional densities $\rho_{A,B}$ in their local rest frames.
The proton-proton overlap function
\begin{equation}
  \label{eq:tpp}
  T_{pp}(b) \equiv \int dx\, dy \int dz\, \rho_A(\x) \int dz\, \rho_B(\x + \mathbf{b}),
\end{equation}
describes the eikonal overlap of the two proton wave packets at fixed impact parameter $b$ where coordinates $(x, y)$ lie in the transverse plane and $z$ is parallel to the beam axis.
Here we assume that each proton is comprised of smaller constituents---e.g.\ valence quarks, sea quarks, and small-x gluons---which may collide to produce secondary particles and contribute to the observed inelastic proton-proton cross section.

Within a picture of independent pairwise collisions between the constituents, a Glauber model model may be used to calculate the probability $P_\mathrm{coll}$ that the two protons collide inelastically at impact parameter $b$. In the limit when the number of constituents is large, it yields the particularly simple form
\begin{equation}
  \label{eq:pcoll}
  P_\mathrm{coll} = 1 - \exp[-\sigma_\mathrm{eff}\, T_{pp}(b)], \\
\end{equation}
where $\sigma_\mathrm{eff}$ is an effective cross section for pairwise inelastic collisions between the constituents, and $\sigma_{pp}^\mathrm{inel}$ is the total inelastic proton-proton cross section:
\begin{equation}
  \label{eq:sigma_nn}
  \sigma_{pp}^\mathrm{inel} = \int 2 \pi b\, db\, P_\mathrm{coll}(b).
\end{equation}
The proton densities $\rho_{A,B}$ in Eqn.~\eqref{eq:tpp} are commonly modeled using a spherically symmetric distribution.
For instance, the original implementation of the \trento\ model uses Gaussian protons, largely because it yields a simple analytic solution to Eqn.~\eqref{eq:pcoll}.
Needless to say, such approximations are admittedly crude and may have a significant effect on the dynamics of small collision systems where the proton size is comparable to the size of the produced QGP medium.

A number of previous studies have investigated the effects of deformed or ``lumpy'' protons.
One common implementation is a superposition of three valence quarks, typically described by Gaussian or exponential form factors \cite{Welsh:2016siu, Bozek:2017jog, Schenke:2014zha, Schlichting:2014ipa, Adare:2015bua, Broniowski:2016pvx}.
The corresponding proton density $\rho(\mathbf{x})$ is then assumed to be that of predominantly small-x gluons, seeded by the distribution of color charge in each of the three valence quarks.

\begin{figure}
  \begin{tikzpicture}
    % spherical proton
    \draw[dashed, xshift=-2.5cm] (0,0) circle (1cm);
    % three partons
    \draw[dashed] (0,0) circle (1cm);
    \foreach \theta in {0, 120, 240}{
      \draw ({\theta}:.5) circle (.4cm);
    }
    % ten partons
    \draw[dashed, xshift=2.5cm] (0,0) circle (1cm);
    \foreach \theta/\radius in {
      0/0.6, 40/0.5, 90/0.6, 130/0.5, 180/0.6, 220/0.6,
      260/0.5, 320/0.6, 300/0.2
    }{
      \draw[xshift=2.5cm] ({\theta}:\radius) circle (.3cm);
    }
  \end{tikzpicture}
  \caption{Schematic of plausible proton shapes. The sketch on the left shows a spherically symmetric proton (dashed line), while the middle and right illustrations depict a fluctuating proton with three and nine constituents respectively (solid lines).}
  \label{fig:substructure}
\end{figure}

In this work, we pursue a less restrictive and more parametric description of the proton where the number of substructure degrees of freedom are uncertain as depicted in Fig.~\ref{fig:substructure}.
We model each nucleon's density $\rho_{A,B}$ as a sum of $n_c$ independent constituents
\begin{equation}
  \label{eq:rho}
  \rho_{A, B}(\x) = \frac{1}{n_c} \sum\limits_{i=1}^{n_c} \rho_c(\mathbf{x} - \mathbf{x_i}),
\end{equation}
where each constituent density $\rho_c$ is described by a Gaussian distribution of width $v$
\begin{equation}
  \label{eq:constituent_density}
  \rho_c(\mathbf{x}) = \frac{1}{(2\pi v^2)^{3/2}} \exp\left(-\frac{\x^2}{2 v^2}\right),
\end{equation}
and each constituent's position $\mathbf{x_i}$ in Eqn.~\eqref{eq:rho} is sampled from a Gaussian radial distribution with standard deviation $r$.

The two protons $A$, $B$ are assigned a random impact parameter, and Eqn.~\eqref{eq:pcoll} is used to sample their inelastic collision probability $P_\mathrm{coll}(b)$.
Note that this proton-proton inelastic collision probability has no direct knowledge of the individual constituent degrees of freedom; it is only \emph{indirectly} sensitive through the geometry of $\rho_{A, B}$ which depends on each of the constituent positions.
This is an important distinction between the present model and a similar nucleon substructure implementation known as the participant or ``wounded'' quark model which allows for a subset of quarks (constituents) to participate inside a single nucleon \cite{ANISOVICH1978477, Broniowski:2016pvx}.
The proton, unlike the nucleus, cannot produce semi-stable spectator fragments in a high-energy collision.
Any spectator quarks produced by a wounded quark model would be colored objects that necessarily contribute to secondary particle production as they fragment and recombine to form color-neutral hadrons.
We correspondingly require that the nucleons in Eqn.~\eqref{eq:rho} participate as singular objects, such that all spectator matter discarded by the simulation is appropriately color-neutral and inert.

Assuming our two protons collide at the sampled impact parameter $\mathbf{b}$, we assign each a \emph{fluctuated} thickness
\begin{equation}
  \label{eq:fluctuated_thick}
  \T_{A, B}(\x) \equiv \int dz\, \frac{1}{n_c} \sum\limits_{i=1}^{n_c} \gamma_i\, \rho_c \,(\mathbf{x} - \mathbf{x_i} \pm \mathbf{b}/2),
\end{equation}
equal to the beam-integrated proton density in Eqn.~\eqref{eq:rho}, with each constituent shifted by the appropriate impact parameter offset, and multiplied by a gamma random variable $\gamma_i$ with unit mean and variance $1/k$.
These \emph{ad hoc} gamma random weights are necessary to describe the large fluctuations observed in minimum bias proton-proton collisions, although their physical justification is not yet well understood.

Within the eikonal approximation, the initial entropy deposited at midrapidity at proper time $\tau=0^+$~fm/$c$ can be represented by some mapping ${f(\T_A, \T_B) \mapsto dS/dy}$.
A natural first guess for the function $f$ is an arithmetic mean $f \propto (\T_A + \T_B)/2$, equal to a wounded nucleon model up to meaningless factor of two in the overall normalization.
The wounded nucleon model was in fact the first such mapping used as a proxy for initial particle production and entropy deposition in relativistic heavy-ion collisions \cite{Bialas:1976ed}.
It was quickly realized, however, that the wounded nucleon model predicts the wrong scaling for charged particle production as a function of collision centrality and hence the wrong scaling for initial entropy deposition as a function of participant thickness $\T_{A,B}$ \cite{Kharzeev:2000ph}.

A simple remedy is to replace the arithmetic mean in the wounded nucleon model with a more flexible parametrization
\begin{equation}
  \label{eq:gmean}
  \frac{dS}{dy}\bigg\vert_{y=0} \propto \left( \frac{\T_A^p + \T_B^p}{2} \right)^{1/p},
\end{equation}
based on a family of functions known as the generalized mean(s).
This parametrization introduces a dimensionless parameter $p$ which varies the scaling behavior of initial entropy deposition at midrapidity.
For certain discrete values of $p$, it reduces to well known functional forms such as the arithmetic, geometric and harmonic means:
\begin{equation}
  \newlength{\extraspace}
  \setlength{\extraspace}{0.5ex}
  \frac{dS}{dy}\bigg\vert_{y=0} \propto
  \begin{cases}
    \max(T_A, T_B) & p \rightarrow +\infty, \\[\extraspace]
    (T_A + T_B)/2 & p = +1, \hfill \text{ (arithmetic)} \\[\extraspace]
    \sqrt{T_A T_B} & p = 0, \hfill \text{ (geometric)} \\[\extraspace]
    2\, T_A T_B/(T_A + T_B) & p = -1, \hfill \text{ (harmonic)} \\[\extraspace]
    \min(T_A, T_B) & p \rightarrow -\infty.
  \end{cases}
\end{equation}
Conveniently, it has been shown that the generalized mean ansatz is able to mimic the scaling behavior of certain saturation based initial condition models \cite{Bernhard:2016tnd}, and hence it should serve as a reasonable parametric form for exploring QGP entropy deposition, assuming imperfect knowledge of saturation effects in nature.
Of course the model is \emph{not} a substitute for first principle saturation calculations, and may fail to reproduce nuanced features of \emph{ab initio} models such as the existence of short-range gluon field fluctuations \cite{Schenke:2012wb}.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \node {\includegraphics{thickness}};
    \node[label=above:{10 fm}] (a) at (0, 1.1) {};
    \node (b) at (0, -1.1) {};
    \draw [<->, semithick] (a) to (b);
  \end{tikzpicture}
  \caption{Effect of nucleon substructure on the nuclear thickness function $T(x, y) \equiv \int dz\, \rho(x, y, z)$ of a $^{208}\mathrm{Pb}$ nucleus. The nucleus on the left has Gaussian nucleons of width $0.8$~fm, while the nucleus on the right has composite nucleons, each containing six constituents of width $0.4$~fm.}
  \label{fig:thickness}
\end{figure}

Equation~\eqref{eq:gmean} is a purely local function of nuclear density in the transverse plane and should (in principle) be equally valid for any pair of colliding nuclei at sufficiently high beam energy.
The model readily generalizes from individual proton-proton collisions to arbitrary nucleus-nucleus collisions by summing the participant thicknesses $\T_{A,B}$ over all nucleons which participate in one or more inelastic collisions.
The only modeling difference between $p$-$p$ collisions and Pb-Pb collisions is the number and the position of the nucleons.

When applying the model to heavy-ions, we sample nucleon positions from a Woods-Saxon density distribution subject to a minimum distance criteria $|\mathbf{x_i} - \mathbf{x_j}| > d_\mathrm{min}$ between all pairs of nucleons $i$, $j$.
The minimum distance algorithm, first described in Ref.~\cite{Bernhard:2018hnz}, uses a simple trick to resample the nucleon positions without modifying the target Woods-Saxon radial distribution.
We first presample the radii of all nucleons in a given nucleus and sort them in ascending order.
We then sample the solid angles of each nucleon one-by-one, starting with the nucleon closest to the center of the nucleus and working our way outwards.
If a sampled nucleon position is too close to any of its previously placed neighbors, its solid angle is resampled until the minimum distance criteria is satisfied.
Similar methods could be used to model correlations between individual constituents inside each nucleon, although the numerical implementation would be somewhat tedious.

\subsection{Pre-equilibrium dynamics}

\fig[b]{coupling}{
Cartoon of the free-streaming approximation for hydrodynamic initialization.
The initial state is free-streamed for proper time $\taufs$ (zero-coupling) before it is matched to hydrodynamics (strong-coupling).
This piecewise evolution approximates the more realistic scenario expected in nature where the medium's coupling strength smoothly changes as a function of time.
}

There are of course two limiting cases for the strength of interactions inside the QGP medium immediately at the collision: infinitely weak coupling where the secondary partons free-stream without interacting, and infinitely strong coupling where the fluid's inter-particle mean free path effectively vanishes.
Realistically, one expects the initial parton interactions to lie somewhere between these two extremes.
We therefore choose to model the QGP's initial off-equilibrium dynamics using a simple step-function approximation, depicted in Fig.~\ref{fig:coupling}, which free-streams the initial state for proper time $\taufs$ (zero coupling) before instantaneously switching to viscous hydrodynamics (strong coupling) \cite{Liu:2015nwa, Broniowski:2008qk}.
The free parameter $\taufs$ allows us to parametrically vary the \emph{time averaged} coupling strength in the approximate window $0 < \taufs \lesssim1$~fm/$c$.

The parametric entropy deposition ansatz in Eqn.~\eqref{eq:gmean} does not provide any information about the initial masses or momenta of particles produced in the collision.
In general, these details will affect the dynamics predicted by the collisionless Boltzmann equation
\begin{equation}
  \label{freestream}
  p^\mu \partial_\mu f(x, p) = 0,
\end{equation}
through its dependence on the underlying distribution function $f(x, p)$, and hence are necessary inputs for any free-streaming implementation.
Equation~\eqref{freestream}, however, simplifies for massless partons with momentum distributions that are locally isotropic.
Under this assumption, it was shown in Refs.~\cite{Broniowski:2008qk, Liu:2015nwa} that the energy-momentum tensor $T^{\mu\nu}$ of partons at midrapidity and time $\tau$ only depends on its spatial distribution at some earlier time $\tau_0$, but not its $p_\perp$-distribution, which could vary as a function of position.

We thus follow the procedure of the authors and reinterpret our initial transverse entropy density at midrapidity $s(\mathbf{x_\perp}, \tau_0)\vert_{\eta=0}$ as a transverse density of noninteracting massless partons $n(\mathbf{x}, \tau_0)\vert_{\eta=0}$.
The free-streamed energy-momentum tensor $T^{\mu\nu}(x, y, \eta, \tau)$ at transverse coordinate $(x, y)$, midrapidity $\eta=0$, and proper time $\tau > \tau_0$ is then given by
\begin{multline}
  \label{energy-momentum}
  T^{\mu\nu}(x, y, 0, \tau) = \frac{1}{\tau} \int_{0}^{2\pi} d\phi\, n(x - \Delta \tau \cos \phi, y - \Delta \tau \sin \phi) \\ \times
  \begin{bmatrix}
    1 & \cos\phi & \sin\phi  & 0\\
    \cos\phi & \cos^2\phi & \cos\phi\sin\phi & 0 \\
    \sin\phi & \sin\phi \cos\phi & \sin^2\phi & 0\\
    0 & 0 & 0 & 0
  \end{bmatrix},
\end{multline}
where $\Delta\tau$ is the free-streaming time $\Delta\tau = \tau - \tau_0$.
The solution \eqref{energy-momentum} is decomposed in hydrodynamic form
\begin{equation}
  \label{hydro-eqn}
  T^{\mu\nu} = e u^\mu u^\nu - (P + \Pi) \Delta^{\mu\nu} + \pi^{\mu\nu},
\end{equation}
where $e$ and $P$ are the energy density and pressure in the local fluid rest frame, $u^\mu$ is the local fluid velocity, ${\Delta^{\mu\nu} \equiv g^{\mu\nu} - u^\mu u^\nu}$ is the projector onto the space orthogonal to $u^\mu$, and $\Pi$ and $\pi^{\mu\nu}$ are the bulk pressure and shear stress tensor respectively.
We then solve for the energy density $e$ and fluid velocity $u^\mu$ using the Landau matching condition which defines the fluid rest frame velocity as the time-like eigenvector of $T^{\mu\nu}$ with energy density $e$ as its eigenvalue,
\begin{equation}
  T^{\mu\nu} u_\nu = e u^\mu.
\end{equation}
The initial bulk and shear corrections are finally solved for by subtracting the ideal pressure from the total pressure to find $\Pi$, then solving for $\pi^{\mu\nu}$ using Eqn.~\eqref{hydro-eqn}
\begin{align}
  \Pi &= -\frac{1}{3} \mathrm{Tr}(\Delta_{\mu\nu} T^{\mu\nu}) - P,\\
  \pi^{\mu\nu} &= T^{\mu\nu} - e u^\mu u^\nu + (P + \Pi) \Delta^{\mu\nu}.
\end{align}

This procedure provides initial values for $T^{\mu\nu}$, $u^\mu$, $\Pi$ and $\pi^{\mu\nu}$ which conserve energy and are consistent with the underlying hydrodynamic equation of state.
We therefore expect it to provide a more realistic description of the initial stages of the collision as compared to a previous study using the \trento\ initial condition model which set $\Pi$, $\pi^{\mu\nu}$ and $u^\mu$ initially to zero \cite{Bernhard:2016tnd}.

\subsection{Hydrodynamics}

After free-streaming for proper time $\taufs$, we transition to viscous hydrodynamics which solves the conservation equations
\begin{equation}
  \label{eq:continuity}
  \partial_\mu T^{\mu\nu} = 0
\end{equation}
for the hydrodynamic energy-momentum tensor $T^{\mu\nu}$ expressed in Eqn.~\eqref{hydro-eqn} using a set of second-order Israel-Stewart equations formulated in the 14-moment approximation
\cite{Israel:1979wp, Israel:1976aa, Denicol:2012cn, Denicol:2010xn}.
This produces a pair of relaxation-type equations
\begin{subequations}
  \label{eq:relaxation}
  \begin{align}
    \tau_\Pi \Pi + \dot{\Pi} &=
      - \zeta \theta - \delta_{\Pi\Pi} \Pi\theta
      + \lambda_{\Pi\pi} \pi^{\mu\nu} \sigma_{\mu\nu}, \\[1ex]
    \tau_\pi \dot{\pi}^{\langle \mu\nu \rangle} + \pi^{\mu\nu} &=
      2\eta\sigma^{\mu\nu} - \delta_{\pi\pi} \pi^{\mu\nu} \theta
      + \phi_7 \pi_\alpha^{\langle \mu} \pi^{\nu \rangle \alpha} \nonumber \\
      &\qquad {} - \tau_{\pi\pi} \pi_\alpha^{\langle \mu}\sigma^{\nu \rangle \alpha}
      + \lambda_{\pi\Pi} \Pi \sigma^{\mu\nu},
  \end{align}
\end{subequations}
for the bulk pressure $\Pi$ and shear-stress $\pi^{\mu\nu}$.
We model the shear viscosity $\eta$ and bulk viscosity $\zeta$ as unknown temperature dependent quantities and fix the remaining transport coefficients $\{\tau_\Pi, \delta_{\Pi\Pi}, \lambda_{\Pi\pi}, \tau_\pi, \delta_{\pi\pi}, \phi_7, \tau_{\pi\pi}, \lambda_{\pi\Pi}\}$ using analytic results derived in the limit of small but finite masses \cite{Denicol:2014vaa}.

\fig[t]{viscosity_dof}{Degrees of freedom in the temperature dependent shear and bulk viscosity parametrizations. Lines are chosen for illustrative purposes only and do not represent all possible variability. For instance, $\eta/s$ could have a large slope and negative curvature, or $\zeta/s$ could have a large max and narrow width, neither of which are depicted above.
}

The hydrodynamic equations of motion are necessarily closed using an equation of state (EoS) to relate the energy density $e$ and pressure $P$ of the produced medium.
We use a parametrization for $P(e)$ that matches a hadron resonance gas EoS at low temperature to a lattice QCD EoS at high temperature by smoothly connecting their trace anomalies in the interval
$165 \le T \le 200$~MeV \cite{Bernhard:2018hnz}.
For the lattice EoS, we use a calculation by the HotQCD collaboration for (2+1)-flavor QCD which was extrapolated to the continuum limit \cite{Bazavov:2014pvz}. Recent developments in lattice QCD now enable calculations in (2+1+1)-flavors \cite{Borsanyi:2016ksw}, i.e.\ with thermalized charm quarks, and the additional charm flavor has been shown to visibly affect predictions of $p_T$-differential flow observables \cite{Noronha-Hostler:2018zxc}.
Investigating this sensitivity would thus be a natural target for future improvements to the present work.

We parametrize the temperature dependence of the QGP viscosities in order to marginalize over their uncertainty when calibrating to data.
For the specific shear viscosity $\eta/s$, we use a modified linear ansatz
\begin{equation}
  \label{eq:shear_viscosity}
  (\eta/s)(T) = (\eta/s)_\mathrm{min} + (\eta/s)_\mathrm{slope}\cdot(T - T_c)\cdot(T/T_c)^{(\eta/s)_\mathrm{crv}}
\end{equation}
where $\eta/s$ min, slope and curvature are tunable parameters and $T_c=0.154$~GeV is the pseudocritical transition temperature of the HotQCD EoS.
While for the specific bulk viscosity $\zeta/s$, we use an unnormalized Cauchy distribution
\begin{equation}
  \label{eq:bulk_viscosity}
  (\zeta/s)(T) = \frac{(\zeta/s)_\mathrm{max}}{1 + \left(\dfrac{T - (\zeta/s)_{T_0}}{(\zeta/s)_\mathrm{width}}\right)^2},
\end{equation}
described by tunable maximum, width, and location ($T_0$) parameters.
Figure~\ref{fig:viscosity_dof} shows several of the possible curves parametrized by Eqns.~\eqref{eq:shear_viscosity} and \eqref{eq:bulk_viscosity}, although many more are possible.

The aforementioned hydrodynamic equations are solved numerically using the boost-invariant \texttt{VISH2+1} viscous hydrodynamics code \cite{Song:2007ux} which is documented extensively in Ref.~\cite{Shen:2014vra}.
We simulate each hydrodynamic event on a spacetime grid with transverse extent $x_\mathrm{max}$, spatial grid step $dx$, and time step $d\tau$ which are optimized \emph{event-by-event} to balance trade-offs between numerical accuracy and computation time (see appendix).
Although these details are somewhat mundane, they are critically important to the present study, since the computation time scales with the number of spacetime cells $n_x^2\, n_\tau$, and $n_x$ and $n_\tau$ typically have to be quite large to resolve the small length scales associated with nucleon substructure.

\subsection{Particlization and Boltzmann transport}

We evolve the system hydrodynamically down to a pre-specified switching isotherm $T_{sw}$ at which point the medium is converted into particles using the Cooper-Frye formula \cite{PhysRevD.10.186}
\begin{equation}
  \label{cooper-frye}
  E \frac{dN_i}{d^3p} = \frac{g_i}{(2\pi)^3} \int_\Sigma f_i(x, p)\, p^\mu\, d^3\sigma_\mu,
\end{equation}
where $i$ is an index over species, $f_i$ is the distribution function of that species, and $d^3\sigma_\mu$ is a volume element of the isothermal hypersurface $\Sigma$ defined by $T_\mathrm{sw}$.
Thermal particles are then sampled in the rest frame of each fluid cell according to a Bose-Einstein or Fermi-Dirac distribution at zero baryon chemical potential
\begin{equation}
  \label{distribution}
  f(m, p) = \frac{1}{\exp(\sqrt{m^2 + p^2}/T) \mp 1},
\end{equation}
where $m$ is the mass of the sampled particle, $p$ is its momentum, and $T$ is the temperature of the fluid cell.

\subsubsection{Sampling with finite resonance widths}

Traditionally, particlization models have sampled resonances using each particle's pole mass in Eqn.~\eqref{distribution}.
This approximation, however, is somewhat crude and has been known to underpredict pion production, particularly at low $p_T$ \cite{Sollfrank:1991xm, Huovinen:2016xxq, Vovchenko:2018fmh}.
We thus follow Ref.~\cite{Bernhard:2018hnz}, and sample particles with a \emph{distribution} of masses
\begin{equation}
  f(p) = \int dm\, \mathcal{P}(m)\, f(m, p),
\end{equation}
where $\mathcal{P}(m)$ is modeled by a Breit-Wigner distribution
\begin{equation}
  \mathcal{P}(m) \propto \frac{\Gamma(m)}{(m - m_0)^2 + \Gamma(m)^2/4}.
\end{equation}
Here $m_0$ is the resonance's Breit-Wigner mass and $\Gamma(m)$ is its mass-dependent width, for which we use a simple form:
\begin{equation}
  \Gamma(m) = \Gamma_0 \sqrt{\frac{m - m_\mathrm{min}}{m_0 - m_\mathrm{min}}},
\end{equation}
where $\Gamma_0$ is the usual Breit-Wigner width and $m_\mathrm{min}$ is a production threshold equal to the total mass of the lightest decay products.

We tabulate the values of $\{\Gamma_0, m_0, m_\mathrm{min}\}$ for all particles and sample the masses of each particle during particlization \cite{PDG:2017}.
The resonances are then passed to a hadronic transport model, described shortly, which simulates subsequent scatterings and decays.

\subsubsection{Viscous corrections to the distribution function}

When the viscous terms $\pi^{\mu\nu}$ and $\Pi$ are nonzero in Eqn.~\eqref{hydro-eqn}, the distribution function $f$ must be modified to preserve the continuity of $T^{\mu\nu}$ as the system transitions from hydrodynamics to Boltzmann transport.
We perform the appropriate modification using a general method which transforms the momentum vector \emph{inside} the distribution function \cite{Pratt:2010jt}
\begin{align}
  \label{viscous_correction}
  p_i \rightarrow p'_i &= p_i + \sum\limits_j \lambda_{ij}\, p_j,\\
  \lambda_{ij} &= (\lambda_\mathrm{shear})_{ij} + \lambda_\mathrm{bulk}\, \delta_{ij},
\end{align}
where $\lambda_{ij}$ is a linear transformation matrix consisting of a traceless shear part and a bulk part which is proportional to the identity matrix.

We use for the shear viscous correction the form \cite{Pratt:2010jt}
\begin{equation}
  (\lambda_\mathrm{shear})_{ij} = \frac{\tau}{2 \eta} \pi_{ij},
\end{equation}
with a value for $\eta/\tau$ obtained from the noninteracting hadron resonance gas model
\begin{equation}
  \frac{\eta}{\tau} = \frac{1}{15 T} \sum\limits_{sp} g \int \frac{d^3p}{(2\pi)^3}\frac{p^4}{E^2} f_0 (1 \pm f_0).
\end{equation}
For the bulk viscous correction, we use a novel procedure developed in Ref.~\cite{Bernhard:2018hnz}.
The total kinetic pressure of the system is
\begin{equation}
  \label{kinetic-pressure}
  P + \Pi = \sum\limits_\mathrm{sp} g \int \frac{d^3p}{(2\pi)^3} \frac{p^2}{3E} f(p).
\end{equation}
For a given bulk pressure, we rescale the momentum $p$ inside the distribution function $f(p) \rightarrow f(p + \lambda_\mathrm{bulk}\, p)$ and adjust the parameter $\lambda_\mathrm{bulk}$ to match the total pressure on the left side of Eqn.~\eqref{kinetic-pressure}.
This substitution of course also modifies the energy density
\begin{equation}
  e = \sum\limits_\mathrm{sp} g \int \frac{d^3p}{(2 \pi)^3} E f(p),
\end{equation}
and so a fugacity term $z_\mathrm{bulk}$ is introduced which decreases the yield of all particles by the same overall factor to compensate.
The full transformation is then given by $f(p) \rightarrow z_\mathrm{bulk}\, f(p + \lambda_\mathrm{bulk}\, p)$, where the parameters $\lambda_\mathrm{bulk}$ and $z_\mathrm{bulk}$ are determined numerically for each value of the bulk pressure.

\subsubsection{Boltzmann transport}

Once the fluid is converted into particles, we simulate its subsequent microscopic dynamics using the Ultra-relativistic Quantum Molecular Dynamics (\texttt{UrQMD}) transport model \cite{Bass:1998ca, Bleicher:1999xi}.
It solves the microscopic Boltzmann equation
\begin{equation}
  \frac{df_i(x, p)}{dt} = \mathcal{C}_i(x, p)
\end{equation}
where $f_i$ is the distribution function for species $i$, and $\mathcal{C}_i$ is its microscopic collision kernel.
The model propagates all produced hadrons along classical trajectories, and simulates their scatterings, resonance formations and decays until the last interactions cease.

One primary advantage of using a microscopic transport model such as \texttt{UrQMD} as an afterburner, is that it realistically simulates the system break-up when the mean free path becomes large relative to the system size.
This dilute limit is expected to play a significant role in small collision systems where the produced medium is smaller and shorter lived.


\section{Parameter estimation}
\label{sec:calibration}

The nuclear collision model constructed in Sec.~\ref{sec:model} includes a number of free parameters $\x$ which describe the initial state, pre-equilibrium dynamics, and hydrodynamic medium.
Given values for the parameters $\x$, the model may be used to predict a vector of simulated experimental observables $\y_m$.
For example, $\y_m$ might be a vector consisting of charged particle yields in different centrality bins.
The physics model thus represents a vector-valued function $f(\x) = \y_m$ which maps the parameter values $\x$ to the simulated observables $\y_m$.

The goal of this work is to estimate the true model parameters $\x_\star$ provided some evidence that our model predictions $\y_m$ describe experimental measurements $\y_e$.
The problem involves three distinct components:
\begin{enumerate}[itemsep=0pt, leftmargin=2\parindent]
  \item $H_f$: the hypothesis that the nuclear collision model $f$ formulated in this work provides a realistic description of reality,
  \item $H_\x$: the hypothesis that the model parameters $\x$ are the true model parameters $\x_\star$ of $f$, and
  \item $E$: the evidence provided by the experimental data $\y_e$ and its corresponding uncertainties.
\end{enumerate}
As a practical matter, we always assume that hypothesis $H_f$ is correct, meaning there are no glaring flaws in our chosen theoretical framework.
This is a significant assumption; the application of hydrodynamic simulations to small collision systems is speculative, and our conclusions are conditional on the framework making sense.

Subject to this assumption, we can apply Bayes' theorem to evaluate the hypothesis $H_\x$ for the true model parameters
\begin{equation}
  \label{eq:bayes}
  P(H_\x | E) \propto P(E | H_\x)\, P(H_\x).
\end{equation}
The left-side of this expression is the \emph{posterior}: the probability for hypothesis $H_\x$ given the experimental evidence $E$.
On the right-side there are two separate terms.
The first term $P(E | H_\x)$ is the \emph{likelihood} function: the probability of observing the experimental evidence $E$ given our model and the hypothesis $H_\x$ for the true model parameters $\x_\star$, and the second term $P(H_\x)$ is the \emph{prior}: an estimate of the probability of hypothesis $H_\x$ in the absence of evidence $E$.

We assume that the likelihood function in Eqn.~\eqref{eq:bayes} is described by a multivariate normal distribution
\begin{equation}
  \label{eq:likelihood}
  P(E | H_\x) = \frac{1}{\sqrt{(2\pi)^m \det \Sigma}} \exp \left ( -\frac{1}{2}\Delta\y^\intercal \Sigma^{-1} \Delta\y \right ),
\end{equation}
where $\Delta\y = \y_m(\x) - \y_e$ is the discrepancy of the model and experiment, and $\Sigma$ is the total covariance matrix which includes all known sources of uncertainty for the simulated and measured observables.

\begin{table}[t]
  \caption{Input parameter ranges for the physics model.}
  \begin{ruledtabular}
  \begin{tabular}{lll}
    Parameter         & Description                        & Range             \\
    \paddedhline
    Norm              & Normalization factor                 & 9--28           \\
    $p$               & Entropy deposition parameter         & $-1$ to $+1$    \\
    $\sigmaf$         & Nucleon fluctuation std.\ dev.\      & 0--2            \\
    $w$               & Nucleon width parameter              & 0.4--1.2 fm     \\
    $n_c$             & Number of nucleon constituents       & 1--9            \\
    $\X$              & Nucleon structure parameter          & 0--1            \\
    $\dmin$           & Minimum inter-nucleon distance       & 0--1.7 fm       \\
    $\taufs$          & Free-streaming time                  & 0.1--1.5 fm/$c$ \\
    $\eta/s$ min      & Minimum value of $\eta/s$ (at $T_c$) & 0--0.2          \\
    $\eta/s$ slope    & Slope of $\eta/s$ above $T_c$        & 0--8 GeV$^{-1}$ \\
    $\eta/s$ crv      & Curvature of $\eta/s$ above $T_c$    & $-1$ to $+1$    \\
    $\zeta/s$ max     & Maximum value of $\zeta/s$           & 0--0.1          \\
    $\zeta/s$ width   & Width of $\zeta/s$ peak              & 0--0.1 GeV      \\
    $\zeta/s$ $T_0$   & Temperature of $\zeta/s$ maximum     & 150--200 MeV    \\
    $T_\text{switch}$ & Switching/particlization temp.       & 135--165 MeV    \\
  \end{tabular}
  \end{ruledtabular}
  \label{tab:design}
\end{table}

\subsection{Parameter design}

For the prior $P(H_\x)$, we specify ranges, i.e.\ minimum and maximum values, for each parameter which are listed in Tbl.~\ref{tab:design}.
We assume the prior distribution is constant and nonzero within each specified range and zero otherwise.
The selected parameter ranges are intentionally wide to avoid clipping the calibrated posterior; for example, a previous analysis of the \trento\ model \cite{Bernhard:2016tnd} found $p \sim 0$, but we use a prior range $p \in [-1, 1]$ to account for differences in the present model, e.g.\ nucleon substructure, which could modify its posterior.
Several of the model parameters require special care and are reparametrized accordingly:
\begin{itemize}[leftmargin=2\parindent]
  \item The constituent fluctuations, modeled by the gamma random weights $\gamma_i$ in Eqn.~\eqref{eq:fluctuated_thick}, generate overall nucleon fluctuations which are suppressed by the number of constituents $n_c$ inside the nucleon.
    The observed nucleon fluctuation variance falls like $1 / n_c$ which means the natural range for the constituent fluctuations is larger when the number of constituents is larger and \emph{vice versa}.
    We therefore reparametrize the constituent fluctuations using the standard deviation of the resulting nucleon fluctuations
    \begin{equation}
      \sigma_\mathrm{fluct} = 1 / \sqrt{k\, n_c},
    \end{equation}
    where $k$ is the shape parameter of the gamma distribution used to fluctuate each constituent.
  \item In Sec.~\ref{sec:initial_state} we parametrized nucleon substructure using three degrees of freedom:
    \begin{enumerate*}[label=(\roman*)]
      \item a parameter $v$ for the Gaussian width of each constituent,
      \item a parameter $r$ for the Gaussian width of the radial distribution used to sample the constituent centers, and
      \item a parameter $n_c$ to vary the number of constituents inside the nucleon.
    \end{enumerate*}
    In the limit $n_c \to \infty$, the composite nucleon's root mean square (rms) radius is simply the convolution of its sampling radius $r$ and constituent width $v$ which add together in quadrature
  \begin{equation}
    \lim_{n_c \to\infty} \mathrm{RMS}\{\rho(\mathbf{x})\} = \sqrt{r^2 + v^2}.
  \end{equation}
  We therefore choose to reparametrize the sampling radius $r$ in terms of a new variable
  \begin{equation}
    \label{eq:nucleon_width}
    w = \sqrt{r^2 + v^2},
  \end{equation}
  depicted in Fig.~\ref{fig:nucleon_schematic}, which approximates the rms radius of the nucleon when the number of sampled constituents is large.
  We thus call $w$ a nucleon ``width'', although for smaller numbers of constituents, the \emph{actual} rms radius of our sampled nucleons can be significantly smaller than our width parameter $w$ due to fluctuations in the nucleon's center of mass, and hence one should account for the difference when discussing the nucleon's posterior rms radius.

\begin{figure}[t]
  \begin{tikzpicture}
    \draw[dashed] (0,0) circle (1.6cm);
    \foreach \theta in {60, 180, 300}{
      \draw ({\theta}:.8) circle (.65cm);
    }
    \coordinate (a) at (0, 0);
    \coordinate (b) at (.8, 1.385);
    \coordinate (c) at (.4, -.693);
    \coordinate (d) at (-1.45, 0);
    \coordinate (e) at (-.8, 0);
    \draw[<->] (a) -- (b) node[midway, xshift=-1ex, yshift=.8ex]{$w$};
    \draw[<->] (a) -- (c) node[midway, xshift=1ex, yshift=.8ex]{$r$};
    \draw[<->] (d) -- (e) node[midway, yshift=1ex]{$v$};
  \end{tikzpicture}
  \caption{Schematic illustrating the constituent sampling radius $r$, constituent width $v$ and nucleon width parameter $w = \sqrt{r^2 + v^2}$ for a nucleon with $n_c=3$ constituents.}
  \label{fig:nucleon_schematic}
\end{figure}

\begin{table*}
  \caption{
    \label{tab:observables}
    Experimental data used to calibrate the model.
  }
  \begin{ruledtabular}
  \begin{tabular}{cc}
    Pb-Pb $\sqrts=5.02$~TeV & $p$-Pb $\sqrts=5.02$~TeV \\
    \paddedhline
    Charged-particle multiplicity $d\nch/d\eta$, $|\eta| < 0.5$ \cite{Adam:2015ptt} & Charged-particle multiplicity $d\nch/d\eta$, $|\eta| < 1.4$ \cite{Adam:2014qja} \\
    \noalign{\smallskip}
  Two-particle flow cumulants  $\vnk{n}{2}$ for $n=2,3,4$, $|\eta| < 0.8$,  & Two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3$, $|\eta| < 2.4$, \\
    charged-particles, $|\Delta\eta| > 1$,\, $0.2 < p_T < 5.0$~GeV \cite{Adam:2016izf} & charged-particles, $|\Delta\eta| > 2$,\, $0.3 < p_T < 3.0$~GeV \cite{Chatrchyan:2013nka}\\
    \noalign{\smallskip}
    & Charged-particle mean $p_T$, $0.15 < p_T < 10$~GeV, $|\eta| < 0.3$ \cite{Abelev:2013bla}\\
  \end{tabular}
  \end{ruledtabular}
\end{table*}

\item Equation~\eqref{eq:nucleon_width} requires the nucleon width to be larger than the constituent width, i.e. $w > v$, lest the sampling radius $r$ turn imaginary, and thus we cannot vary the nucleon width $w$ and the constituent width $v$ independently.
  We therefore reparametrize the constituent width $v$ using a new variable $\X$ to interpolate between minimum and maximum allowed values:
  \begin{equation}
    v = v_\mathrm{min} + \X (v_\mathrm{max} - v_\mathrm{min}).
  \end{equation}
  Here we choose a minimum width $v_\mathrm{min} = 0.2$~fm determined by computational limits and a maximum width $v_\mathrm{max} = w$ equal to the nucleon width.
  Thus for $\X=0$, the nucleons consist of small, distinct hot spots, whereas for $\X=1$ the nucleon is a single Gaussian blob of width $w$.
  The parameter $\X$ thus varies the granularity of the nucleon while keeping the number of constituents $n_c$ and approximate size of the nucleon $w$ fixed.
\end{itemize}

\subsection{Observables}

The likelihood function \eqref{eq:likelihood} provides evidence for \mbox{(or against)} the model parameters $\x$ by comparing the model predictions $\y_m(\x)$ to experimental data $\y_e$.
We focus on simple experimental observables in the present study which are sensitive to the bulk properties of the produced medium.
We calculate for each set of model parameters the following observables at midrapidity:
\begin{itemize}[leftmargin=2\parindent, itemsep=0pt]
  \item Charged-particle multiplicity $d\nch/d\eta$.
  \item Identified particle yields $dN/dy$ of pions, kaons, and protons.
  \item Transverse energy production $dE_T/d\eta$.
  \item Charged particle mean transverse momentum $\langle p_T \rangle$ ($0.15 < p_T < 10$~GeV).
  \item Identified particle mean transverse momentum $\langle p_T \rangle$ of pions, kaons and protons.
  \item Mean transverse momentum fluctuations $\delta p_T / \langle p_T \rangle$ (charged particles, $0.15 < p_T < 2.0$~GeV).
  \item Two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3,4$\\ (charged particles, $0.2 < p_T < 5.0$~GeV, and \\$0.3 < p_T < 3.0$~GeV).
  \item Four-particle flow cumulant $\vnk{2}{4}$ \\(charged particles, $0.2 < p_T < 5.0$).
  \item Symmetric cumulants $\mathrm{SC}(4, 2)$ and $\mathrm{SC}(3,2)$.
\end{itemize}
Each observable is calculated from the list of final state particles produced by \texttt{UrQMD} using the same methods applied by experiment.
We generally match the kinematic cuts of all measurements with two exceptions: we use use a larger rapidity interval $|\eta| < 0.8$ than experiment for some boost-invariant observables to improve our finite particle statistics, and we do not apply a finite rapidity gap, e.g.\ $|\Delta \eta| > 1$, between pairs of particles when calculating the two-particle cumulant $\vnk{n}{2}$ since we already oversample particles from each hydrodynamic event, and this oversampling suppresses non-flow correlations.

At the time of this writing, many of the aforementioned experimental observables are not yet published for $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV.
We therefore restrict our calibration to the subset of measured and published observables listed in Tbl.~\ref{tab:observables}.
Notably absent from this list are the four-particle cumulants $\vnk{n}{4}$ at $\sqrts=5.02$~TeV despite being measured and published.
Unfortunately, the four-particle cumulants require minimum-bias event statistics an order of magnitude larger than those used in this work.
We therefore refrain from \emph{calibrating} on the four-particle cumulants, although we do show calculations of the four-particle cumulant $\vnk{2}{4}$ later in the text, using a single set of calibration parameters.

Most of the calibration observables listed in Tbl.~\ref{tab:observables} are calculated as a function of collision centrality, where centrality is defined using some measure of the underlying event activity, e.g.\ the charged particle yield in a given rapidity window.
When calculating these observables, we generate $\mathcal{O}(10^4)$ minimum-bias events at each design point and divide the events into centrality bins using the charged particle yield at midrapidity, similar to the procedure used by experiment.

However, for some observables such as $p$-Pb mean $p_T$ \cite{Abelev:2013bla} and flow cumulants $\vnk{n}{k}$ \cite{Chatrchyan:2013nka}, the experiments use a special high-multiplicity trigger to select rare, ultra-central events according to the number of charged particles produced or detector tracks offline.
These high-multiplicity bins are too selective for our modest minimum-bias event sample, and so a different procedure is required.
We exploit, for this purpose, the approximate correspondence
\begin{equation}
  \label{eq:bin_equiv}
  \ntrk \sim \nch \propto N_\rho
\end{equation}
between each event's initial secondary-parton multiplicity ${N_\rho = \int d^2x\,n(\x)}$ in Eqn.~\eqref{energy-momentum} and its final-state event activity, characterized by $\nch$ or $\ntrk$ \cite{Abelev:2013bla, Chatrchyan:2013nka}.

Consider, for example, an experimental multiplicity bin $(\nch^\text{low}, \nch^\text{high})$ with some kinematic cuts on $p_T$ and $\eta$.
We rescale this bin by the average multiplicity $\langle \nch \rangle$ of the corresponding minimum-bias event sample, i.e.\
\begin{equation}
  (\nch^\text{low}, \nch^\text{high}) \rightarrow (\nch^\text{low} / \langle \nch \rangle, \nch^\text{high} / \langle \nch \rangle)
\end{equation}
in order to reexpress each bin edge as a unitless variable.
These rescaled bins are then used to select initial condition events using the equivalence \eqref{eq:bin_equiv}:
\begin{equation}
  \label{eq:mult_trigger}
  \left (\frac{N^\text{low}_\rho}{\langle N_\rho \rangle}, \frac{N^\text{high}_\rho}{\langle N_\rho \rangle} \right) \leftrightarrow \left (\frac{\nch^\text{low}}{\langle \nch \rangle}, \frac{\nch^\text{high}}{\langle \nch \rangle} \right ).
\end{equation}

Finally, we mimic the method used by experiment and apply \eqref{eq:mult_trigger} to select rare high-multiplicity events from a continuous stream of minimum-bias \trento\ events that satisfy the correct relative multiplicity bin edges.
This of course means that, in addition to running a large sample of minimum-bias events for centrality binned observables, we must also generate (much like experiment) a separate sample of multiplicity triggered events.
In practice, we use a few hundred to a few thousand events per multiplicity bin, depending on the type of observable.

We also take stock of the statistical and systematic errors reported by each experiment and incorporate their uncertainty into the likelihood covariance matrix
\begin{equation}
  \Sigma = \Sigma_m + \Sigma_e,
\end{equation}
which includes uncertainty contributions from both the model $\Sigma_m$ and experimental data $\Sigma_e$.
The experimental contribution to the covariance $\Sigma_e$ is further broken down into statistical and systematic components,
\begin{equation}
  \Sigma_e = \Sigma_e^\text{stat} + \Sigma_e^\text{sys}.
\end{equation}

The statistical errors are uncorrelated and thus its covariance matrix is diagonal:
\begin{equation}
  \Sigma_e^\text{stat} = \text{diag}[(\sigma^\text{stat}_1)^2, (\sigma^\text{stat}_2)^2, \dots (\sigma^\text{stat}_m)^2 ],
\end{equation}
where $\sigma^\text{stat}_i$ is the statistical uncertainty of observable $y_i$ in the experimental observable vector $\y_e = [y_1, \dots, y_m]$.
The systematic errors, meanwhile, are typically correlated, but the correlation structure is not reported by the experiments so we assert a reasonable form.
We can expand the systematic covariance matrix as
\begin{equation}
  (\Sigma_e^\text{sys})_{ij} =  \rho_{ij} \sigma_i \sigma_j,
\end{equation}
where $\sigma_i$ and $\sigma_j$ are the systematic errors of observables $y_i$ and $y_j$ respectively, and $\rho_{ij}$ is the Pearson correlation coefficient between observable $y_i$ and $y_j$:
\begin{equation}
  \rho_{ij} = \frac{\text{cov}(y_i, y_j)}{\sigma_i \sigma_j},
\end{equation}
which satisfies $\rho_{ij}=1$ for $i=j$ and $|\rho_{ij}| \le 1$ for $i \ne j$.
We assume that each observable is correlated across different centrality/multiplicity bins, and uncorrelated with observables of a different type, e.g.\ correlations between yields and flows.
This is a crude simplifying assumption but it is better than neglecting the correlation structure of the experimental data entirely.

For the correlation structure between different observable bins, we assert a simple Gaussian form
\begin{equation}
  \rho_{ij}^\text{sys} = \exp \left[ -\frac{1}{2} \left(\frac{b_i - b_j}{l} \right)^2 \right],
\end{equation}
where $b_i$ and $b_j$ are the midpoints of two observable bins of a single type (centrality or relative multiplicity) and $l$ is a correlation length which describes how quickly the observable bins decorrelate as the distance between the bins increases.
We use centrality correlation lengths ${l=100\%}$ for all Pb-Pb observables and ${l=30\%}$ for the $p$-Pb charged particle yield $d\nch/d\eta$.
The $p$-Pb mean $p_T$ and flow observables, meanwhile, use relative multiplicity bins $\nch / \langle \nch \rangle$ and $\ntrk / \langle \ntrk \rangle$ so we use a much smaller correlation length $l=5$ to reflect the smaller range of bin values.

\fig*{correlation_matrices}{
  Correlation matrices for Pb-Pb collisions at $\sqrts=5.02$~TeV for the model/emulator (left) and experimental data (right) used to construct the covariance matrix ${\Sigma = \Sigma_\mathrm{model} + \Sigma_\mathrm{expt}}$ in Eqn.~\eqref{eq:likelihood}.
  Experimental statistical and systematic errors are from ALICE and CMS \note.
  The experimental correlation structure is modeled using Eqn.~?.
}

\subsection{Model emulator}

In principle, one could calculate the likelihood function in Eqn.~\eqref{eq:likelihood} directly, e.g.\ by running the model a large number of times at a given parameter point $\x$ to calculate the model observables $\y_m(\x)$ from the ensemble of simulated events, but in practice such a procedure would be intractable.
The model is computationally intensive to evaluate, ${\sim}10$~min/event per processor core, and thousands of events are required to calculate the simplest observables at a single parameter point.
Moreover, we need to evaluate Eqn.~\eqref{eq:likelihood} \emph{numerous} times in order to sample the multidimensional posterior distribution so that the samples may be histogrammed and visualized.

We therefore follow an established framework for computationally intensive models and train a Gaussian process emulator (GPE) to act as a fast surrogate for the full physics simulation \note.
The GPE enables essentially instantaneous predictions for $\y_m = f(\x)$ which allows us to sample the posterior distribution millions of times.
In order to train the GPE, we first generate a scaffolding of the parameter space using a maximin Latin hypercube design to distribute 500 points uniformly in our 15-dimensional parameter space according to the ranges in Tbl.~\ref{tab:design}.
We then run minimum-bias and multiplicity triggered $p$-Pb and Pb-Pb events at each design point, and calculate the model observables from the ensemble of events.

Let $X$ denote the $d \times m$ dimensional matrix of design points where $d=500$ is the number of design points and $m$ is the number of emulated observables $y \in \y_m$.


\subsection{Experimental }


\section{Results}
\label{sec:results}

\fig*[t]{validation_example}{
  Placeholder
}

\begin{figure}[h]
  \makebox[\textwidth]{
    \begin{minipage}[t]{\paperwidth}
      \includegraphics[width=.95\paperwidth]{posterior}
      \captionsetup{
        width={.83\textwidth},
        justification=raggedright,
      }
      \caption{Bayesian posterior distribution of the model input parameters. The diagonal panels show the marginalized distributions of individual model parameters, while off-diagonal panels show the joint distributions for pairs of model parameters, visualizing their correlations. The marginalized distribution medians and 90\% credible intervals are annotated along the diagonal.}
    \end{minipage}
  }
  \label{fig:posterior}
\end{figure}

\begin{figure*}[p]
  \includegraphics{observables_pbpb}
  \caption{Simulated observables compared to experimental data for Pb-Pb collisions at $\sqrts=5.02$~TeV. Top row: explicit model calculations (no emulator) for each of the 500 design points; bottom row: emulator predictions for $n=100$ random samples drawn from the posterior. Points with error bars are experimental data from ALICE with statistical and systematic errors added in quadrature \note.}
  \vspace{5ex}
  \includegraphics{observables_ppb}
  \caption{Same as above but for $p$-Pb collisions at $\sqrts=5.02$~TeV. Note that multiplicity bins are used for mean $p_T$ and flow cumulant observables to match the bins used by experiment. Experimental data are from ALICE and CMS \note.}
\end{figure*}

\fig*[t]{observables_map}{
  Model calculations using the \emph{maximum a posterior} (MAP) parameters compared to experiment.
Colored lines are model calculations for $p$-Pb collisions (left) and Pb-Pb collisions (right) at $\sqrts=5.02$~TeV. Points with error bars are the experimental data with statistical uncertainties, and gray bands their corresponding systematic uncertainties, from ALICE and CMS \note. The ratio axes show the ratio of model over data where where available with gray bands indicating $\pm 10\%$.
}

\fig*[t]{flow_corr}{
  Model calculations of the symmetric cumulants for Pb-Pb collisions at $\sqrts=5.02$~TeV using the \emph{maximum a posteriori} (MAP) parameters.
}

\fig{posterior_proton_shape}{
%  Posterior distribution for the constituent sampling radius $\R$ and constituent width $\w$. The white trapezoidal region (lower-right) covers the initial range of allowed values for $\R$ and $\w$, while the gray triangular region (upper-left) indicates values which are excluded by the prior $\w > \R$. The posterior distribution, shown in blue, indicates the preferred values for $\R$ and $\w$ determined by the analysis.
}

\fig*[t]{region_shear_bulk}{
  Left figure: estimated temperature dependence of the QGP specific shear viscosity $(\eta/s)(T)$ determined by the present Bayesian analysis of $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV (orange line/band) compared to a previous Bayesian analysis of Pb-Pb collisions at $\sqrts=2.76$ and 5.02~TeV (blue line/band) \note. The lines are the medians of each posterior distribution, and the bands are their 90\% credible regions. Right figure: same as before, but for the temperature dependence of the QGP specific bulk viscosity $(\zeta/s)(T)$.
}

\section{summary}
\label{sec:summary}

\begin{acknowledgments}
  The authors thank ...
\end{acknowledgments}

\appendix*

\section{Adaptive grid sizing}
\label{appendix}
This is the appendix

\bibliography{substructure}

\end{document}
