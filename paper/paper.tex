\documentclass[aps,prc,reprint,amsmath,nofootinbib]{revtex4-1}

\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\graphicspath{{../plots/}{./fig/}}

\usepackage{tikz}
\usepackage{subcaption}

\usepackage{color}
\definecolor{theblue}{RGB}{0,50,230}

\usepackage[font=small, justification=raggedright, labelsep=quad]{caption}

\usepackage[inline]{enumitem}

\usepackage{booktabs}

\usepackage[pdfencoding=auto, psdextra]{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=theblue,
  citecolor=theblue,
  urlcolor=theblue
}

\newcommand{\trento}{T\raisebox{-0.5ex}{R}ENTo}
\newcommand{\sqrts}{\sqrt{s_\mathrm{NN}}}
\newcommand{\fmc}{\ensuremath{\text{fm}/c}}
\newcommand{\nch}{N_\text{ch}}
\newcommand{\ntrk}{N_\text{trk}^\text{offline}}
\newcommand{\vnk}[2]{v_#1\{#2\}}
\newcommand{\sigmaf}{\sigma_\text{fluct}}
\newcommand{\taufs}{\tau_\text{fs}}
\newcommand{\dmin}{d_\text{min}}
\newcommand{\Tsw}{T_\text{switch}}
\newcommand{\Tc}{T_c}
\newcommand{\T}{\tilde{T}}

\newcommand{\rc}{r_{cp}}
\newcommand{\nc}{n_c}
\newcommand{\wc}{w_c}

\newcommand{\smin}{{(\eta/s)_\mathrm{min}}}
\newcommand{\sslope}{{(\eta/s)_\mathrm{slope}}}
\newcommand{\scrv}{{(\eta/s)_\mathrm{crv}}}
\newcommand{\bmax}{{(\zeta/s)_\mathrm{max}}}
\newcommand{\bwidth}{{(\zeta/s)_\mathrm{width}}}
\newcommand{\bloc}{{(\zeta/s)_{T_\mathrm{peak}}}}

\newcommand{\bv}{\mathbf b}
\newcommand{\xv}{\mathbf x}
\newcommand{\yv}{\mathbf y}
\newcommand{\zv}{\mathbf z}
\newcommand{\ym}{{\mathbf y}_\text{model}}
\newcommand{\ye}{{\mathbf y}_\text{expt}}
\newcommand{\Sigmam}{\Sigma_\text{model}}
\newcommand{\Sigmae}{\Sigma_\text{expt}}
\newcommand{\paddedhline}{\noalign{\smallskip}\hline\noalign{\smallskip}}
\newcommand{\order}[1]{$\mathcal O(10^{#1})$}

\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\SC}{SC}
\DeclareMathOperator{\NSC}{NSC}

% Reduce the size of the itemize bullets
\renewcommand\labelitemi{$\vcenter{\hbox{\scriptsize$\bullet$}}$}

% Create a temporary full-page single column environment
\newenvironment{fullpage}{\onecolumngrid}{\clearpage\twocolumngrid}

% hyperref throws warning when a line break is used in the title
% this removes the warning by redefining the line break in a pdf string
\pdfstringdefDisableCommands{\def\\#1{ #1}}


\begin{document}

\title{
  Bayesian calibration of a hybrid nuclear collision model\\
  using $\boldsymbol p$-Pb and Pb-Pb data at energies available\\
  at the CERN Large Hadron Collider
}

\author{J.\ Scott Moreland}
\author{Jonah E.\ Bernhard}
\author{Steffen A.\ Bass}

\affiliation{Department of Physics, Duke University, Durham, NC 27708-0305}

\date{\today}

\begin{abstract}
  We posit a unified hydrodynamic and microscopic description of the quark-gluon plasma (QGP) produced in ultrarelativistic $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV and evaluate our assertion using Bayesian inference.
  Specifically, we model the dynamics of both collision systems using initial conditions with parametric nucleon substructure, a pre-equilibrium free streaming stage, event-by-event viscous hydrodynamics, and a microscopic hadronic afterburner.
  Free parameters of the model which describe the initial state and QGP medium are then simultaneously calibrated to fit charged-particle yields, mean $p_T$, and flow cumulants.
  We argue that the global agreement of the calibrated model with the experimental data strongly supports the existence of hydrodynamic flow in small collision systems at ultrarelativistic energies, and that the flow produced develops at length scales smaller than a single proton.
  Posterior estimates for the model's input parameters are obtained, and new insights into the temperature dependence of the QGP transport coefficients and event-by-event structure of the proton are discussed.
\end{abstract}

\maketitle

\section{Introduction}

Ultrarelativistic nuclear collisions between one light-ion and one heavy-ion, e.g.\ $^3$He-Au and $p$-Pb collisions, generate dense, compact sources of nuclear matter which produce long-range multiparticle correlations that are strikingly similar to the correlations observed in heavy-ion collisions where collectivity is commonly explained by the existence of hydrodynamic flow \cite{CMS:2012qk, Abelev:2012ola, Aad:2012gla, Adare:2015ctn}.
This observation suggests that hydrodynamic behavior could be manifest in small droplets of quark-gluon-plasma (QGP) \cite{Bozek:2011if, Bozek:2013uha}, and that flow even might develop at length scales smaller than a single proton \cite{Schenke:2014zha}.

Hydrodynamic models of ultrarelativistic nuclear collisions are complicated by a number of theoretical unknowns including the detailed geometry of the QGP initial conditions, the strength and duration of pre-equilibrium dynamics, the temperature dependence of QGP transport coefficients, and the boundaries of hydrodynamic applicability \cite{Niemi:2014lha, deSouza:2015ena, Ollitrault:2012cm, Song:2012ua}.
In general, these theoretical uncertainties tend to grow with decreasing system size, where emergent physics at sub-femtometer length scales becomes important to describe bulk properties of the produced system.

One method for reducing theoretical uncertainties is to test model calculations by varying the species of colliding nuclei at a single beam energy \cite{Adare:2015bua, Schenke:2014tga, Aidala:2018mcw, Adare:2017wlc, Adamczyk:2015obl, Shen:2016zpp, Aidala:2017ajz, Adare:2006ti}.
Since initial condition and hydrodynamic models generally factorize the structure of the colliding nuclei from the subsequent time dynamics of the collision, a proposed theory framework can be validated by testing its predictions for multiple collision systems using a single self-consistent set of model parameters, where only the nuclear structure in the model is permitted to vary.

Typically, the macroscopic structure of heavy nuclei, characterized e.g.\ by an atomic mass number and set of Woods-Saxon coefficients \cite{MOLLER1995185, DEVRIES1987495}, is regarded as a known input to hydrodynamic models which contributes negligible uncertainty to simulation predictions, outweighed by large uncertainties in modeling initial energy deposition and off-equilibrium dynamics \cite{Niemi:2014lha, Song:2011hk, Retinskaya:2013gca, Liu:2015nwa, Kurkela:2016vts}.
The geometry of light-ions, meanwhile, is naturally more sensitive to the detailed size and shape of individual protons and neutrons inside the nucleus, which may fluctuate event-by-event and differ significantly from the round blobs typically used to approximate nucleons in heavy-ion collisions \cite{Schenke:2014zha, Welsh:2016siu, Moreland:2017kdx, Schenke:2014gaa, Schlichting:2014ipa}.
These nucleon substructure properties are difficult to measure and calculate from first principles and hence contribute significant uncertainty to model predictions of small systems.

Early substructure studies replaced round nucleons with composite nucleons, described by a few salient model parameters, in order to investigate the effect of each parameter on simulated observables \cite{Adler:2013aqf, Mitchell:2016jio, Welsh:2016siu, Broniowski:2016pvx, Bozek:2017jog}.
These sensitivity studies were able to identify cause and effect relationships between model inputs and outputs, but lacked the ability to constrain nucleon substructure parameters in any kind of global or systematic fashion.
It quickly became apparent that numerous substructure implementations might be compatible with available data, and that additional work would be required to identify observables which are particularly sensitive to the average size, shape, and fluctuations of the nucleon.

Several such observables have been identified in proton-proton and proton-lepton scattering data.
Measurements by the TOTEM collaboration at $\sqrt{s}=7$~TeV, for instance, found an unexpected dip in the inelasticity density of $p$-$p$ collisions at zero impact parameter \cite{Antchev:2011zz}.
It was later realized that this depression, or so-called hollowness effect, in the $p$-$p$ inelastic collision profile \cite{Arriola2016} can be explained by the existence of correlated domains inside the proton, and that aspects of these domains, such as their size and correlation strength, may be constrained by comparing model predictions to inelastic $p$-$p$ measurements \cite{Albacete:2016gxu, Albacete:2016pmp}.

Independently, studies of coherent and incoherent $J/\psi$ production based on a color dipole picture of vector meson production were used to simultaneously constrain both the average color charge density of the proton as well as its event-by-event fluctuations in a saturation based framework \cite{Mantysaari:2016ykx, Mantysaari:2016jaz, Aaron:2009aa, Abramowicz:2015mha}.
Initial condition studies using the IP-Glasma model of Color Glass Condensate effective field theory \cite{Schenke:2012wb} simultaneously demonstrated that these color charge fluctuations leave a lasting imprint on the \mbox{small-x} gluon distribution of the proton and hence the initial geometry of QGP energy deposition \cite{Schlichting:2014ipa}.
In addition, it was recently shown that hydrodynamic simulations using IP-Glasma initial conditions with color charge fluctuations calibrated to fit coherent and incoherent $J/\psi$ diffraction measured by the H1 and Zeus experiments at HERA \cite{Aaron:2009aa, Abramowicz:2015mha} provide a good description of collectivity in small and large collision systems \cite{Schenke:2018fci}.

Model parameters, such as those calibrated by the aforementioned studies, are of course always in some degree of tension.
For instance, fitting one observable may require parameter values that degrade the quantitative description of some other observable.
Similarly, parameters which provide an optimal description of small-system observables may lead to a sub-optimal description of heavy-ion observables or \emph{vice versa}.
It is thus import to look at the experimental data holistically, and to use model calibration methods which
\begin{enumerate*}[label=(\arabic*)]
  \item
    explore all parameter combinations and
  \item
    compare model predictions to all relevant experimental measurements in a statistically rigorous fashion.
\end{enumerate*}

With these considerations in mind, we present progress toward a fully global analysis of $p$-Pb and Pb-Pb bulk observables at $\sqrts=5.02$~TeV using a model calibration framework known as Bayesian parameter estimation.
We begin in Sec.~\ref{sec:model} by constructing a nuclear collision model for $p$-Pb and Pb-Pb collisions using initial conditions with parametric nucleon substructure and transport dynamics described by a pre-equilibrium free streaming stage, viscous hydrodynamics, and microscopic Boltzmann transport.
In Sec.~\ref{sec:calibration}, we calibrate free parameters of the model to fit charged-particle yields, mean $p_T$, and anisotropic flow cumulants of \emph{both} collision systems at $\sqrts=5.02$~TeV, and finally, in Secs.~\ref{sec:results} and \ref{sec:summary}, we present posterior results for the model input parameters and comment on the implications for hydrodynamic descriptions of small collision systems.

\section{Nuclear collision model}
\label{sec:model}

We employ a multi-stage hybrid transport model that uses relativistic viscous hydrodynamics to describe the QGP medium and microscopic Boltzmann transport to simulate the dynamics of the system after hadronization \cite{Shen:2014vra, Bernhard:2016tnd}.
The hydrodynamic initial conditions are provided by a modified version of the \trento\ model \cite{Moreland:2014oya} with additional parameters to vary the number and size of hot spots inside the nucleon.
Each initial condition profile is free streamed to the hydrodynamic starting time and matched onto the hydrodynamic energy-momentum tensor using the Landau matching procedure \cite{Broniowski:2008qk, Heinz:2015arc}.
Many of the components of the present model have been documented in previous studies \cite{Moreland:2014oya, Bernhard:2016tnd, Bernhard:2018hnz}; we review each component here for completeness.

\subsection{Initial state}
\label{sec:initial_state}

We model the initial state of $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV using the boost-invariant \trento\ model \cite{Moreland:2014oya}.
Generally speaking, the initial three-dimensional energy density deposited by relativistic nuclear collisions is \emph{not} boost-invariant.
The longitudinal energy density fluctuates both locally point-to-point in the transverse plane as well as globally event-by-event due to asymmetries in the sampled density of participant matter \cite{Ke:2016jrd, Bozek:2010vz}.
Nevertheless, boost-invariance has been shown to be a good approximation for both large and small collision systems when hydrodynamic observables are calculated from particles that are detected close to midrapidity \cite{Shen:2016zpp}.

The \trento\ model operates in the ultrarelativistic limit with a Lorentz factor $\gamma \gg 1$ such that each nucleus appears as a thin sheet of nuclear density in the laboratory frame.
The sheets of colliding nuclear density penetrate and pass through each other in time $\tau_\text{overlap} \approx D_\text{nucl} / (\gamma\, \beta_z)$ in the laboratory frame, where $D_\text{nucl}$ is the diameter of the nucleus in its rest frame, $\gamma$ is the usual Lorentz factor of the accelerated ions, and $\beta_z$ is their velocity along the beam axis.
The resulting nuclear overlap time $\tau_\text{overlap} \lesssim 0.1\ \fmc$ at top RHIC and LHC energies, and thus it is reasonable to neglect the initial transverse dynamics which occur while the nuclei pass through each other.
We therefore assume that the collision produces all secondary particles at a uniform proper time $\tau_0 \ll 1~\fmc$, and that it deposits energy at midrapidity which is a function of the locally varying transverse density inside each nucleus.

Consider the collision of two nucleons, labeled $A$ and $B$, with three-dimensional densities $\rho_A$ and $\rho_B$ in their local rest frames.
The nucleon-nucleon overlap function
\begin{equation}
  \label{eq:tpp}
  T_{nn}(\bv) = \int d^2x_\perp \int dz_1\, \rho_A(\xv_\perp, z_1) \int dz_2\, \rho_B(\xv_\perp - \,\bv, z_2)
\end{equation}
describes the eikonal overlap of the two nucleons at impact parameter $\bv$ in the transverse plane $\xv_\perp$, orthogonal to the beam axis coordinates $z_{1,2}$.
Here we assume that each nucleon is comprised of smaller constituents---e.g.\ valence quarks, sea quarks, and small-x gluons---which may collide to produce secondary particles and contribute to the observed inelastic nucleon-nucleon cross section.

Within a picture of independent pairwise collisions between the constituents, a Glauber model model may be used to calculate the probability $P_\mathrm{coll}$ that the two nucleons collide inelastically at impact parameter $\bv$.
In the limit when the number of constituents is large, it yields the particularly simple form
\begin{equation}
  \label{eq:pcoll}
  P_\mathrm{coll}(\bv) = 1 - \exp[-\sigma_\mathrm{eff}\, T_{nn}(\bv)],
\end{equation}
where $\sigma_\mathrm{eff}$ is the effective cross section for inelastic collisions between each pair of constituents, determined by fitting the inelastic proton-proton cross section
\begin{equation}
  \label{eq:sigma_nn}
  \sigma_{pp}^\mathrm{inel} = \int d^2b\, P_\mathrm{coll}(\bv)
\end{equation}
at the specified collision energy $\sqrts$.
We tune $\sigma_\mathrm{eff}$ in the present work to fit the experimental inelastic nucleon-nucleon cross section $\sigma_\text{NN}^\text{inel}= 7.0$~fm$^2$ at $\sqrts=5.02$~TeV for comparison with our chosen datasets \cite{ALICE:2012xs}.
The resulting \trento\ inelastic nucleon-nucleon cross section agrees with the experimental value to better than 2\% accuracy, as verified by one of the model's standard unit tests.

The nucleon densities $\rho_{A,B}$ in Eq.~\eqref{eq:tpp} are commonly modeled using a spherically symmetric distribution.
For instance, the original implementation of the \trento\ model uses Gaussian nucleons, largely because it yields a simple analytic solution to Eq.~\eqref{eq:pcoll}.
Needless to say, such approximations are admittedly crude and may have a significant effect on the dynamics of small collision systems where the nucleon size is comparable to the size of the produced QGP medium.

A number of previous studies have investigated the effects of deformed or ``lumpy'' nucleons.
One common implementation is a superposition of three valence quarks, typically described by Gaussian or exponential form factors \cite{Welsh:2016siu, Bozek:2017jog, Schenke:2014zha, Schlichting:2014ipa, Adare:2015bua, Broniowski:2016pvx}.
The corresponding nucleon density is then assumed to be that of predominantly small-x gluons, seeded by the distribution of color charge in each of the three valence quarks.

\begin{figure}
  \begin{tikzpicture}
    % spherical nucleon
    \draw[dashed, xshift=-2.5cm] (0,0) circle (1cm);
    % three partons
    \draw[dashed] (0,0) circle (1cm);
    \foreach \theta in {0, 120, 240}{
      \draw ({\theta}:.5) circle (.4cm);
    }
    % ten partons
    \draw[dashed, xshift=2.5cm] (0,0) circle (1cm);
    \foreach \theta/\radius in {
      0/0.6, 40/0.5, 90/0.6, 130/0.5, 180/0.6, 220/0.6,
      260/0.5, 320/0.6, 300/0.2
    }{
      \draw[xshift=2.5cm] ({\theta}:\radius) circle (.3cm);
    }
  \end{tikzpicture}
  \caption{
    \label{fig:substructure}
    Schematic of plausible nucleon shapes.
    The sketch on the left shows a spherically symmetric nucleon (dashed line), while the middle and right illustrations depict a fluctuating nucleon with three and nine constituents respectively (solid lines).
  }
\end{figure}

In this work, we pursue a less restrictive and more parametric description of the nucleon where the number of substructure degrees of freedom is uncertain as depicted in Fig.~\ref{fig:substructure}.
We model each nucleon's three-dimensional density $\rho_{A,B}$ as a sum of $\nc$ independent constituents
\begin{equation}
  \label{eq:rho}
  \rho_{A, B}(\xv) = \frac{1}{\nc} \sum\limits_{i=1}^{\nc} \rho_c(\xv - \xv_i),
\end{equation}
where each constituent density $\rho_c$ is described by a Gaussian distribution
\begin{equation}
  \rho_c(\xv) = \frac{1}{(2 \pi \wc^2)^{3/2}} \exp\left(- \frac{|\xv|^2}{2 \wc^2}\right)
\end{equation}
of variable width $\wc$.
The constituent positions $\xv_i$ are sampled independently (ignoring correlations) from a Gaussian probability distribution
\begin{equation}
  \label{eq:radial_dist}
  P(\xv_i) = \frac{1}{(2\pi \rc^2)^{3/2}} \exp\left(-\frac{|\xv_i - \xv_n|^2}{2\, \rc^2}\right),
\end{equation}
where $\rc$ is a free parameter that varies the dispersion of the constituent positions $\xv_i$ about each nucleon position $\xv_n$.
As a matter of convenience, we sample the nucleon positions \emph{before} determining the constituent positions.
This leads to small discrepancies between the designated nucleon positions and the actual position of each nucleon's center-of-mass, owing to fluctuations in the constituent positions.
The parameter $\rc$ should thus be interpreted with care.
It is a computational sampling radius, \emph{not} the Gaussian width of the sampled nucleons in their center-of-mass frame.

The two nucleons $A$, $B$ are assigned a random impact parameter offset $\bv$, and Eq.~\eqref{eq:pcoll} is used to sample their inelastic collision probability $P_\mathrm{coll}(\bv)$.
Note, this collision probability has no direct knowledge of the individual constituent positions; it is only \emph{indirectly} sensitive to the constituent positions via their effect on the nucleon densities $\rho_{A, B}$.
The constituents are merely used as a mechanism to deform and fluctuate each nucleon profile.

This is an important distinction between the present model and a similar nucleon substructure implementation known as the participant quark model, which allows for a subset of quarks (constituents) to participate inside a single nucleon \cite{ANISOVICH1978477, Broniowski:2016pvx}.
The nucleon, unlike the nucleus, cannot produce semi-stable spectator fragments in a high-energy collision.
Any spectator quarks produced by a wounded quark model would be colored objects that necessarily contribute to secondary particle production.
We correspondingly require that the nucleons in Eq.~\eqref{eq:rho} participate as singular objects, such that all spectator matter discarded by the simulation is appropriately color-neutral and inert.

Hereafter, we switch to Milne coordinates $(\xv_\perp, \eta_s, \tau)$, where $\xv_\perp$ is a Cartesian coordinate $(x, y)$ in the transverse plane, $\eta_s = \tfrac{1}{2} \log[(t+z)/(t-z)]$ is the system's spacetime rapidity, and $\tau = \sqrt{t^2 - z^2}$ its proper time.
Assuming the nucleons collide at the sampled impact parameter $\bv$, we assign each nucleon a participant thickness
\begin{equation}
  \label{eq:fluctuated_thick}
  \T_{A, B}(\xv_\perp) \equiv \int dz\, \tilde{\rho}_{A,B}(\xv_\perp \pm \bv/2, z),
\end{equation}
which projects its \emph{fluctuated} nucleon density
\begin{equation}
  \tilde{\rho}_{A,B}(\xv) \equiv \frac{1}{\nc} \sum\limits_{i=1}^{\nc} \gamma_i\, \rho_c(\xv - \xv_i)
\end{equation}
onto the transverse plane $\xv_\perp$ centered at midrapidity.
This fluctuated nucleon density $\tilde{\rho}_{A,B}$ equals the nucleon density $\rho_{A,B}$ in Eq.~\eqref{eq:rho} with additional weights $\gamma_i$ sampled from a gamma distribution with unit mean and variance $1/k$.
Such \emph{ad hoc} random weights are necessary to describe the large fluctuations observed in minimum bias proton-proton collisions, although their exact physical origin is not well understood.

The resulting nucleon fluctuation variance naturally falls like ${\sim}1/ \nc$, where $\nc$ is the number of constituents.
This means that the natural range for the constituent fluctuations is larger when the number of constituents is larger.
We therefore reparametrize the constituent fluctuations using
\begin{equation}
  \sigma_\mathrm{fluct} = 1 / \sqrt{k\, \nc},
\end{equation}
where $k$ is the shape parameter (inverse variance) of the gamma distribution weights and $\nc$ is the number of nucleon constituents.

The energy density $e$ deposited at midrapidity $\eta_s=0$ and proper time $\tau_0 \ll 1~\fmc$ is then some function
\begin{equation}
  e(\xv_\perp, \eta_s=0, \tau_0) = f(\T_A, \T_B)
\end{equation}
of the local participant thicknesses $\T_A$ and $\T_B$.
A natural first guess for this mapping is an arithmetic mean
\begin{equation}
  e(\xv_\perp, \eta_s=0, \tau_0) = \text{const} \times \frac{\T_A + \T_B}{2},
\end{equation}
equal to a participant or ``wounded nucleon'' model for initial energy deposition up to meaningless factor of two in the overall normalization constant.
This wounded nucleon scaling was in fact one of the first such mappings used as a proxy for initial particle production in relativistic heavy-ion collisions \cite{Bialas:1976ed}.
It was subsequently realized, however, that the wounded nucleon model predicts the wrong scaling for charged-particle production as a function of collision centrality \cite{Kharzeev:2000ph} and hence the wrong scaling for midrapidity energy deposition as a function of participant density.\footnote{Typically, wounded nucleon scaling is used to parametrize the entropy density $s$, but the shortcomings of the model are nevertheless the same when parametrizing the system's energy density $e$. Both parametrizations underpredict the steep rise of particle production observed in central collisions.}

A simple remedy is to replace the arithmetic mean of the wounded nucleon model with a more flexible parametrization
\begin{equation}
  \label{eq:gmean}
  e(\xv_\perp, \eta_s=0, \tau_0) = \text{const} \times M_p(\T_A, \T_B),
\end{equation}
where $M_p$ is a family of functions known as the generalized means
\begin{equation}
  M_p(x, y) = \left( \frac{x^p + y^p}{2} \right)^{1/p}.
\end{equation}
This parametrization introduces a dimensionless parameter $p$ which varies the scaling behavior of initial energy deposition at midrapidity.
For certain discrete $p$ values, it reduces to well known functional forms such as the arithmetic, geometric, and harmonic means:
\begin{equation}
  \newlength{\extraspace}
  \setlength{\extraspace}{0.5ex}
  M_p(x, y) =
  \begin{cases}
    \max(x, y) & p \rightarrow +\infty, \\[\extraspace]
    (x + y)/2 & p = +1, \hfill \text{ (arithmetic)} \\[\extraspace]
    \sqrt{x y} & p = 0, \hfill \text{ (geometric)} \\[\extraspace]
    2\, x y/(x + y) & p = -1, \hfill \text{ (harmonic)} \\[\extraspace]
    \min(x, y) & p \rightarrow -\infty.
  \end{cases}
  \label{eq:trento_p}
\end{equation}

Note, the form of Eq.~\eqref{eq:gmean} differs somewhat from our previous studies \cite{Moreland:2014oya, Bernhard:2016tnd}, which parametrized the system's \emph{entropy} density using the generalized mean ansatz:
\begin{equation}
  s(\xv_\perp, \eta_s=0, \tau_0) \propto M_p(\T_A, \T_B).
\end{equation}
Our motivation for reinterpreting the left-side of this equation as an energy density in the present work is to enable the application of pre-equilibrium free streaming equations (described shortly) which require the system's initial energy density as input.
This modification is permissible since the \trento\ model is formulated using particle yield observations and the approximate scaling relation $d\nch/d\eta \propto M_p(\T_A, \T_B)$, which remains valid for both static entropy density and free streamed energy density initialization.

\begin{figure}
  \begin{tikzpicture}
    \node {\includegraphics{thickness}};
    \node[label=above:{10 fm}] (a) at (0, 1.1) {};
    \node (b) at (0, -1.1) {};
    \draw [<->, semithick] (a) to (b);
  \end{tikzpicture}
  \caption{
    \label{fig:thickness}
    Effect of nucleon substructure on the nuclear thickness function $T(x, y) \equiv \int dz\, \rho(x, y, z)$ of a $^{208}\mathrm{Pb}$ nucleus.
    The nucleus on the left has Gaussian nucleons of width $0.8$~fm, while the nucleus on the right has composite nucleons, each containing six constituents of width $0.4$~fm.
  }
\end{figure}

Up to this point we have restricted our attention to a single nucleon-nucleon collision.
Equation~\eqref{eq:gmean} is a purely local function of nuclear density in the transverse plane and hence it should, in principle, be equally valid for any pair of colliding nuclei at sufficiently high beam energy.
The \trento\ model readily generalizes from individual nucleon-nucleon collisions to arbitrary nucleus-nucleus collisions by summing the participant thicknesses $\T_{A,B}$ in Eq.~\eqref{eq:fluctuated_thick} over all nucleons which participate in one or more inelastic collisions.
The only modeling difference between $p$-$p$, $p$-Pb, and Pb-Pb collisions is the number and the position of the nucleons.
Figure~\ref{fig:thickness} shows the effect of adding nucleon substructure to a generic lead nucleus.
Additional fluctuations emerge over the length scale of a nucleon, but the macroscopic geometry of the nucleus is largely unchanged.

When applying the model to heavy-ions, we sample nucleon positions from a Woods-Saxon density distribution subject to a minimum distance criteria $|\xv_i - \xv_j| > d_\mathrm{min}$ between all pairs of nucleons $i$, $j$.
The minimum distance algorithm, first described in Ref.~\cite{Bernhard:2018hnz}, uses a simple trick to resample the nucleon positions without modifying the target Woods-Saxon radial distribution.
We first presample the radii of all nucleons in a given nucleus and sort them in ascending order.
We then sample the solid angles of each nucleon one-by-one, starting with the nucleon closest to the center of the nucleus and working our way outwards.
If a sampled nucleon position is too close to any of its previously placed neighbors, its solid angle is resampled---but not its radial coordinate---until the minimum distance criteria is satisfied.
Similar methods could be used to model correlations between individual constituents inside each nucleon, although the implementation would be somewhat tedious.

\begin{figure}
  \includegraphics{coupling}
  \caption{
    \label{fig:coupling}
    Cartoon of the free streaming approximation for hydrodynamic initialization. The initial state is free streamed for proper time $\taufs$ (zero coupling) before it is matched to hydrodynamics (strong coupling).
    This piecewise evolution approximates the more realistic scenario expected in nature where the medium's coupling strength smoothly changes as a function of time.
  }
\end{figure}

\subsection{Pre-equilibrium dynamics}

There are of course two limiting cases for the strength of interactions inside the QGP medium immediately after the collision: infinitely weak coupling where the secondary partons free stream without interacting, and infinitely strong coupling where the fluid's inter-particle mean-free-path effectively vanishes.
Realistically, one expects the initial parton interactions to lie somewhere between these two extremes.
We therefore choose to model the QGP's initial off-equilibrium dynamics using a simple step-function approximation, depicted in Fig.~\ref{fig:coupling}, which free streams the initial state for proper time $\taufs$ (zero coupling) before instantaneously switching to viscous hydrodynamics (strong coupling) \cite{Liu:2015nwa, Broniowski:2008qk}.
The free parameter $\taufs$ allows us to parametrically vary the \emph{time averaged} coupling strength during the initial stage of the collision.

The parametric energy deposition ansatz in Eq.~\eqref{eq:gmean} does not provide any information about the initial masses or momenta of particles produced in the collision.
In general, these details will affect the dynamics predicted by the collisionless Boltzmann equation
\begin{equation}
  \label{freestream}
  p^\mu \partial_\mu f(x, p) = 0
\end{equation}
through its dependence on the underlying distribution function $f(x, p)$.
Equation~\eqref{freestream}, however, simplifies for a boost-invariant gas of massless noninteracting partons with locally isotropic transverse momentum distributions.
Subject to these assumptions, the energy-momentum tensor $T^{\mu\nu}(\xv_\perp, \eta_s=0, \tau)$ at transverse coordinate $\xv_\perp$ and time $\tau > \tau_0$ equals \cite{Broniowski:2008qk, Liu:2015nwa}
\begin{multline}
  \label{energy-momentum}
  T^{\mu\nu}(\xv_\perp, \eta_s=0, \tau) =\\
  \frac{\tau_0}{\tau} \int_{-\pi}^{\pi} d\phi_p\, \hat{p}^\mu \hat{p}^\nu e(\xv_\perp - (\tau - \tau_0)\hat{\mathbf{p}}_\perp, \eta_s=0, \tau_0),
\end{multline}
where $\hat{p}^\mu = (1, \cos \phi_q, \sin \phi_q, 0)$ and $\hat{\mathbf{p}}_\perp = (\cos \phi_q, \sin \phi_q)$ are momentum unit vectors.
Here we assume that the time $\tau_0 \ll 1~\fmc$ is small and define $\taufs \equiv \tau - \tau_0 \approx \tau$.
We also combine the unknown constant in Eq.~\eqref{eq:gmean} with the pre-factor $\tau_0$ in Eq.~\eqref{energy-momentum} to produce a single normalization factor $\text{Norm} = \text{const} \times \tau_0$ with units of energy.
The solution \eqref{energy-momentum} is then decomposed in hydrodynamic form
\begin{equation}
  \label{hydro-eqn}
  T^{\mu\nu} = e u^\mu u^\nu - (P + \Pi) \Delta^{\mu\nu} + \pi^{\mu\nu},
\end{equation}
where $e$ and $P$ are the energy density and pressure in the local fluid rest frame, $u^\mu$ is the local fluid velocity, ${\Delta^{\mu\nu} = g^{\mu\nu} - u^\mu u^\nu}$ is the projector onto the space orthogonal to $u^\mu$, and $\Pi$ and $\pi^{\mu\nu}$ are the bulk pressure and shear stress tensor respectively.
We then solve for the energy density $e$ and fluid velocity $u^\mu$ using the Landau matching condition which defines the fluid rest frame velocity as the time-like eigenvector of $T^{\mu\nu}$ with energy density $e$ as its eigenvalue
\begin{equation}
  T^{\mu\nu} u_\nu = e u^\mu.
\end{equation}
The initial bulk and shear corrections are finally solved for by subtracting the ideal pressure from the total pressure to find $\Pi$, then solving for $\pi^{\mu\nu}$ using Eq.~\eqref{hydro-eqn}:
\begin{align}
  \Pi &= -\frac{1}{3} \mathrm{Tr}(\Delta_{\mu\nu} T^{\mu\nu}) - P,\\
  \pi^{\mu\nu} &= T^{\mu\nu} - e u^\mu u^\nu + (P + \Pi) \Delta^{\mu\nu}.
\end{align}

This procedure provides initial values for $T^{\mu\nu}$, $u^\mu$, $\Pi$, and $\pi^{\mu\nu}$ which conserve energy and are consistent with the underlying hydrodynamic equation of state.
We therefore expect it to provide a more realistic description of the initial stages of the collision as compared to a previous study using the \trento\ initial condition model which set $\Pi$, $\pi^{\mu\nu}$, and $u^\mu$ initially to zero \cite{Bernhard:2016tnd}.

\subsection{Hydrodynamics}

After free streaming for proper time $\taufs$, we transition to viscous hydrodynamics which solves the conservation equations
\begin{equation}
  \label{eq:continuity}
  \partial_\mu T^{\mu\nu} = 0
\end{equation}
for the hydrodynamic energy-momentum tensor $T^{\mu\nu}$ expressed in Eq.~\eqref{hydro-eqn} using a set of second-order Israel-Stewart equations formulated in the 14-moment approximation
\cite{Israel:1979wp, Israel:1976aa, Denicol:2012cn, Denicol:2010xn}.
This produces a pair of relaxation-type equations
\begin{subequations}
  \label{eq:relaxation}
  \begin{align}
    \tau_\Pi \Pi + \dot{\Pi} &=
      - \zeta \theta - \delta_{\Pi\Pi} \Pi\theta
      + \lambda_{\Pi\pi} \pi^{\mu\nu} \sigma_{\mu\nu}, \\[1ex]
    \tau_\pi \dot{\pi}^{\langle \mu\nu \rangle} + \pi^{\mu\nu} &=
      2\eta\sigma^{\mu\nu} - \delta_{\pi\pi} \pi^{\mu\nu} \theta
      + \phi_7 \pi_\alpha^{\langle \mu} \pi^{\nu \rangle \alpha} \nonumber \\
      &\qquad {} - \tau_{\pi\pi} \pi_\alpha^{\langle \mu}\sigma^{\nu \rangle \alpha}
      + \lambda_{\pi\Pi} \Pi \sigma^{\mu\nu},
  \end{align}
\end{subequations}
for the bulk pressure $\Pi$ and shear-stress $\pi^{\mu\nu}$.
We model the shear viscosity $\eta$ and bulk viscosity $\zeta$ as unknown temperature dependent quantities and fix the remaining transport coefficients $\{\tau_\Pi, \delta_{\Pi\Pi}, \lambda_{\Pi\pi}, \tau_\pi, \delta_{\pi\pi}, \phi_7, \tau_{\pi\pi}, \lambda_{\pi\Pi}\}$ using analytic results derived in the limit of small but finite masses \cite{Denicol:2014vaa}.

\begin{figure}[t]
  \includegraphics{viscosity_dof}
  \caption{
    \label{fig:viscosity_dof}
    Degrees of freedom in the temperature dependent shear and bulk viscosity parametrizations. Lines are chosen for illustrative purposes only and do not represent all possible variability.
    For instance, $\eta/s$ could have a large slope and negative curvature, or $\zeta/s$ could have a large max and narrow width, neither of which are depicted above.
  }
\end{figure}

The hydrodynamic equations of motion are necessarily closed using an equation of state (EoS) to relate the energy density $e$ and pressure $P$ of the produced medium.
We use a parametrization for $P(e)$ that matches a hadron resonance gas EoS at low temperature to a lattice QCD EoS at high temperature by smoothly connecting their trace anomalies in the interval $165 \le T \le 200$~MeV \cite{Bernhard:2018hnz}.
For the lattice EoS, we use a calculation by the HotQCD collaboration for (2+1)-flavor QCD which was extrapolated to the continuum limit \cite{Bazavov:2014pvz}.
Recent developments in lattice QCD now enable calculations in (2+1+1)-flavors \cite{Borsanyi:2016ksw}, i.e.\ with thermalized charm quarks, and the additional charm flavor has been shown to visibly affect predictions of $p_T$-differential flow observables \cite{Noronha-Hostler:2018zxc}.
Investigating this sensitivity would thus be a natural target for future improvements to the present work.

We parametrize the temperature dependence of the QGP viscosities in order to marginalize over their uncertainty when calibrating to data.
For the specific shear viscosity $\eta/s$, we use a modified linear ansatz
\begin{equation}
  \label{eq:shear_viscosity}
(\eta/s)(T) = \smin + \sslope \cdot (T - \Tc) \left ( \frac{T}{\Tc} \right)^\scrv,
\end{equation}
where $\smin$, $\sslope$, and $\scrv$ are tunable parameters, and $\Tc=0.154$~GeV is the pseudocritical transition temperature of the HotQCD EoS.
Meanwhile, for the specific bulk viscosity $\zeta/s$, we use an unnormalized Cauchy distribution
\begin{equation}
  \label{eq:bulk_viscosity}
  (\zeta/s)(T) = \frac{\bmax}{1 + \left(\dfrac{T - \bloc}{\bwidth}\right)^2},
\end{equation}
described by a tunable maximum $\bmax$, temperature width $\bwidth$, and temperature location parameter $\bloc$.
Figure~\ref{fig:viscosity_dof} shows several of the possible curves parametrized by Eqs.~\eqref{eq:shear_viscosity} and \eqref{eq:bulk_viscosity}, although infinitely more are possible.

The aforementioned hydrodynamic equations are solved numerically using the boost-invariant VISH2+1 viscous hydrodynamics code \cite{Song:2007ux, Shen:2014vra}.
We vary the spatial grid's maximum width $x_\mathrm{max}$ event-by-event to accommodate systems of varying size and determine the optimal spatial step $dx$ and time step $d\tau$ for each set of model parameters to balance trade-offs between numerical accuracy and computation time (see Appendix~\ref{app:adaptive_grid}).
Although these details are somewhat mundane, they are critically important to the present study, since the computation time scales with the number of spacetime cells $n_\text{cell} \sim n_x^2\, n_\tau$, and this quantity grows rapidly when $n_x$ and $n_\tau$ are large, as is typically the case for simulations with nucleon substructure.

\subsection{Particlization and Boltzmann transport}

We evolve the system hydrodynamically down to a pre-specified switching isotherm $\Tsw$ at which point the medium is converted into particles using the Cooper-Frye formula \cite{PhysRevD.10.186}
\begin{equation}
  \label{cooper-frye}
  E \frac{dN_i}{d^3p} = \frac{g_i}{(2\pi)^3} \int_\Sigma f_i(x, p)\, p^\mu\, d^3\sigma_\mu,
\end{equation}
where $i$ is an index over species, $f_i$ is the distribution function of that species, and $d^3\sigma_\mu$ is a volume element of the isothermal hypersurface $\Sigma$ defined by $\Tsw$.
Thermal particles are then sampled in the rest frame of each fluid cell according to a Bose-Einstein or Fermi-Dirac distribution at zero baryon chemical potential
\begin{equation}
  \label{distribution}
  f(m, p) = \frac{1}{\exp(\sqrt{m^2 + p^2}/T) \mp 1},
\end{equation}
where $m$ is the mass of the sampled particle, $p$ is its momentum, and $T$ is the temperature of the fluid cell.

Traditionally, particlization models have sampled resonances using each particle's pole mass in Eq.~\eqref{distribution}.
This approximation, however, is somewhat crude and has been known to underpredict pion production, particularly at low $p_T$ \cite{Sollfrank:1991xm, Huovinen:2016xxq, Vovchenko:2018fmh}.
We thus follow Ref.~\cite{Bernhard:2018hnz}, and sample particles with a \emph{distribution} of masses
\begin{equation}
  f(p) = \int dm\, \mathcal{P}(m)\, f(m, p),
\end{equation}
where $\mathcal{P}(m)$ is modeled by a Breit-Wigner distribution
\begin{equation}
  \mathcal{P}(m) \propto \frac{\Gamma(m)}{(m - m_0)^2 + \Gamma(m)^2/4}.
\end{equation}
Here $m_0$ is the resonance's Breit-Wigner mass, and $\Gamma(m)$ is its mass-dependent width, for which we use the simple form
\begin{equation}
  \Gamma(m) = \Gamma_0 \sqrt{\frac{m - m_\mathrm{min}}{m_0 - m_\mathrm{min}}},
\end{equation}
where $\Gamma_0$ is the usual Breit-Wigner width, and $m_\mathrm{min}$ is a production threshold equal to the total mass of the lightest decay products.
We tabulate the values of $\{\Gamma_0, m_0, m_\mathrm{min}\}$ for all particles and sample the masses of each particle during particlization \cite{PDG:2017}.
The resonances are then passed to a hadronic transport model, described shortly, which simulates subsequent scatterings and decays.

When the viscous terms $\pi^{\mu\nu}$ and $\Pi$ are nonzero in Eq.~\eqref{hydro-eqn}, the distribution function $f$ must be modified to preserve the continuity of $T^{\mu\nu}$ as the system transitions from hydrodynamics to Boltzmann transport.
We perform the appropriate modification using a general method which transforms the momentum vector \emph{inside} the distribution function \cite{Pratt:2010jt}
\begin{align}
  \label{viscous_correction}
  p_i \rightarrow p'_i &= p_i + \sum\limits_j \lambda_{ij}\, p_j,\\
  \lambda_{ij} &= (\lambda_\mathrm{shear})_{ij} + \lambda_\mathrm{bulk}\, \delta_{ij},
\end{align}
where $\lambda_{ij}$ is a linear transformation matrix consisting of a traceless shear part and a bulk part which is proportional to the identity matrix.

We use for the shear viscous correction the form \cite{Pratt:2010jt}
\begin{equation}
  (\lambda_\mathrm{shear})_{ij} = \frac{\tau}{2 \eta} \pi_{ij},
\end{equation}
with a value for $\eta/\tau$ obtained from the noninteracting hadron resonance gas model
\begin{equation}
  \frac{\eta}{\tau} = \frac{1}{15 T} \sum\limits_{sp} g \int \frac{d^3p}{(2\pi)^3}\frac{p^4}{E^2} f_0 (1 \pm f_0),
\end{equation}
where the sum runs over all species in the hadron gas, and $g$ and $f_0$ are the degeneracy factor and equilibrium distribution function of each species respectively.

For the bulk viscous correction, we use a novel procedure developed in Ref.~\cite{Bernhard:2018hnz}.
The total kinetic pressure of the system is
\begin{equation}
  \label{kinetic-pressure}
  P + \Pi = \sum\limits_\mathrm{sp} g \int \frac{d^3p}{(2\pi)^3} \frac{p^2}{3E} f(p).
\end{equation}
For a given bulk pressure, we rescale the momentum $p$ inside the distribution function $f(p) \rightarrow f(p + \lambda_\mathrm{bulk}\, p)$ and adjust the parameter $\lambda_\mathrm{bulk}$ to match the total pressure on the left side of Eq.~\eqref{kinetic-pressure}.
This substitution of course also modifies the energy density
\begin{equation}
  e = \sum\limits_\mathrm{sp} g \int \frac{d^3p}{(2 \pi)^3} E f(p),
\end{equation}
and so a fugacity term $z_\mathrm{bulk}$ is introduced which modifies the yield of all particles by the same overall factor to compensate.
The full transformation is then given by $f(p) \rightarrow z_\mathrm{bulk}\, f(p + \lambda_\mathrm{bulk}\, p)$, where the parameters $\lambda_\mathrm{bulk}$ and $z_\mathrm{bulk}$ are determined numerically for each value of the bulk pressure.

Once the fluid is converted into particles, we simulate its subsequent microscopic dynamics using the Ultra-relativistic Quantum Molecular Dynamics (UrQMD) transport model \cite{Bass:1998ca, Bleicher:1999xi}.
It solves the microscopic Boltzmann equation
\begin{equation}
  \frac{df_i(x, p)}{dt} = \mathcal{C}_i(x, p),
\end{equation}
where $f_i$ is the distribution function for species $i$, and $\mathcal{C}_i$ is its microscopic collision kernel.
The model propagates all produced hadrons along classical trajectories and simulates their scatterings, resonance formations, and decays until the last interactions cease.

One primary advantage of using a microscopic transport model such as UrQMD as an afterburner, is that it realistically simulates the system break-up when the mean-free-path becomes large relative to the system size.
This dilute limit is expected to play a significant role in small collision systems where the produced medium is smaller and shorter lived.

\section{Parameter estimation}
\label{sec:calibration}

The nuclear collision model constructed in Sec.~\ref{sec:model} includes a number of free parameters $\xv$ which describe the initial state, pre-equilibrium dynamics, and hydrodynamic medium.
Given values for the parameters $\xv$, the model may be used to predict a vector of simulated observables $\ym$.
For example, $\ym$ might be a vector consisting of charged-particle yields in different centrality bins.
The physics model thus represents a vector-valued function $\ym = f(\xv)$ which maps the parameter values $\xv$ to the model observables $\ym$.

The goal of this work is to estimate the true model parameters $\xv_\star$ provided some evidence that our model predictions $\ym$ describe experimental measurements $\ye$.
The problem involves three distinct components:
\begin{enumerate}[itemsep=0pt, leftmargin=2\parindent]
  \item
    $H_f$: the hypothesis that the nuclear collision model $f$ formulated in this work provides a realistic description of reality,
  \item
    $H_\xv$: the hypothesis that the model parameters $\xv$ are the true model parameters $\xv_\star$ of $f$, and
  \item
    $E$: the evidence provided by the model $\ym$, the experimental data $\ye$, and their corresponding uncertainties.
\end{enumerate}
As a practical matter, we always assume that hypothesis $H_f$ is correct, meaning there are no glaring flaws in our chosen theoretical framework.
This is a significant assumption; the application of hydrodynamic simulations to small collision systems is speculative, and our conclusions are conditional on the framework making sense.

Subject to this assumption, we can apply Bayes' theorem to evaluate the hypothesis $H_\xv$, \emph{given} the evidence provided by $E$.
Simplifying notation and writing $H_\xv$ as just $\xv$, Bayes' theorem yields
\begin{equation}
  \label{eq:bayes}
  P(\xv | E) \propto P(E | \xv)\, P(\xv).
\end{equation}
The left-side of this expression is the \emph{posterior}: the probability of $\xv = \xv_\star$ given the experimental evidence $E$.
On the right-side there are two separate terms.
The first term $P(E | \xv)$ is the \emph{likelihood} function: the probability of observing the evidence $E$ provided that $\xv = \xv_\star$, and the second term $P(\xv)$ is the \emph{prior}: an estimate of the probability of hypothesis $\xv = \xv_\star$ in the absence of evidence $E$.

We assume that the likelihood function in Eq.~\eqref{eq:bayes} is described by a multivariate normal distribution
\begin{equation}
  \label{eq:likelihood}
  P(E | \xv) = \frac{1}{\sqrt{(2\pi)^m \det \Sigma}} \exp \left ( -\frac{1}{2}\Delta\yv^\intercal \Sigma^{-1} \Delta\yv \right ),
\end{equation}
where $\Delta\yv = \ym(\xv) - \ye$ is a vector of size $m$, equal to the discrepancy of the model and experiment, and $\Sigma = \Sigmam(\xv) + \Sigmae$ is the \emph{total} covariance matrix, equal to the sum of a modeling component $\Sigmam(\xv)$ and an experimental component $\Sigmae$ which account for all known sources of uncertainty in the simulated and measured observables.

\begin{table}[t]
  \caption{
    \label{tab:design}
    Input parameter ranges for the physics model.
  }
  \begin{ruledtabular}
  \begin{tabular}{lll}
    Parameter         & Description                          & Range            \\
    \paddedhline
    Norm              & Normalization factor                 & 9--28 GeV        \\
    $p$               & Energy deposition parameter          & $-1$ to $+1$     \\
    $\sigmaf$         & Nucleon fluctuation std.\ dev.\      & 0--2             \\
    $\rc$             & Constituent position radius          & 0--1.2 fm        \\
    $\nc$             & Number of constituents               & 1--9             \\
    $\wc$             & Width of constituents                & 0.2--1.2 fm      \\
    $\dmin$           & Minimum inter-nucleon dist.\         & 0--1.7 fm        \\
    $\taufs$          & Free streaming time                  & 0.1--1.5 \fmc    \\
    $\smin$           & Minimum value of $\eta/s$ (at $\Tc$) & 0--0.2           \\
    $\sslope$         & Slope of $\eta/s$ above $\Tc$        & 0--8 GeV$^{-1}$  \\
    $\scrv$           & Curvature of $\eta/s$ above $\Tc$    & $-1$ to $+1$     \\
    $\bmax$           & Maximum value of $\zeta/s$           & 0--0.1           \\
    $\bwidth$         & Width of $\zeta/s$ peak              & 0--0.1 GeV       \\
    $\bloc$           & Temp.\ of $\zeta/s$ maximum          & 0.150--0.200 GeV \\
    $\Tsw$            & Switching/particlization temp.       & 0.135--0.165 GeV \\
  \end{tabular}
  \end{ruledtabular}
\end{table}

\subsection{Parameter design and observables}
\label{sec:observables}

\begin{table*}
  \caption{
    \label{tab:observables}
    Experimental data used to calibrate the model.
  }
  \begin{ruledtabular}
  \begin{tabular}{cc}
    Pb-Pb $\sqrts=5.02$~TeV & $p$-Pb $\sqrts=5.02$~TeV \\
    \paddedhline
    Charged-particle multiplicity $d\nch/d\eta$, $|\eta| < 0.5$ \cite{Adam:2015ptt} & Charged-particle multiplicity $d\nch/d\eta$, $|\eta| < 1.4$ \cite{Adam:2014qja} \\
    \noalign{\smallskip}
    Two-particle flow cumulants  $\vnk{n}{2}$ for $n=2,3,4$, $|\eta| < 0.8$,  & Two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3$, $|\eta| < 2.4$, \\
    charged-particles, $|\Delta\eta| > 1$,\, $0.2 < p_T < 5.0$~GeV \cite{Adam:2016izf} & charged-particles, $|\Delta\eta| > 2$,\, $0.3 < p_T < 3.0$~GeV \cite{Chatrchyan:2013nka}\\
    \noalign{\smallskip}
    & Charged-particle mean $p_T$, $0.15 < p_T < 10$~GeV, $|\eta| < 0.3$ \cite{Abelev:2013bla}\\
  \end{tabular}
  \end{ruledtabular}
\end{table*}

For the prior $P(\xv)$, we specify ranges, i.e.\ minimum and maximum values, for each parameter which are listed in Table~\ref{tab:design}.
We assume the prior distribution is constant and nonzero within each specified range and zero otherwise.
The selected parameter ranges are intentionally wide to avoid clipping the calibrated posterior.
For example, a previous analysis of the \trento\ model \cite{Bernhard:2016tnd} found $p \sim 0$, but we use a prior range $p \in [-1, 1]$ to account for differences in the present model, e.g.\ nucleon substructure, which could modify its posterior.
One exception is the constituent number $\nc$ which we limit for practical considerations.
Recall that each constituent fluctuates independently, weighted by a gamma random variable.
Hence for constituent numbers $\nc \gg 1$, the fluctuations average out, and the resulting nucleon fluctuations vanish.
To counteract this effect, the constituent fluctuation variance must increase as $\nc$ increases.
Eventually, these required fluctuations become unreasonably large.
We find that for $\nc < 10$, the energy density fluctuations are reasonable, and hence we limit $\nc$ to this prior range.

The likelihood function \eqref{eq:likelihood} provides evidence for (or against) the model parameters $\xv$ by comparing the model predictions $\ym$ to experimental data $\ye$.
We focus on simple experimental observables in the present study which are sensitive to the bulk properties of the produced medium.
We calculate for each set of model parameters the following observables at midrapidity:
\begin{itemize}[leftmargin=1\parindent, itemsep=0pt]
  \item
    Charged-particle multiplicity $d\nch/d\eta$.
  \item
    Identified particle yields $dN/dy$ of pions, kaons, and protons.
  \item
    Transverse energy production $dE_T/d\eta$.
  \item
    Charged-particle mean transverse momentum $\langle p_T \rangle$ ($0.15 < p_T < 10$~GeV).
  \item
    Identified particle mean transverse momentum $\langle p_T \rangle$ of pions, kaons, and protons.
  \item
    Mean transverse momentum fluctuations $\delta p_T / \langle p_T \rangle$ (charged-particles, $0.15 < p_T < 2.0$~GeV).
  \item
    Two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3,4$\\ (charged-particles, $0.2 < p_T < 5.0$~GeV for ALICE, and $0.3 < p_T < 3.0$~GeV for CMS).
  \item
    Four-particle flow cumulant $\vnk{2}{4}$ \\(charged-particles, $0.2 < p_T < 5.0$~GeV).
  \item
    Symmetric cumulants $\SC(4, 2)$ and $\SC(3,2)$.
\end{itemize}

Each observable is calculated from the list of final state particles produced by UrQMD using the same methods applied by experiment.
We generally match the kinematic cuts of all measurements with two exceptions: we use a larger rapidity interval $|\eta| < 0.8$ than experiment for some boost-invariant observables to improve our finite particle statistics, and we do not apply a rapidity gap, e.g.\ $|\Delta \eta| > 1$, between pairs of particles when calculating the two-particle cumulant $\vnk{n}{2}$ since we already oversample particles from each hydrodynamic event, and this oversampling suppresses non-flow correlations.

At the time of this writing, many of the aforementioned experimental observables are not yet published for $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV.
We therefore restrict our calibration to the subset of measured and published observables listed in Table~\ref{tab:observables}.
Notably absent from this list are the four-particle cumulants $\vnk{n}{4}$ at $\sqrts=5.02$~TeV despite being measured and published.
Unfortunately, the four-particle cumulants require minimum-bias event statistics an order of magnitude larger than those used in this work.
We therefore refrain from \emph{calibrating} on the four-particle cumulants, although we do show calculations of the Pb-Pb four-particle cumulant $\vnk{2}{4}$ later in the text, using a single set of calibration parameters.

\begin{figure*}[t]
  \includegraphics{correlation_matrices}
  \caption{
    \label{fig:correlation}
    Visualization of the Pb-Pb correlation matrix $\corr(y_i, y_j) = \cov(y_i,y_j)/(\sigma_i \sigma_j)$ for the model (emulator) at a random point in parameter space (left-side) and for the experimental data (right-side).
    Each cell represents an observable in a single centrality bin. Experimental statistical and systematic errors are from ALICE \cite{Adam:2015ptt, Adam:2016izf}.
    The experimental correlation structure is modeled using Eq.~\eqref{eq:corr}.
  }
\end{figure*}

Most of the calibration observables listed in Table~\ref{tab:observables} are calculated as a function of collision centrality, where centrality is defined using some measure of the underlying event activity, e.g.\ the charged-particle yield in a given rapidity window.
When calculating these observables, we generate \order{4} minimum-bias events at each design point and divide the events into centrality bins using the charged-particle yield at midrapidity, similar to the procedure used by experiment.

However, for some observables such as $p$-Pb mean $p_T$ \cite{Abelev:2013bla} and flow cumulants $\vnk{n}{k}$ \cite{Chatrchyan:2013nka}, the experiments use a special high-multiplicity trigger to select rare, ultra-central events according to the number of charged-particles produced $\nch$ or detector tracks offline $\ntrk$.
These high-multiplicity bins are too selective for our modest minimum bias event sample, and so a different procedure is required.
We exploit, for this purpose, the approximate monotonic relation between each event's initial energy density
\begin{equation}
  \frac{dE}{d\eta_s} \bigg\vert_{\eta_s=0} = \tau_0 \int d^2x_\perp e(\xv_\perp, \eta_s=0, \tau_0)
\end{equation}
and its final charged-particle density $(d\nch/d\eta) \vert_{\eta=0}$ at midrapidity.

Consider, for example, a single multiplicity bin $[\nch^\text{low}, \nch^\text{high}]$ which selects events from a minimum bias event sample with $p_T^\text{min} < p_T < p_T^\text{max}$ and $|\eta| < \eta^\text{max}$.
Let $\langle\nch\rangle$ denote the average charged-particle multiplicity of these events.
We first rescale the experimental multiplicity bin edges
\begin{equation}
  [\nch^\text{low}, \nch^\text{high}] \rightarrow \left[\frac{\nch^\text{low}}{\langle \nch \rangle},\, \frac{\nch^\text{high}}{\langle \nch \rangle}\right ]
\end{equation}
in order to reexpress each bin edge as a unitless variable.
These bin edges are then associated with a pair of energy bin edges
\begin{equation}
  \label{eq:mult_trigger}
  \Bigg [\frac{E^\text{min}}{\langle E \rangle}, \frac{E^\text{max}}{\langle E \rangle} \Bigg ] \leftrightarrow \Bigg [\frac{\nch^\text{low}}{\langle \nch \rangle}, \frac{\nch^\text{high}}{\langle \nch \rangle} \Bigg ],
\end{equation}
where $E \propto (dE/d\eta_s) \vert_{\eta_s=0}$ is the midrapidity energy of a single event in the desired kinematic range, and $\langle E \rangle$ is the corresponding average energy over the full minimum bias event sample.

Finally, we mimic the method used by experiment and apply Eq.~\eqref{eq:mult_trigger} to select rare high-multiplicity events from a continuous stream of minimum-bias \trento\ events satisfying the correct bin edges.
This of course means that, in addition to running a large sample of minimum-bias events for centrality binned observables, we must also generate (much like experiment) a separate sample of multiplicity triggered events.
In practice, we use a few hundred to a few thousand events per multiplicity bin, depending on the type of observable.

\subsection{Experimental uncertainties}

We also take stock of the statistical and systematic errors reported by each experiment and incorporate their uncertainty into the likelihood covariance matrix
\begin{equation}
  \Sigma = \Sigmam + \Sigmae
\end{equation}
appearing in Eq.~\eqref{eq:likelihood}, which includes uncertainty contributions from both the model $\Sigmam$ and experimental data $\Sigmae$.
The experimental contribution to the covariance $\Sigmae$ is further broken down into statistical and systematic components
\begin{equation}
  \Sigmae = \Sigmae^\text{stat} + \Sigmae^\text{sys}.
\end{equation}
The statistical errors in $\Sigmae^\text{stat}$ are uncorrelated, and thus its covariance matrix is diagonal
\begin{equation}
  \Sigmae^\text{stat} = \diag[(\sigma^\text{stat}_1)^2, (\sigma^\text{stat}_2)^2, \dots (\sigma^\text{stat}_m)^2 ],
\end{equation}
where $\sigma^\text{stat}_i$ is the statistical uncertainty of observable $y_i$ in the observable vector $\ye = (y_1, \dots, y_m)$.
The systematic errors, meanwhile, are typically correlated, but the correlation structure is not reported by the experiments so we assert a reasonable form.
We can expand the systematic covariance as
\begin{equation}
  (\Sigmae^\text{sys})_{ij} =  \rho_{ij} \sigma_i \sigma_j,
\end{equation}
where $\sigma_i$ and $\sigma_j$ are the systematic errors of observables $y_i$ and $y_j$ respectively, and $\rho_{ij}$ is the Pearson correlation coefficient
\begin{equation}
  \rho_{ij} = \frac{\cov(y_i, y_j)}{\sigma_i \sigma_j}
\end{equation}
between observable $y_i$ and $y_j$, which satisfies $\rho_{ij}=1$ for $i=j$ and $|\rho_{ij}| \le 1$ for $i \ne j$.
We assume that each observable is correlated across different centrality/multiplicity bins, and uncorrelated with observables of a different type, e.g.\ correlations between yields and flows.
This is a crude simplifying assumption, but it is better than neglecting the correlation structure of the experimental data entirely.

For the correlation structure between different observable bins, we assert the simple Gaussian form
\begin{equation}
  \label{eq:corr}
  \rho_{ij}^\text{sys} = \exp \left[ -\frac{1}{2} \left(\frac{b_i - b_j}{\ell} \right)^2 \right],
\end{equation}
where $b_i$ and $b_j$ are the midpoints of two observable bins of a single type (centrality or relative multiplicity), and $\ell$ is a correlation length which describes how quickly the observable bins decorrelate as the distance between the bins increases.
We use centrality correlation lengths ${\ell=100}$ for all of the centrality binned Pb-Pb observables and $\ell=30$ for the centrality binned $p$-Pb charged-particle yield $d\nch/d\eta$.
The \mbox{$p$-Pb} mean $p_T$ and flow observables, meanwhile, use relative multiplicity bins $\nch / \langle \nch \rangle$ and $\ntrk / \langle \ntrk \rangle$ which necessitate a smaller correlation length $\ell=5$.
We show an example correlation matrix
\begin{equation}
  \mathrm{corr}(y_i, y_j) = \cov(y_i, y_j)/(\sigma_i \sigma_j)
\end{equation}
for the Pb-Pb experimental data constructed using Eq.~\eqref{eq:corr} on the right-side of Fig.~\ref{fig:correlation}.
Here $y_i$ denotes an element of the experimental data $\ye$ and $\sigma_i$ its corresponding uncertainty.

\subsection{Model emulator}

In principle, one could calculate the likelihood function in Eq.~\eqref{eq:likelihood} directly, e.g.\ by running the model a large number of times at a given parameter point $\xv$ to calculate the model observables $\ym(\xv)$ from the ensemble of simulated events, but in practice such a procedure would be intractable.
The model is computationally intensive to evaluate, and thousands of events are required to calculate the simplest observables at a single parameter point.
Moreover, we need to evaluate Eq.~\eqref{eq:likelihood} numerous times in order to sample the multidimensional posterior distribution so that the samples may be histogrammed and visualized.

We therefore follow an established framework for computationally intensive models and train an emulator to act as a fast surrogate for the full physics simulation \cite{OHagan:2006ba, Higdon:2008cmc, Higdon:2014tva}.
The emulator enables essentially instantaneous predictions for $\ym = f(\xv)$ and allows us to sample the posterior distribution millions of times.
In order to train the emulator, we first generate a scaffolding of the parameter space using a maximin Latin hypercube design \cite{Morris:1995lh} to distribute 500 points throughout our 15-dimensional parameter space according to the parameter ranges in Table~\ref{tab:design}.
We then run minimum-bias and multiplicity triggered $p$-Pb and Pb-Pb events at each design point and calculate the model observables from the ensemble of events.

Specifically, let $X$ denote the ${d \times n}$ design matrix of $d=500$ training points, where each training point is a vector $\xv = (x_1, x_2, \dots, x_n)$ of the $n=15$ model parameters in Table~\ref{tab:design}.
Similarly, let $Y$ denote the corresponding ${d \times m}$ observables matrix, where each row of $Y$ is a vector $\ym = (y_1, y_2, \dots, y_m)$ of $m$ model observables.
The emulator operates strictly on model outputs, so let's temporarily drop the subscript on $\ym$ to declutter the notation.
Our goal is to train an emulator for the physics model $f$ using the discrete observations $f: X \mapsto Y$.

We use for this purpose a specific type of emulator known as a Gaussian process (GP) emulator \cite{Rasmussen:2006gp}.
The advantage of using GPs is that they provide an estimate of their own uncertainty which allows us to account for this uncertainty when constructing the covariance matrix $\Sigma$ in Eq.~\eqref{eq:likelihood}.
One quirk of GPs is that they are restricted to scalar-valued functions, i.e.\ functions of one output, whereas we require an emulator for vector-valued functions with multiple outputs.
This restriction is commonly circumvented using principal component analysis (PCA): a general procedure which transforms a set of correlated variables $\yv = (y_1, y_2, \dots, y_m)$ into a new basis representation $\zv = (z_1, z_2, \dots, z_m)$ where the linear correlations between $z_i$, $z_j$ vanish for all $i\ne j \in m$ \cite{Tipping:1999}.
Independent GPs can then be used to emulate each $z \in \zv$ since the variables $(z_1, \dots, z_m)$ are linearly uncorrelated.
The emulated vector $\zv$ is then easily reexpressed in the basis of $\yv$ through its inverse transformation.

We first preprocess our model observables by centering and scaling each column of $Y$ (single observable) to zero mean and unit variance to generate a standardized observable matrix $\hat{Y}$.
PCA is then used to reexpress each row-vector $\hat{\yv}_i$ of $\hat{Y}$ (all standardized observables at a single design point) in the new principal component basis
\begin{equation}
  \hat{\yv}_i = \sum\limits_{j=1}^m z_{ij} \mathbf{v_j},
\end{equation}
where $\hat{\yv}_i$ are the standardized observables of the $i$-th row-vector (design point) of matrix $\hat{Y}$, and $z_{ij}$ and $\mathbf{v}_j$ are the coefficients and vectors of its $j$-th principal component.

The principal components are reported in order of explained variance, with the first principal component vector $\mathbf{v}_1$ accounting for the most variance in $\hat{Y}$, and the last principal component vector $\mathbf{v}_m$ accounting for the least.
We then train a set of independent GPs $\{z_i=\mathrm{gp}_i(\xv)\}$ to predict the first $k$ principal components $(z_1, \dots, z_k)$ as a function of the model parameters $\xv$ which vary across the design $X$.
For the present study, we use $k=7$ principal components when emulating the $p$-Pb system and $k=8$ principal components when emulating Pb-Pb, chosen to describe 99.5\% of the total observed variance of each system.

The GPs are essentially fancy interpolators applied to the model's training points and PCA transformed observables.
Each GP reports a mean value $z(\xv)$ as well as an estimated error $\delta z(\xv)$ which accounts for statistical noise in the training data and interpolation error between the design points.
Once the GPs are trained, we can predict the observables $\ym$ at parameter point $\xv$ by transforming the vector of principal components
\begin{equation}
  \mathbf{z}(\xv) = [z_1(\xv), z_2(\xv), \dots, z_k(\xv)]
\end{equation}
\emph{back} to physical space.

\begin{fullpage}
  {\centering\includegraphics{observables_pbpb}\par}
  \captionof{figure}{
    \label{fig:obs_pbpb}
    Simulated observables compared to experimental data for Pb-Pb collisions at $\sqrts=5.02$~TeV.
    Top row: explicit model calculations (no emulator) for each of the $d=500$ design points; bottom row: emulator predictions for $n=100$ random samples drawn from the posterior.
    Points with error bars are experimental data from ALICE with statistical and systematic errors added in quadrature \cite{Adam:2015ptt, Adam:2016izf}.
  }
  \vspace{.5cm}
  {\centering\includegraphics{observables_ppb}\par}
  \captionof{figure}{
    \label{fig:obs_ppb}
    Same as Fig.~\ref{fig:obs_pbpb} but for $p$-Pb collisions at $\sqrts=5.02$~TeV.
    Note that multiplicity bins are used for mean $p_T$ and flow cumulant observables to match the bins used by experiment.
    Experimental data are from ALICE \cite{Adam:2014qja, Abelev:2013bla} and CMS \cite{Chatrchyan:2013nka}.
  }
\end{fullpage}

Similarly, we can construct the covariance matrix of the observables in PCA space
\begin{equation}
  \cov(z_i, z_j) = \diag[(\delta z_{1})^2, (\delta z_2)^2, \dots, (\delta z_k)^2 ],
\end{equation}
and transform it back to physical space as well to obtain the covariance matrix $\Sigmam$ of the model observables at parameter point $\xv$.

The resulting emulator therefore predicts both a mean prediction $\ym(\xv)$ and an uncertainty covariance matrix $\Sigmam(\xv)$ which accounts for multiple sources of model and emulator uncertainty, including the truncation error expected from using a finite number of principal components $k < m$.
The model covariance matrix $\Sigmam$ includes so-called \emph{known}-unknowns such as statistical error and emulator interpolation error, but not \emph{unknown}-unknowns such as the overall validity of small-system hydrodynamics, i.e.\ things which lack a unified consensus or are difficult to quantify.
We show in Fig.~\ref{fig:correlation} the resulting Pb-Pb correlation matrix $\mathrm{corr}(y_i,y_j)$ for the model (emulator) at a random parameter point $\xv$ in the design space (left-side), along side the same correlation matrix for the experimental data (right-side) discussed previously.
For additional information on the model emulator, we direct the reader to Appendix~\ref{app:validation} which includes several validation tests of the emulator prediction accuracy.

\subsection{Bayesian calibration}

In order to calibrate the model on two different collision systems, we expand the likelihood function \eqref{eq:likelihood} into a joint likelihood
\begin{equation}
  \label{eq:joint_likelihood}
  P(E | \xv) = P(E_\text{Pb-Pb} | \xv) \cdot P(E_\text{$p$-Pb} | \xv),
\end{equation}
where $E$ subsumes all evidence from the $p$-Pb and \mbox{Pb-Pb} collision systems.
We then perform Markov-chain Monte Carlo (MCMC) importance sampling on the posterior distribution $P(\xv | E)$ in Eq.~\eqref{eq:bayes} to draw random samples of the ``true'' model parameters $\xv_\star$, given the evidence provided by the model predictions and the experimental data \cite{Goodman:2010en, FM:2013mc}.
For this we use an affine-invariant sampler which uses a large ensemble of interdependent walkers \cite{Goodman:2010en, FM:2013mc} and allow the MCMC chain to burn-in before generating \order{7} posterior samples.

\section{Results}
\label{sec:results}

We show the simulated and emulated model observables (thin colored lines) for Pb-Pb collisions in Fig.~\ref{fig:obs_pbpb} and for \mbox{$p$-Pb} collisions in Fig.~\ref{fig:obs_ppb} at $\sqrts=5.02$~TeV compared to experimental data from the CMS \cite{Chatrchyan:2013nka} and ALICE collaborations \cite{Adam:2015ptt, Adam:2016izf, Adam:2014qja, Abelev:2013bla}.
The top row of each figure shows explicit model calculations at each of the $d=500$ design points (training data), while the bottom row shows emulator predictions for $n=100$ random parameter samples drawn from the Bayesian posterior (sampled from the MCMC chain).
Each column shows a different class of observable.
The charged-particle yield $d\nch/d\eta$ is shown on the left, mean $p_T$ is in the middle, and two-particle flow cumulants $\vnk{n}{2}$ for $n=2,3,4$ are on the right.
The Pb-Pb mean $p_T$ and $p$-Pb $\vnk{4}{2}$ datasets are missing and hence are omitted from the present calibration.

Notice the large spread of the observables calculated at the training points (top row of each figure).
The design is constructed to vary each parameter across a wide range of values, specified in Table~\ref{tab:design}, and hence the corresponding model calculations are equally uncertain.
We also point out that there is considerably more variance in the $p$-Pb training data than the Pb-Pb training data.
The $p$-Pb yields, mean $p_T$, and flow cumulants all vary wildly within the chosen parameter ranges.
For instance, we can turn the $p$-Pb flows completely \emph{off} with suitably chosen parameters which is not possible in the Pb-Pb system.
Evidently the $p$-Pb model predictions are far more sensitive to modeling uncertainties.

Conversely, the calibrated (posterior sampled) emulator predictions (bottom row of Figs.~\ref{fig:obs_pbpb} and \ref{fig:obs_ppb}) are far better constrained and nicely track the experimental data points.
We emphasize here that the posterior parameter values are obtained from a \emph{simultaneous} calibration to $p$-Pb and Pb-Pb data, and thus they are self-consistent between the two systems.
The spread in the posterior samples reflects different sources of model and experimental uncertainty as well as tension in the optimal fit parameters which describe each observable.
We demonstrate later in the text that a single set of model parameters well describes all of the calibration data, and thus we believe that much of the spread in the posterior samples is uncertainty contributed by our emulator.

We also note that although the $p$-Pb posterior samples have a somewhat larger spread than the Pb-Pb samples, the percentage uncertainty of the $p$-Pb emulator is similar to that of the Pb-Pb emulator, and thus the difference is likely due to the larger variance of the $p$-Pb training data.
The uncertainty in the posterior distribution could thus be improved by running the calibration with more design points or with a narrower range of parameter values to increase the density of the training points and reduce interpolation uncertainty.

We now direct our attention to Fig.~\ref{fig:posterior} which shows the main result of this work, the posterior distribution of the model input parameters.
Recall that the posterior $P(\xv | E)$ is the probability that our hypothesis $\xv = \xv_\star$ is correct, given the evidence $E$ provided by the predictions of the model and the experimental data.
The present posterior has 15 dimensions, one for each parameter listed in Table~\ref{tab:design}, and thus its joint distribution cannot be summarized by one figure alone.
We therefore sample the distribution and histogram the samples to project the distribution onto one or two dimensions at a time.

\begin{fullpage}
  \centering
  \vspace*{10pt}
  \makebox[\textwidth]{
    \includegraphics[width=.95\paperwidth]{posterior}
  }
  \captionof{figure}{
    \label{fig:posterior}
    Bayesian posterior distribution of the model input parameters.
    The diagonal panels show the marginalized distributions of individual model parameters, while off-diagonal panels show the joint distributions for pairs of model parameters, visualizing their correlations.
    The marginalized distribution medians and 90\% credible intervals are annotated along the diagonal.
  }
\end{fullpage}

Each diagonal panel is the distribution of a \emph{single} model parameter (marginalized over all others), and each lower-diagonal panel is the joint distribution of a \emph{pair} of model parameters (marginalized over all others).
We also report numeric estimates for each parameter's median value and 90\% credible interval and annotate their values along the distribution diagonal (see Table~\ref{tab:post_param}).
For example, the fictitious parameter estimate $q=2.45_{-0.15}^{+0.20}$ reports a median value $\tilde{q}=2.45$ and 90\% credible interval $2.30 < q < 2.65$.

\begin{table}
  \caption{
    \label{tab:post_param}
    Posterior parameter estimates corresponding to Fig.~\ref{fig:posterior}.
    The reported values are for the distribution median and 90\% highest posterior density credible interval.
  }
  \begin{ruledtabular}
    \newlength{\cellwidth}
    \settowidth{\cellwidth}{$-0.00$}
    \newcommand{\est}[3]{\parbox{\cellwidth}{\hfill$#1$}$_{-#2}^{+#3}$}
    \begin{tabular}{llll}
      \toprule
      \multicolumn{2}{c}{Initial condition / Pre-eq}     & \multicolumn{2}{c}{QGP medium}              \\
      \cmidrule(r){1-2}                                    \cmidrule(l){3-4}
      \addlinespace[.4ex]
      Norm       & \est{20.0}{2.5}{2.6} GeV      & $\smin$     & \est{0.08}{0.07}{0.07}            \\[1.1ex]
      $p$        & \est{0.002}{0.181}{0.156}     & $\sslope$   & \est{1.23}{1.23}{1.46} GeV$^{-1}$ \\[1.1ex]
      $\sigmaf$  & \est{0.91}{0.33}{0.32}        & $\scrv$     & \est{-0.08}{0.92}{0.79}           \\[1.1ex]
      $\rc$      & \est{0.88}{0.23}{0.26} fm     & $\bmax$     & \est{0.026}{0.026}{0.032}         \\[1.1ex]
      $\nc$      & \est{6.0}{3.4}{3.0}           & $\bwidth$   & \est{0.036}{0.036}{0.043} GeV     \\[1.1ex]
      $\wc$      & \est{0.52}{0.20}{0.28} fm     & $\bloc$     & \est{0.174}{0.024}{0.020} GeV     \\[1.1ex]
      $\dmin$    & \est{1.12}{0.49}{0.58} fm     & $\Tsw$      & \est{0.149}{0.014}{0.013} GeV     \\[1.1ex]
      $\taufs$   & \est{0.47}{0.37}{0.55} \fmc   & \\
      \addlinespace[.4ex]
      \bottomrule
    \end{tabular}
  \end{ruledtabular}
\end{table}

\subsection{Initial condition properties}

The \trento\ normalization factor ${\text{Norm} = 20.0^{+2.6}_{-2.5}}$ and energy deposition parameter $p=0.002^{+0.156}_{-0.181}$ are well constrained by the present analysis.
Moreover, Figs.~\ref{fig:obs_pbpb} and \ref{fig:obs_ppb} show that the model predictions using these values nicely describe both the $p$-Pb and Pb-Pb calibration observables.
While it would not be surprising to fit one or two of these observables using a narrow range of parameter values, the quality of the combined fit (more on this later) and the number of observables described is highly non-trivial.
For example, consider the ratio of the $p$-Pb charged-particle yield to the Pb-Pb charged-particle yield.
As the energy deposition parameter $p$ trends toward positive (negative) infinity, particle production scales like the maximum (minimum) of the two nuclear thickness functions.
This has a much stronger effect on the highly asymmetric $p$-Pb system than it does on the Pb-Pb system; hence the parameter $p$ strongly affects the ratio of the two average yields.

It is therefore compelling that $p \sim 0$ correctly describes the charged-particle yield $d\nch/d\eta$ of both systems, while simultaneously describing the centrality dependence of $\vnk{n}{k}$, an observable which is also known to strongly depend on $p$ \cite{Bernhard:2018hnz}.
Specifically, this value $p \sim 0$ corresponds to an energy deposition mapping proportional to the geometric mean of participant nuclear thickness
\begin{equation}
  \label{eq:geometric_mean}
  e(\xv_\perp, \eta_s=0, \tau_0) \propto \sqrt{\T_A\, \T_B}.
\end{equation}
We caution, however, that this specific analytic form should not be interpreted too literally.
For instance, a generalized mean described by $p=0.05$ is well within our 90\% credible interval, but it does not equal the geometric mean of Eq.~\eqref{eq:geometric_mean}.
We also note that this scaling is somewhat different than the scaling obtained from previous analyses of the \trento\ model, which parametrized the \emph{entropy} density using a framework which assumed instant thermalization and zero pre-equilibrium flow.
Evidently, both prescriptions prefer geometric mean scaling, but each prescription leads to a somewhat different interpretation of the initially produced quantity.

\begin{figure}
  \includegraphics{posterior_proton_shape}
  \caption{
    \label{fig:posterior_proton_shape}
    Posterior distribution (blue histogram) for the constituent position sampling radius $\rc$ and constituent width $\wc$.
    The prior for $\rc$ and $\wc$ spans the full plot range.
  }
\end{figure}

Continuing down the diagonal in Fig.~\ref{fig:posterior}, we see that the constituent position sampling radius $\rc=0.88_{-0.23}^{+0.26}$~fm, and the constituent width $\wc=0.52_{-0.20}^{+0.28}$~fm.
Figure~\ref{fig:posterior_proton_shape} shows the joint posterior distribution of both parameters, illustrating the constraining power of the Bayesian analysis.
While the sampling radius $\rc$ varies the size of the nucleons, we caution that its specific meaning should be interpreted with care; it specifies a computational \emph{sampling} radius, not a physical nucleon width.
Consider, for instance, a single nucleon with $\nc = 2$ constituents.
If the two constituents positions land on the same side of the nucleon, the effective nucleon size will be smaller than the Gaussian sampling radius $\rc$.
Despite this idiosyncrasy, one can easily define a physical nucleon width in the nucleon center-of-mass frame \emph{ex post facto}, given specific values for the sampling radius $\rc$, constituent width $\wc$, and constituent number $\nc$.

For example, using the posterior distribution's median values, $\rc=0.88$~fm, $\nc=6$, and $\wc=0.52$~fm, we can generate a large ensemble of random nucleon configurations and average their density in each nucleon's center-of-mass frame.
The resulting ensemble-averaged nucleon density
\begin{equation}
  \langle \rho_n(\xv)\rangle = \frac{1}{(2 \pi w^2)^{3/2}} \exp \left( -\frac{|\xv|^2}{2 w^2} \right)
\end{equation}
is described by a single Gaussian of width $w = 0.96$~fm.
This nucleon width is consistent with a previous estimate $w = 0.96_{-0.05}^{+0.04}$~fm obtained by a similar Bayesian analysis of Pb-Pb collisions at $\sqrts=2.76$ and 5.02~TeV using a physics model without nucleon substructure \cite{Bernhard:2018hnz}.

This is perhaps the single largest difference between our work and the conclusions of recent saturation-based calculations which constrained the event-by-event fluctuations of the proton using a color dipole picture of vector meson production \cite{Mantysaari:2016ykx, Mantysaari:2016jaz}.
Those studies find that the measured coherent and incoherent $J/\Psi$ spectra at HERA prefer a compact gluon distribution inside each nucleon, with a Gaussian width $w_g \approx 0.4$~fm which is roughly \emph{half} the Gaussian width preferred by our analysis.
Evidently, it may be necessary to place an informative prior on our nucleon substructure parameters in order to resolve the apparent tension between our parameter values and those needed to describe DIS measurements at HERA.

\begin{figure}[b]
  \includegraphics{inelasticity_profile}
  \caption{
    \label{fig:inelasticity_profile}
    Estimates of the normalized proton-proton inelasticity density $G_\mathrm{inel}(b) = d^2\sigma_\mathrm{inel}/d^2b$ at two different LHC beam energies.
    Blue line: Inelasticity density obtained by fitting a parametric form to LHC data at $\sqrts=7$~TeV \cite{Albacete:2016pmp}.
    Orange line: \trento\ predictions at $\sqrts=5.02$~TeV using median parameters from the present study's Bayesian posterior.
  }
\end{figure}

Additional constraints on the proton size and shape are provided by the proton-proton inelasticity density $G_\mathrm{inel}(b) = d^2\sigma_\mathrm{inel}(b)/d^2b$, which measures the proton-proton inelastic collision probability as a function of its impact parameter.
We compare in Fig~\ref{fig:inelasticity_profile} the normalized proton-proton inelasticity density $G_\mathrm{inel}(b) / G_\mathrm{inel}(0)$ at $\sqrts=5.02$~TeV predicted by \trento\ using the present study's posterior median parameters with a calculation that extracted the proton-proton inelasticity density using a parametrization fit to proton-proton differential scattering data at $\sqrts=7$~TeV \cite{Albacete:2016pmp}.
Our Bayesian median estimate (orange line) prefers a larger width for the proton-proton normalized inelasticity density compared to that of reference \cite{Albacete:2016pmp}, and this trend is \emph{opposite} what one would expect based on the difference in beam energy alone.
This suggests that our posterior estimate for the proton radius is somewhat oversized.
Nevertheless, it is fascinating that the present Bayesian estimate is as close as it is, given that the model is fit to quantities which are not typically used to extract the proton-proton inelasticity density.

\begin{figure}
  \includegraphics{posterior_parton_number}
  \caption{
    \label{fig:posterior_parton_number}
    Posterior distribution for the number of nucleon constituents $\nc$ determined by the analysis.
    The parameter $\nc$ is an integer (discrete) variable at every design point, but the emulator interpolation produces a posterior distribution which is continuous.
  }
\end{figure}

Moving on, we direct our attention to the constituent number $\nc$, shown enlarged in Fig.~\ref{fig:posterior_parton_number}.
The distribution is not sharply peaked, and hence we refrain from quoting a distribution median and 90\% credible interval.
Note, however, that the posterior clearly favors $\nc > 1$ constituents.
This is not surprising.
The \trento\ model mimics saturation-based initial condition models \cite{Bernhard:2016tnd}, and saturation models tend to produce proton-sized fireballs in $p$-Pb collisions \cite{Bzdak:2013zma}.
When the proton is spherically symmetric, the resulting QGP is also largely symmetric and thus produces very little anisotropic flow.
Saturation-based models are therefore unable to describe the significant anisotropic flow measured in central $p$-Pb collisions without nucleon substructure, or alternatively, some other source of additional correlations \cite{Schenke:2017bog}.

\subsection{Transport properties}

\begin{figure*}
  \includegraphics{region_shear_bulk}
  \caption{
    \label{fig:region_shear_bulk}
    Left figure: estimated temperature dependence of the QGP specific shear viscosity $(\eta/s)(T)$ determined by the present Bayesian analysis of $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV (orange line/band) compared to a previous Bayesian analysis of Pb-Pb collisions at $\sqrts=2.76$ and 5.02~TeV (blue line/band) \cite{Bernhard:2018hnz}.
    The lines are the medians of each posterior distribution, and the bands are their 90\% credible regions.
    Right figure: same as before, but for the temperature dependence of the QGP specific bulk viscosity $(\zeta/s)(T)$.
  }
\end{figure*}

In this section, we compare several of our posterior estimates to those obtained from a similar Bayesian analysis in Ref.~\cite{Bernhard:2018hnz} which used an (almost) identical version of the present physics model.
The only modeling difference is the inclusion of nucleon substructure in the present study which was absent in Ref.~\cite{Bernhard:2018hnz}.
Several calibration details, however, are different between the two analyses.
This work used a modest number of $p$-Pb and Pb-Pb observables at $\sqrts=5.02$~TeV (limited by availability), whereas Ref.~\cite{Bernhard:2018hnz} calibrated on a much larger number of Pb-Pb observables at $\sqrts=2.76$ and 5.02~TeV.

The posterior free streaming time $\taufs=0.47_{-0.37}^{+0.55}~\fmc$ obtained in this work is significantly smaller than the previous estimate $\taufs=1.16_{-0.25}^{+0.29}~\fmc$ quoted in Ref.~\cite{Bernhard:2018hnz}.
We point out that the present study is missing several important observables which could affect the estimated free streaming time, e.g.\ the Pb-Pb mean $p_T$ and mean $p_T$ fluctuations at $\sqrts=5.02$~TeV.
Nevertheless, it appears that the inclusion of nucleon substructure significantly reduces the maximum allowed free streaming time, although more work is needed to establish if this is indeed the case.

We also compare in Fig.~\ref{fig:region_shear_bulk} our estimates for the temperature dependence of the QGP specific shear viscosity $(\eta/s)(T)$ and bulk viscosity $(\zeta/s)(T)$ with those of Ref.~\cite{Bernhard:2018hnz}.
The lines are the distribution medians, and the bands are their 90\% credible regions.
The results of this work are shown in orange, and the results of Ref.~\cite{Bernhard:2018hnz} are shown in blue.
In general, our estimates are broader and less certain but otherwise self-consistent.
Evidently, the combined analysis of Pb-Pb data at $\sqrts=2.76$ and 5.02~TeV in Ref.~\cite{Bernhard:2018hnz} provides a better constraint on the QGP viscosities which is not surprising given the additional observables and multiple beam energies studied.
The $p$-Pb data, meanwhile, does not appear to provide strong viscous constraints.

\subsection{Verification of high-probability parameters}

We verified the emulator and tested the accuracy of our physics model framework using a single set of high-probability parameters selected from the Bayesian posterior.
These parameters, listed in Table~\ref{tab:mode_params}, are the approximate ``best fit'' values of the calibrated model, commonly referred to as the \emph{maximum a posteriori} (MAP) estimate
\begin{equation}
  \xv_\mathrm{MAP} \equiv \operatorname*{arg\, max}_{\xv} P(\xv | E).
\end{equation}
We then ran \order{6} minimum-bias and multiplicity triggered events using the MAP estimate $\xv_\mathrm{MAP}$ and computed all of the model observables listed in Sec.~\ref{sec:observables}.
The resulting model calculations are shown in Fig.~\ref{fig:obs_map} alongside experimental data from CMS \cite{Chatrchyan:2013nka} and ALICE \cite{Adam:2015ptt, Adam:2016izf, Adam:2014qja, Abelev:2013bla}.
The left and right columns show the results for the $p$-Pb and Pb-Pb collision systems respectively, and each row shows a different group of related observables.

The global agreement of the MAP model calculations with the experimental data is generally quite good.
The largest tension is observed in the two-particle cumulants $\vnk{2}{2}$ and $\vnk{3}{2}$ of the $p$-Pb system, although even that tension is only about 10--15\%.
Quite remarkably, the model perfectly describes the shape of the $p$-Pb and Pb-Pb two-particle correlations which is strong evidence that these correlations are hydrodynamic in origin.

\begin{table}
  \caption{
    \label{tab:mode_params}
    \emph{Maximum a posteriori} (MAP) parameters determined from the posterior distribution and used to generate Fig.~\ref{fig:obs_map}.
    The posterior distribution on the particlization temperature $\Tsw$ is flat (agnostic), so we fix it's value using Ref.~\cite{Bernhard:2018hnz}.
  }
  \begin{ruledtabular}
    \begin{tabular}{ll@{\hspace{2em}}ll}
      \multicolumn{2}{c}{Initial condition / Pre-eq} & \multicolumn{2}{c}{QGP medium} \\
      \paddedhline
      Norm     & 20. GeV        & $\smin$      & 0.11           \\
      $p$      & 0.0            & $\sslope$    & 1.6 GeV$^{-1}$ \\
      $k$      & 0.19           & $\scrv$      & -0.29          \\
      $\nc$    & 6              & $\bmax$      & 0.032          \\
      $\rc$    & 0.81 fm        & $\bwidth$    & 0.024 GeV      \\
      $\wc$    & 0.43 fm        & $\bloc$      & 0.175 GeV      \\
      $\dmin$  & 0.81 fm        & $\Tsw$       & 0.151 GeV      \\
      $\taufs$ & 0.37 \fmc
    \end{tabular}
  \end{ruledtabular}
\end{table}

Moreover, we obtain an excellent description of the \mbox{$p$-Pb} mean $p_T$, although this fit is somewhat less meaningful since we are unable to calibrate on the Pb-Pb mean $p_T$ simultaneously (data is not yet available).
Additionally, the model provides a simultaneous description of the \mbox{$p$-Pb} and Pb-Pb charged-particle yields using a single energy deposition parameter $p=0$.
This is the \emph{exact same} generalized mean $p$-value supported by multiple previous studies \cite{Moreland:2014oya, Bernhard:2016tnd, Ke:2016jrd, Bernhard:2018hnz}.
Evidently, this scaling continues to hold for initial conditions with sizable nucleon substructure.

We also present calculations for several observables which were omitted from the calibration due to missing experimental data and the statistical limitations of our training data.
Here our MAP event sample is several orders of magnitude larger so the statistics are no issue.
The bottom-right panel of Fig.~\ref{fig:obs_map} shows our model calculation for the Pb-Pb four-particle elliptic flow cumulant $\vnk{2}{4}$ along with the measured data points from \mbox{ALICE} \cite{Adam:2016izf}.
We see that the MAP estimate nicely describes the measured $\vnk{2}{4}$ data which is encouraging since this particular observable was never used to calibrate the model.

The relative mean $p_T$ fluctuation $\delta p_T / \langle p_T \rangle$ is another important bulk observable to test the predictions of the calibrated model.
It measures the \emph{dynamical} component of event-by-event mean $p_T$ fluctuations, quantified by the two-particle correlator
\begin{equation}
  \label{eq:mean_pT_corr}
  (\delta p_T)^2 = \langle \langle (p_{T,i} - \langle p_T \rangle) (p_{T,j} - \langle p_T \rangle) \rangle \rangle.
\end{equation}
The inner-average in Eq.~\eqref{eq:mean_pT_corr} runs over all pairs of particles $i,j$ in the same event, the outer average runs over all events in a given bin (centrality or multiplicity), and the symbol $\langle p_T \rangle$ denotes the usual mean transverse momentum of particles in the bin.
The observable is typically presented in terms of the dimensionless ratio $\delta p_T / \langle p_T \rangle$, which quantifies the strength of the dynamical fluctuations in units of the average transverse momentum $\langle p_T \rangle$.

We show the MAP estimate predictions for the $p$-Pb and Pb-Pb relative mean $p_T$ fluctuations $\delta p_T / \langle p_T \rangle$ at $\sqrts=5.02$~TeV in the third row of Fig.~\ref{fig:obs_map}.
For the Pb-Pb system, we use centrality bins and for the $p$-Pb system we use the same relative multiplicity bins used for the $p$-Pb charged-particle mean $p_T$.
The relative mean $p_T$ fluctuations have been shown to be particularly sensitive to the existence of nucleon substructure \cite{Bozek:2017elk}, and thus it would be interesting to ultimately include this observable in the calibration when the data becomes available.

Lastly, we compute the symmetric cumulants $\SC(m,n)$ for the Pb-Pb collision system at $\sqrts=5.02$~TeV which quantify correlations between event-by-event fluctuations of the flow harmonics of different order \cite{Bilandzic:2013kga, ALICE:2016kpq}
\begin{align}
  \SC(m, n) &= \langle\langle \cos[m(\phi_1 - \phi_3) + n(\phi_2-\phi_4)]\rangle\rangle \nonumber \\
  \nonumber &- \langle\langle\cos[m(\phi_1-\phi_2)]\rangle\rangle\langle\langle\cos[n(\phi_1-\phi_2)]\rangle\rangle \label{eq:scmn}\\
  &\approx \langle v_m^2 v_n^2 \rangle - \langle v_m^2\rangle\langle v_n^2\rangle.
\end{align}
We show these model predictions in Fig.~\ref{fig:flow_corr} along with the \emph{normalized} symmetric cumulants
\begin{equation}
  \NSC(m,n) = \SC(m,n)/\langle v_m^2\rangle\langle v_n^2\rangle,
\end{equation}
which are expected to be less sensitive to the medium response and more sensitive to the properties of the initial state.
The solid lines are the MAP estimate of the present study, and the dashed lines are the MAP estimate of Ref.~\cite{Bernhard:2018hnz} which did not include nucleon substructure and was calibrated on Pb-Pb collisions at $\sqrts=2.76$ and 5.02~TeV.
We observe that the gap between $\SC(3,2)$ and $\SC(4,2)$ is generally wider in the present analysis than in Ref.~\cite{Bernhard:2018hnz}, as is the gap between the normalized symmetric cumulants $\NSC(3,2)$ and $\NSC(4,2)$.

We emphasize that multiple aspects of the two analyses are different such as the collision systems and beam energies considered, the observables which were included in each calibration, and the existence of nucleon substructure in the model.
Thus we can only speculate what might have caused the large difference in the MAP estimate for the symmetric flow cumulants.
Two reasonable culprits would be the inclusion of nucleon substructure and the large difference in the preferred pre-equilibrium free streaming time determined by the two studies.

\section{Summary and conclusions}
\label{sec:summary}

Relativistic heavy-ion collisions produce long-range multiparticle correlations which are commonly explained by the existence of hydrodynamic flow \cite{deSouza:2015ena}.
This narrative is evidenced by the global, self-consistent, and highly non-trivial quantitative agreement of hydrodynamic models with a large number of heavy-ion bulk observables \cite{Niemi:2015qia, Bernhard:2016tnd, Gale:2012rq}.
Naturally, such descriptions rely on the validity of hydrodynamic approximations, and these approximations begin to break down in the so-called dilute limit where discrete particle degrees-of-freedom dominate and continuous field descriptions of the medium cease to make sense.
Tell-tale signatures of hydrodynamic collectivity were thus generally expected to vanish in smaller nuclear collision systems, e.g.\ $p$-$p$ and $p$-Pb collisions, where the number of produced particles is orders of magnitude smaller than a typical Pb-Pb collision.

\begin{fullpage}
  {\centering\includegraphics{observables_map}\par}
  \captionof{figure}{
    \label{fig:obs_map}
    Model calculations using the \emph{maximum a posterior} (MAP) parameters compared to experiment.
    Colored lines are model calculations for $p$-Pb collisions (left) and Pb-Pb collisions (right) at $\sqrts=5.02$~TeV.
    Points with error bars are the experimental data with statistical uncertainties, and gray bands their corresponding systematic uncertainties, from CMS \cite{Chatrchyan:2013nka} and ALICE \cite{Adam:2015ptt, Adam:2016izf, Adam:2014qja, Abelev:2013bla}.
    The sub-axes show the ratio of model over data where available with gray bands indicating $\pm 10\%$.
  }
\end{fullpage}

\begin{figure}
  \includegraphics{flow_corr}
  \caption{
    \label{fig:flow_corr}
    Model calculations of the symmetric cumulants (top) and normalized symmetric cumulants (bottom) for Pb-Pb collisions at $\sqrts=5.02$~TeV using the \emph{maximum a posteriori} (MAP) parameters.
    The solid lines are the MAP estimate of the present analysis (with nucleon substructure), and the dashed lines are the MAP estimate of Ref.~\cite{Bernhard:2018hnz} (without nucleon substructure) which was calibrated on Pb-Pb observables at $\sqrts=2.76$ and 5.02~TeV.
    In general, most model parameters are somewhat different between the two studies.
  }
\end{figure}

These expectations were upended, however, when long-range multiparticle correlations were detected in high-multiplicity $p$-Pb collisions and found to be similar in magnitude to those observed in Pb-Pb collisions \cite{CMS:2012qk, Abelev:2012ola, Aad:2012gla}.
Nuclear collision systems which were previously thought to be too small for hydrodynamic flow, were subsequently found to generate the same collectivity used to justify hydrodynamic flow in heavy-ion collisions.
It is thus natural to wonder if a single unified hydrodynamic framework might be able to describe $p$-Pb and Pb-Pb bulk observables simultaneously.

In this work, we performed a semi-exhaustive search for a unified description of $p$-Pb and Pb-Pb collisions at $\sqrts=5.02$~TeV using Bayesian methods to rigorously calibrate and constrain free parameters of a flexible nuclear collision model based on viscous hydrodynamics.
The goal of our study was two fold.
First, we aimed to establish whether or not our hydrodynamic framework was able to describe both collision systems simultaneously.
And second, in the event that the former was true, we wished to obtain estimates for the \emph{true} parameters of our model given the assumptions of our framework and the evidence provided by the model predictions and the experimental data.

We built, for this purpose, a flexible multi-stage nuclear collision model characterized by a number of free parameters which vary theoretically uncertain aspects of the framework such as the QGP initial conditions and hydrodynamic transport properties.
For the QGP initial conditions, we employed a modified version of the \trento\ model \cite{Moreland:2014oya} which adds new parameters to vary the fluctuating size and shape of each nucleon.
Specifically, we modeled each nucleon as a cluster of $\nc$ constituents (hot spots), where each constituent is described by a Gaussian density of width $\wc$.
The constituent positions were each sampled randomly (without correlations) from a Gaussian radial distribution of width $\rc$ centered about each pre-defined nucleon position.

The transport dynamics of the collision were simulated using a pre-equilibrium free streaming stage followed by boost-invariant viscous hydrodynamics for hot and dense regions of the fireball and a microscopic hadronic afterburner for the relatively dilute corona.
We parametrized various sources of uncertainty in each stage of the collision including the duration of the pre-equilibrium free streaming stage, the temperature dependence of the QGP shear and bulk viscosities, and the particlization temperature used to switch from a hydrodynamic description to microscopic Boltzmann transport.

With the full evolution model in hand, we applied Bayesian methods which were developed to estimate the parameters of computationally intensive models \cite{OHagan:2006ba, Higdon:2008cmc, Higdon:2014tva}.
We first constructed a scaffolding of $n=500$ parameters points distributed throughout our 15-dimensional parameter space and evaluated the nuclear collision model using \order{4} events at each parameter point.
The ensemble of events was then used to calculate a large number of experimental observables at each design point and train Gaussian process emulators to interpolate the model predictions as a function of the input parameters.
Finally, we used Markov chain Monte Carlo (MCMC) importance sampling to explore the parameter space and draw samples from the Bayesian posterior distribution for the \emph{true} values of the model parameters, given our emulated model predictions, the experimental data, and their associated uncertainties.
The model calibration process is summarized by Figs.~\ref{fig:obs_pbpb} and \ref{fig:obs_ppb}, and the resulting posterior distribution for the model input parameters is shown in Fig.~\ref{fig:posterior}.
We also evaluated the model predictions using a single set of high-probability parameters in Fig.~\ref{fig:obs_map}.
With these results, we are able to address the two primary goals of the study.

First, we demonstrated in Fig.~\ref{fig:obs_map} the existence of a single set of model parameters which can simultaneously describe $p$-Pb and Pb-Pb charged-particle yields, mean $p_T$, and flow cumulants at $\sqrts=5.02$~TeV.
The excellent quantitative agreement of the model calculation with the experimental data is strong evidence for a unified hydrodynamic description of $p$-Pb and Pb-Pb collisions at ultrarelativistic energies.
Moreover, the modifications to the physics model which were required to obtain this agreement are generally modest; one must simply replace Gaussian nucleons with composite nucleons of several or more constituents.

Second, we obtained a posterior distribution for the model input parameters in Fig.~\ref{fig:posterior} and reported quantitative estimates for these parameters in Table~\ref{tab:post_param}.
Here we summarize our key findings about the model:
\begin{enumerate}[itemsep=0pt, leftmargin=2\parindent]
  \item
    Using \trento\ initial conditions followed by a pre-equilibrium free streaming stage, we find that the initially produced energy density scales like the geometric mean of participant nuclear thickness; see Eq.~\eqref{eq:geometric_mean}.
    Previous Bayesian studies have reported similar scaling for the initially produced \emph{entropy} density \cite{Bernhard:2016tnd, Ke:2016jrd}, although these studies assumed static initialization, an approximation which neglects the initial values of $u^\mu$, $\pi^{\mu\nu}$ and $\Pi$ at the hydrodynamic starting time.
  \item
    We find that nucleon substructure is necessary to simultaneously describe $p$-Pb and Pb-Pb bulk observables.
    However, we observe no strong preference for a specific number of constituents $\nc$ (hot spots) inside the nucleon.
    In particular, we find no evidence to support the specific number $\nc=3$ which is commonly used in the literature.
  \item
    The present Bayesian analysis prefers larger nucleons, $w \sim 1$~fm, in agreement with a similar Bayesian analysis calibrated to Pb-Pb data at $\sqrts=2.76$ and 5.02~TeV using a hybrid model without nucleon substructure \cite{Bernhard:2018hnz}.
    We note that our result is in significant tension with an estimate for the effective nucleon width based on the gluon distribution extracted from HERA data \cite{Rezaeian:2012ji}.
    Our model calculations also predict a broader proton-proton inelasticity density at $\sqrts=5.02$~TeV than supported by the data \cite{Albacete:2016pmp}.
    This suggests that our reconstructed protons are somewhat oversized.
  \item
    We obtain an estimate $\wc=0.52_{-0.20}^{+0.28}$~fm for the Gaussian width of the constituent hot spots inside each nucleon.
    This width is considerably larger than the length scales typically associated with nucleon substructure.
    However, it is natural to wonder if this estimate is oversized, given the aforementioned comments regarding our extracted nucleon width.
  \item
    We report an estimate $\taufs=0.47_{-0.37}^{+0.55}~\fmc$ for the model's pre-equilibrium free streaming time.
    This is significantly shorter than the estimate obtained from a similar Bayesian study in Ref.~\cite{Bernhard:2018hnz}, which found $\taufs=1.16_{-0.25}^{+0.29}~\fmc$.
    It is not clear whether the difference is a result of nucleon substructure or the different observables used to calibrate each analysis.
  \item
    We compare in Fig.~\ref{fig:region_shear_bulk} our estimate for the temperature dependence of the QGP specific shear and bulk viscosities to those of Ref.~\cite{Bernhard:2018hnz}, which performed a Bayesian calibration to Pb-Pb bulk observables at $\sqrts=2.76$ and 5.02~TeV using a physics model without nucleon substructure.
    The two studies are in good agreement, although Ref.~\cite{Bernhard:2018hnz} obtains a more precise estimate for $(\eta/s)(T)$, likely due to the additional beam energies and observables included and an enhanced sensitivity of larger collision systems to the QGP viscosity.
  \item
    We make predictions in Figs.~\ref{fig:obs_map} and \ref{fig:flow_corr} for several quantities which were not included in the model calibration, including the identified yields, transverse energy, symmetric cumulants, and mean $p_T$ fluctuations at $\sqrts=5.02$~TeV.
    Interestingly, our MAP estimate for the Pb-Pb symmetric cumulants at $\sqrts=5.02$~TeV are significantly different than those estimated in Ref.~\cite{Bernhard:2018hnz}.
    This could be a direct (or indirect) result of including nucleon substructure in the model calibration.
\end{enumerate}

The present study would benefit from a number of improvements.
Perhaps the most obvious target for improvement is the absence of several important experimental datasets.
Specifically, we are missing the transverse energy, identified particle yields, and the mean $p_T$ fluctuations of both collision systems, as well the charged-particle mean $p_T$ for the Pb-Pb system.
These observables would certainly influence the quality of the combined fit and correspondingly our estimates for the model parameters.

Similarly, the results would greatly benefit from additional beam energies and collision systems.
For example, future studies should incorporate proton-proton collisions at top LHC energies, as well as the numerous collisions systems studied at top RHIC energy.
Such data would undoubtedly improve constraints on the model parameters and would enable more stringent tests of the calibrated model predictions.
The RHIC data may also help elucidate the beam-energy dependence of the model parameters which would be worth investigating.
We leave these improvements for future studies.

\medskip

All software used in this work is open source:
\begin{itemize}[leftmargin=2\parindent, itemsep=0pt, topsep=5pt]
  \item \trento\ with nucleon substructure (C++) \cite{trento:code}
  \item Pre-equilibrium free streaming (Python) \cite{freestream:code}
  \item VISH2+1 hydrodynamics (Fortran) \cite{osuhydro:code}
  \item FRZOUT particle sampler (Python) \cite{frzout:code}
  \item UrQMD microscopic transport model (Fortran) \cite{urqmd:code}
  \item DukeQCD event generator wrapper (Python) \cite{eventgen:code}
  \item Bayesian parameter estimation (Python) \cite{bayesian:code}
\end{itemize}

\medskip

\begin{acknowledgments}
  JSM thanks Ulrich Heinz, Jamie Nagle, Berndt M\"uller, Weiyao Ke, Bj\"orn Schenke, and Heikki M\"antysaari for helpful discussions and clarifying comments.
  This research was completed using 6 million CPU hours provided by the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility operated under Contract No. DE-AC02-05CH11231.
  JSM and SAB are supported by the U.S.\ Department of Energy Grant No.\ DE-FG02-05ER41367 and JEB by NSF Grant No.\ NSF-ACI-1550225.
\end{acknowledgments}

\bibliography{references}

\appendix

\section{Event-by-event grid resizing}
\label{app:adaptive_grid}

\begin{figure}[b]
  \begin{tikzpicture}[scale=.9]
    \draw [black, thin, step=0.3cm] (0.9, 0.9) grid +(4.5, 4.5);
    \draw [gray, thin, step=0.9cm] (0, 0) grid +(6.3, 6.3);
    \draw [theblue] (3.15, 3.15) circle (2.25cm);
  \end{tikzpicture}
  \caption{
    \label{fig:adaptive_grid}
    Diagram of the adaptive grid resizing algorithm (not drawn to scale).
    Each initial condition event is first run on a very large coarse-grained mesh (large gray grid) of one-third the spatial resolution otherwise required to measure hydrodynamic observables.
    We then measure the maximum transverse radius $R_\text{max}$ (blue circle) of the hypersurface defined by the temperature isotherm $T = T(e_\text{min})$, where $e_\text{min}$ is the largest energy density which can be truncated without modifying the hydrodynamic observables calculated from the event.
    Finally, the initial condition event is rerun on a smaller and finer mesh (smaller black grid) with three-times the cell density of the pre-run event and a smaller transverse extent $-R_\text{max} < x < R_\text{max}$.
  }
\end{figure}

\begin{figure}
  \includegraphics{validation_example_vert}
  \caption{
    \label{fig:validation_example}
    Example emulator validation for one observable, the Pb-Pb charged-particle yield $d\nch/d\eta$ in the 20-30\% centrality class.
    We use the k-fold cross validation method (explained in the text) to partition the model inputs $X$ and outputs $Y$ into training and validation data.
    The scatter plot on the left shows the emulator predictions and one sigma error bars (x-axis) against explicit model calculations (y-axis).
    Perfect emulator/model agreement is indicated by the black like $y_\text{pred}=y_\text{obs}$.
    The histogram on the right shows that the errors are properly accounted for, i.e.\ the normalized residuals follow a normal distribution with unit variance and zero mean.
  }
\end{figure}

The boost-invariant VISH2+1 hydrodynamics code used in this work \cite{Song:2007ux, Shen:2014vra} runs on a Cartesian transverse grid specified by a maximum grid size $x_\text{max}$ and grid step width $dx$ which fix the transverse grid extent ${-x_\text{max} < x < x_\text{max}}$ and number of grid cells along each dimension ${n_x = 2\, x_\text{max} / dx}$.
In general, the maximum grid size $x_\text{max}$ should be set large enough to contain the full spacetime evolution of the event.
This means that the truncation of $T^{\mu\nu}$ at the boundaries of the grid should never modify the final-state observables.
We enforce this requirement by finding an energy density cutoff $e_\text{min}$ for which the matter $e < e_\text{min}$ can be effectively discarded without significantly modifying the simulation observables.
We then fix the maximum grid size $x_\text{max}$ such that it fully encloses the isotherm $T=T(e_\text{min})$ for the full lifetime of the fireball.

We find that we can quickly estimate the maximum transverse radius $R_\text{max}$ of the spacetime hypersurface ${T = T(e_\text{min})}$ by running the event on a coarse-grained spatial grid with one-third the spatial resolution we would otherwise require to resolve typical hydrodynamic observables such as mean $p_T$ and flows.
The simulation time of a single VISH2+1 event scales like ${\sim}n_x^3$ since $dx \propto d\tau$, and thus our ``pre-run'' event requires only ${\sim}1/27$th the time of a production event.
We therefore start by running a coarse-grained event on an excessively large grid for \emph{every} minimum-bias event to estimate $R_\text{max}$, then rerun the same event on a thrice finer grid with a trimmed spatial extent $x_\text{max} = R_\text{max}$.
See Fig.~\ref{fig:adaptive_grid} for a simple diagram of the procedure.

In practice, we find that event-by-event grid resizing leads to a massive speed increase for minimum bias events compared to using a single fixed grid for the entire minimum bias sample.
This is because the maximum transverse size of each event varies dramatically, from a few fm in peripheral Pb-Pb collisions to 50 fm or more in central Pb-Pb collisions.
The procedure should generalize to other hydrodynamic codes.

\section{Emulator validation}
\label{app:validation}

The emulator is a surrogate for the full physics simulation which generates probabilistic predictions for the model observables $\ym$ at a given point $\xv$.
Here we validate these probabilistic predictions using a method known as k-fold cross validation.
We first randomly partition our $d=500$ training points into $k=20$ equal sized subsamples or ``folds''.
One of the subsamples is used to validate the emulator and the remaining $k-1$ subsamples are used to train it.
The process is then repeated for each of the subsamples so that we end up validating on all of the training data.

\begin{figure*}[t]
  \includegraphics{validation_PbPb5020}\\
  \includegraphics{validation_pPb5020}
  \caption{
    \label{fig:validation_all}
    Emulator validation for the Pb-Pb collision system (top) and $p$-Pb collision system (bottom) at $\sqrts=5.02$~TeV.
    The ``piano keys'' in the top row of each figure are horizontally stacked box plots for the normalized residuals of each model observable.
    The boxes are 50\% interquartile ranges and whiskers are the 90\% interquantiles.
    The bottom row of each figure is the RMS fractional error defined by Eq.~\eqref{eq:frac_error}.
  }
\end{figure*}

Figure~\ref{fig:validation_example} shows a scatter plot of the emulator predictions with one-sigma error bars (x-axis) against explicit model calculations (y-axis).
Perfect emulator and model agreement is indicated by the black line $y_\text{pred} = y_\text{obs}$.
If the emulator errors are properly accounted for, then the normalized residuals ${z=(y_\text{pred} - y_\text{obs})/\sigma_\text{pred}}$ sample a unit normal distribution
\begin{equation}
  \label{eq:frac_error}
  P(z) \sim \mathcal{N}(\mu=0,\sigma=1).
\end{equation}

This comparison is shown by the histogram and box plot on the right side of Fig.~\ref{fig:validation_example}.
The emulator error is clearly significant, but it is also properly modeled, as indicated by the agreement between the normalized residuals and the unit normal distribution on the right (black curve).
Moreover, since we include this uncertainty in the likelihood covariance matrix \eqref{eq:likelihood}, we expect our results to be robust to the emulator limitations.
This is an important point that bears repeating.
The emulator uncertainty does not erode the veracity of the posterior distribution if it is correctly modeled and accounted for.

More generally, we can perform the validation test in Fig.~\ref{fig:validation_example} for \emph{every} observable $y \in \yv$ and check that each observable's normalized residuals ${z=(y_\text{pred} - y_\text{obs})/\sigma_\text{pred}}$ follow a unit normal distribution.
This test is applied to the $p$-Pb and Pb-Pb collision systems in Fig.~\ref{fig:validation_all}.
The top row of each figure shows a box-plot for the normalized residuals of each observable compared to the quantiles of a unit normal distribution.
The thin horizontal black lines correspond to the 10th and 90th percentiles of a unit normal distribution, and the gray band its interquartile range.
These visual references should be compared to the whiskers and interquartile range respectively of each box plot, analogous to the comparison test of Fig.~\ref{fig:validation_example}.
The emulators generally behave as expected, although the validation is better for the Pb-Pb system than the $p$-Pb system.
For instance, the $p$-Pb charged-particle yield $d\nch/d\eta$ uncertainties are over predicted.
It is not immediately clear why this would be the case, but the MAP observables in Fig.~\ref{fig:obs_map} are in good agreement with their emulator predictions which suggests it should not be a grave concern.

We also show in Fig.~\ref{fig:validation_all} an estimate of the emulator error magnitude.
This error is expressed in terms of the unitless variable
\begin{equation}
  \hat{z} = \frac{y_\text{pred} - y_\text{obs}}{(\Delta y)_{99\%}},
\end{equation}
where $(\Delta y)_{99\%}$ is 99\% of the full variability of $y$ across the design.
Thus $\hat{z}$ can be thought of as a fractional emulator error relative to the full design variability.
The bottom row of each figure shows the root-mean-square (RMS) value of $\hat{z}$.
We see that RMS $\hat{z}$ ranges from a few percent for most observables to a maximum value of 15\% for the $p$-Pb triangular flow $v_3\{2\}$ in the lowest multiplicity bin.
This suggests that the present analysis would benefit the most from more $p$-Pb events, in particular, from more multiplicity triggered events which are used to calculate the flows.

\end{document}
